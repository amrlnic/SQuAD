{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BIDAF_preprocessing.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyM0C/eQC5igIxuBtyCtak9r"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"iaDAPpHX8zCv"},"source":["# Set up"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7eZ2Hg2O78nj","executionInfo":{"status":"ok","timestamp":1618312683994,"user_tz":-120,"elapsed":596,"user":{"displayName":"Nicola Poggialini","photoUrl":"","userId":"11981468023328037551"}},"outputId":"12c85e40-3db5-4c87-d15e-43bc8e8c1e30"},"source":["import json\n","import os\n","import requests\n","import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","import nltk\n","from nltk import word_tokenize\n","nltk.download('punkt')\n","import gensim.downloader as gloader\n","from sklearn.model_selection import train_test_split\n","import re\n","import pickle\n","import tensorflow as tf\n","import random\n","\n","random.seed(42)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W3LCcFzs9DKj","executionInfo":{"status":"ok","timestamp":1618312759905,"user_tz":-120,"elapsed":45932,"user":{"displayName":"Nicola Poggialini","photoUrl":"","userId":"11981468023328037551"}},"outputId":"a3e8e7f7-0c6f-445f-dc79-3ead96135735"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"IbnaYSTU821I"},"source":["# Load data"]},{"cell_type":"markdown","metadata":{"id":"N4D9N5BhCIbf"},"source":["First of all, let's define a couple of functions to load the dataset and the Glove model."]},{"cell_type":"code","metadata":{"id":"N4q1TURq8l39"},"source":["url = \"https://raw.githubusercontent.com/amrlnic/SQuAD/main/data/training_set.json\" \n","download = requests.get(url).content\n","data = json.loads(download)\n","\n","def load_dataset(file, record_path = ['data', 'paragraphs', 'qas', 'answers'], verbose = True, with_answer = True):\n","\n","  \"\"\"\n","  parse the SQUAD dataset into a dataframe\n","  \"\"\"\n","\n","  if verbose:\n","      print(\"Reading the json file\")\n","\n","  if verbose:\n","      print(\"[INFO] processing...\")\n","\n","  # parsing different level's in the json file\n","  if with_answer:\n","    js = pd.json_normalize(file , record_path )\n","  m = pd.json_normalize(file, record_path[:-1] )\n","  r = pd.json_normalize(file, record_path[:-2])\n","  title = pd.json_normalize(file['data'], record_path = ['paragraphs'], meta = 'title')\n","  t = pd.json_normalize(file, record_path[0])\n","\n","  #combining it into single dataframe\n","  contexts = np.repeat(r['context'].values, r['qas'].str.len())\n","  m['context'] = contexts\n","  m['title'] = np.repeat(title['title'].values, r['qas'].str.len())\n","  m['c_id'] = m['context'].factorize()[0]\n","  m = m.drop(['answers'], axis = 1)\n","\n","  if with_answer:\n","    main = js.merge(m, left_index = True, right_index = True)\n","  else:\n","    main = m\n","  if verbose:\n","      print(f\"[INFO] there are {main.shape[0]} questions with single answer\")\n","      print(f\"[INFO] there are {main.groupby('c_id').sum().shape[0]} different contexts\")\n","      print(f\"[INFO] there are {len(t)} unrelated subjects\")\n","      print(\"[INFO] Done\")\n","\n","  return main\n","\n","def download_glove_model(embedding_dimension = 50):\n","\n","  \"\"\"\n","  download glove model\n","  \"\"\"\n","\n","  download_path = \"glove-wiki-gigaword-{}\".format(embedding_dimension)\n","  try:\n","    print('[INFO] downloading glove {}'.format(embedding_dimension))\n","    emb_model = gloader.load(download_path)\n","    print('[INFO] done !')\n","  except ValueError as e:\n","      print(\"Glove: 50, 100, 200, 300\")\n","      raise e\n","  return emb_model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v-I1a22LCV4o","executionInfo":{"status":"ok","timestamp":1618314208914,"user_tz":-120,"elapsed":7043,"user":{"displayName":"Nicola Poggialini","photoUrl":"","userId":"11981468023328037551"}},"outputId":"f32cee4e-bf07-45a6-b566-6a212854340c"},"source":["# Load dataset\n","squad_dataset = load_dataset(data)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Reading the json file\n","[INFO] processing...\n","[INFO] there are 87599 questions with single answer\n","[INFO] there are 18891 different contexts\n","[INFO] there are 442 unrelated subjects\n","[INFO] Done\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":374},"id":"v4sbx49x9SiR","executionInfo":{"status":"ok","timestamp":1618314208929,"user_tz":-120,"elapsed":1194,"user":{"displayName":"Nicola Poggialini","photoUrl":"","userId":"11981468023328037551"}},"outputId":"b1cadb77-e9a6-474c-f809-be863cecd4d5"},"source":["squad_dataset.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>answer_start</th>\n","      <th>text</th>\n","      <th>question</th>\n","      <th>id</th>\n","      <th>context</th>\n","      <th>title</th>\n","      <th>c_id</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>515</td>\n","      <td>Saint Bernadette Soubirous</td>\n","      <td>To whom did the Virgin Mary allegedly appear i...</td>\n","      <td>5733be284776f41900661182</td>\n","      <td>Architecturally, the school has a Catholic cha...</td>\n","      <td>University_of_Notre_Dame</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>188</td>\n","      <td>a copper statue of Christ</td>\n","      <td>What is in front of the Notre Dame Main Building?</td>\n","      <td>5733be284776f4190066117f</td>\n","      <td>Architecturally, the school has a Catholic cha...</td>\n","      <td>University_of_Notre_Dame</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>279</td>\n","      <td>the Main Building</td>\n","      <td>The Basilica of the Sacred heart at Notre Dame...</td>\n","      <td>5733be284776f41900661180</td>\n","      <td>Architecturally, the school has a Catholic cha...</td>\n","      <td>University_of_Notre_Dame</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>381</td>\n","      <td>a Marian place of prayer and reflection</td>\n","      <td>What is the Grotto at Notre Dame?</td>\n","      <td>5733be284776f41900661181</td>\n","      <td>Architecturally, the school has a Catholic cha...</td>\n","      <td>University_of_Notre_Dame</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>92</td>\n","      <td>a golden statue of the Virgin Mary</td>\n","      <td>What sits on top of the Main Building at Notre...</td>\n","      <td>5733be284776f4190066117e</td>\n","      <td>Architecturally, the school has a Catholic cha...</td>\n","      <td>University_of_Notre_Dame</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   answer_start  ... c_id\n","0           515  ...    0\n","1           188  ...    0\n","2           279  ...    0\n","3           381  ...    0\n","4            92  ...    0\n","\n","[5 rows x 7 columns]"]},"metadata":{"tags":[]},"execution_count":50}]},{"cell_type":"markdown","metadata":{"id":"qqNFqfA49mJQ"},"source":["# Pre-processing"]},{"cell_type":"code","metadata":{"id":"jN9dCvKw9kjk"},"source":["SAMPLES = squad_dataset.shape[0]\n","\n","def preprocess_sentence(text):\n","\n","  \"\"\"\n","  lowercase and strip the given text\n","  \"\"\"\n","\n","  text = text.lower()\n","  text = text.strip()\n","  return text\n","\n","def clean_dataset(dataset, with_answer = True):\n","\n","  \"\"\"\n","  preprocess the dataset\n","  \"\"\"\n","\n","  _dataset = dataset.copy()\n","\n","  cleaned_questions = _dataset['question'].apply(preprocess_sentence)\n","\n","  # we process only different contexts and then we duplicate them\n","  unique_context = pd.Series(_dataset['context'].unique())\n","  count_c = _dataset.groupby('c_id').size()\n","  cleaned_contexts = unique_context.apply(preprocess_sentence)\n","\n","  _dataset['question'] = cleaned_questions\n","\n","  if with_answer:\n","    cleaned_texts = _dataset['text'].apply(preprocess_sentence)\n","    _dataset['text'] = cleaned_texts\n","  _dataset['context'] = pd.Series(np.repeat(cleaned_contexts, count_c).tolist())\n","\n","  return _dataset"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2o2K7WRk9tAB"},"source":["squad_dataset = clean_dataset(squad_dataset)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xPQrWPTQ917L"},"source":["# Split"]},{"cell_type":"code","metadata":{"id":"0wCsMWp392_R"},"source":["def split(dataset, train_size = 0.8):\n","\n","  \"\"\"\n","  split the dataset in two part: the training and the validation based on titles\n","  \"\"\"\n","\n","  # find unique titles\n","  titles = squad_dataset['title']\n","  unique_titles = titles.unique()\n","\n","\n","  n_titles = len(unique_titles)\n","  titles_seq = list(range(n_titles))\n","\n","  train_len = int(n_titles*train_size)\n","\n","  # sample train indexes\n","  train_ind = random.sample(titles_seq, train_len)\n","  test_ind = list(set(titles_seq) - set(train_ind))\n","\n","  train_titles = unique_titles[train_ind]\n","  test_titles = unique_titles[test_ind]\n","\n","  squad_columns = list(squad_dataset.columns)\n","\n","  # initialize empty train and test df\n","  train_data = pd.DataFrame(columns = squad_columns)\n","  test_data = pd.DataFrame(columns = squad_columns)\n","\n","  for train_title in train_titles:\n","\n","    train_section = squad_dataset[squad_dataset['title'] == train_title]\n","    train_data = train_data.append(train_section)\n","\n","  for test_title in test_titles:\n","\n","    test_section = squad_dataset[squad_dataset['title'] == test_title]\n","    test_data = test_data.append(test_section)\n","\n","\n","  return train_data, test_data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vPlWF_UF96z5","executionInfo":{"status":"ok","timestamp":1618314234824,"user_tz":-120,"elapsed":10509,"user":{"displayName":"Nicola Poggialini","photoUrl":"","userId":"11981468023328037551"}},"outputId":"b9f05fdd-daf3-4f09-e447-43619b5e1b2f"},"source":["tr_df, vl_df = split(squad_dataset)\n","tr_df.shape[0],vl_df.shape[0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(68452, 19147)"]},"metadata":{"tags":[]},"execution_count":54}]},{"cell_type":"markdown","metadata":{"id":"RyfY0KvG9_by"},"source":["# Tokenization"]},{"cell_type":"code","metadata":{"id":"vCZmnLy7-BJ9"},"source":["def get_tokenizer(dataset, glove_model = None):\n","\n","  \"\"\"\n","  create the word and char tokenizers and feed them \n","  on the given dataset and the glove vocabulary\n","  \"\"\"\n","\n","  tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token = 'UNK', filters = '')\n","\n","  # we will only keep the 200 - 1 most frequent characters (otherwise oom issue)\n","  # others tokens are replaced by UNK token \n","  # we keep 199 most frequent tokens and indice 1 is UNK token (so we keep 198 tokens)\n","  char_tokenizer = tf.keras.preprocessing.text.Tokenizer(char_level = True, filters = '', oov_token = 'UNK', num_words = 200)\n","\n","  if glove_model == None:\n","    glove_model = download_glove_model(EMBEDDING_SIZE)\n","\n","  tokenized_questions = dataset['question'].apply(word_tokenize).to_list()\n","\n","  contexts = pd.Series(dataset['context'].unique())\n","  tokenized_contexts = contexts.apply(word_tokenize).to_list()\n","\n","  sequences = glove_model.index2entity + tokenized_questions + tokenized_contexts\n","\n","  del glove_model # we  don't need anymore the glove model\n","\n","  tokenizer.fit_on_texts(sequences)\n","  char_tokenizer.fit_on_texts(dataset['question'].to_list() + contexts.to_list())\n","\n","  return tokenizer, char_tokenizer\n","  \n","\n","\n","def update_tokenizer(dataset, tokenizer, char_tokenizer):\n","\n","  \"\"\"\n","  update the existing word/char vocabulary on a new dataset\n","  \"\"\"\n","\n","  tokenized_questions = dataset['question'].apply(word_tokenize).to_list()\n","\n","  contexts = pd.Series(dataset['context'].unique())\n","  tokenized_contexts = contexts.apply(word_tokenize).to_list()\n","\n","  sequences = tokenized_questions + tokenized_contexts\n","  tokenizer.fit_on_texts(sequences)\n","\n","  char_tokenizer.fit_on_texts(dataset['question'].to_list() + contexts.to_list())\n","\n","\n","\n","def tokenize(dataset, tokenizer, char_tokenizer):\n","\n","  \"\"\"\n","  tokenize the given dataset\n","  \"\"\"\n","\n","  _dataset = dataset.copy()\n","\n","  tokenized_questions = _dataset['question'].apply(word_tokenize).to_list()\n","  tokenized_contexts = _dataset['context'].apply(word_tokenize).to_list()\n","\n","  t_q = tokenizer.texts_to_sequences(tokenized_questions)\n","  t_c = tokenizer.texts_to_sequences(tokenized_contexts)\n","\n","  c_q = []\n","  c_c = []\n","\n","  for question, context in zip(tokenized_questions, tokenized_contexts):\n","    _q = char_tokenizer.texts_to_sequences(question)\n","    _c = char_tokenizer.texts_to_sequences(context)\n","    c_q.append(_q)\n","    c_c.append(_c)\n","\n","  _dataset['tokenized_question'] = t_q\n","  _dataset['tokenized_context'] = t_c\n","\n","  _dataset['char_tokenized_question'] = c_q\n","  _dataset['char_tokenized_context'] = c_c\n","\n","  return _dataset"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-V89ZLCR-vU2"},"source":["Our vocabulary is based on the Glove vocabulary, and we add terms from the training set."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fZlt0Rou-Hip","executionInfo":{"status":"ok","timestamp":1618314396711,"user_tz":-120,"elapsed":165433,"user":{"displayName":"Nicola Poggialini","photoUrl":"","userId":"11981468023328037551"}},"outputId":"c0ced5db-9baa-42c0-b05f-59dfd9331b95"},"source":["tokenizer, char_tokenizer = get_tokenizer(tr_df)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[INFO] downloading glove 300\n","[INFO] done !\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YSXT1n3C-06h","executionInfo":{"status":"ok","timestamp":1618314398583,"user_tz":-120,"elapsed":1823,"user":{"displayName":"Nicola Poggialini","photoUrl":"","userId":"11981468023328037551"}},"outputId":"5ce7e463-eee2-408a-fa88-32a664fffbaa"},"source":["print(len(tokenizer.word_index))\n","len(char_tokenizer.word_index)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["423870\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["1133"]},"metadata":{"tags":[]},"execution_count":57}]},{"cell_type":"markdown","metadata":{"id":"kSe0NYom-83e"},"source":["We then update our vocabulary with terms from the validation set."]},{"cell_type":"code","metadata":{"id":"uof2QIDO-4D_"},"source":["update_tokenizer(vl_df, tokenizer, char_tokenizer)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uV7bZOUC_CbR","executionInfo":{"status":"ok","timestamp":1618314407012,"user_tz":-120,"elapsed":10086,"user":{"displayName":"Nicola Poggialini","photoUrl":"","userId":"11981468023328037551"}},"outputId":"323f4ebb-f011-4be5-a097-cfe0f6f812e2"},"source":["print(len(tokenizer.word_index))\n","len(char_tokenizer.word_index)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["429758\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["1265"]},"metadata":{"tags":[]},"execution_count":59}]},{"cell_type":"markdown","metadata":{"id":"qW8iOq55_FyK"},"source":["# Filter rows"]},{"cell_type":"code","metadata":{"id":"MS1Dkrwi_He7"},"source":["def get_start_end(row):\n","\n","  \"\"\"\n","  get the start and end span for each sample,\n","  if the span cannot be found return -1\n","  \"\"\"\n","\n","  context = row['context']\n","  answer = row['text']\n","  tok_answer = word_tokenize(answer)\n","\n","  _start = context.find(answer)\n","\n","  if _start == -1:\n","    # the answer is not in the context\n","    # maybe due to a typo\n","    row['start'] = -1\n","    row['end'] = -1\n","    return row\n","\n","  lc = context[:_start]\n","  lc = word_tokenize(lc)\n","\n","  start = len(lc)\n","  end = start + len(tok_answer)\n","\n","  row['start'] = start\n","  row['end'] = end\n","\n","  return row"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OOiXEQfI_Q6F"},"source":["# take a while\n","tr_df = tr_df.apply(get_start_end, axis = 1)\n","vl_df = vl_df.apply(get_start_end, axis = 1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LupOWjtm_Xl0"},"source":["We get rid of samples where the answer doesn't match the context (maybe there is a typo in the answer or the context).  "]},{"cell_type":"code","metadata":{"id":"fHb3kjgZ_dxw"},"source":["# we get rid of samples where the answer doesn't match the context\n","tr_df = tr_df[tr_df['start'] != -1]\n","vl_df = vl_df[vl_df['start'] != -1]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FVfFF787_iJl"},"source":["tr_df = tokenize(tr_df, tokenizer, char_tokenizer)\n","vl_df = tokenize(vl_df, tokenizer, char_tokenizer)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dtf7BcKG_mcZ"},"source":["# Padding"]},{"cell_type":"markdown","metadata":{"id":"RHIB2spv_v0M"},"source":["We display some useful stats in order to define the padding size (at the word and character level, for both question and context)."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7tJCvQ6h_rnD","executionInfo":{"status":"ok","timestamp":1618314760355,"user_tz":-120,"elapsed":363282,"user":{"displayName":"Nicola Poggialini","photoUrl":"","userId":"11981468023328037551"}},"outputId":"ae356037-a4f8-47d9-c58b-8603e95c4e31"},"source":["print(tr_df['tokenized_question'].str.len().describe())\n","vl_df['tokenized_question'].str.len().describe()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["count    68384.000000\n","mean        11.289424\n","std          3.751633\n","min          1.000000\n","25%          9.000000\n","50%         11.000000\n","75%         13.000000\n","max         60.000000\n","Name: tokenized_question, dtype: float64\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["count    19131.000000\n","mean        11.280749\n","std          3.621645\n","min          3.000000\n","25%          9.000000\n","50%         11.000000\n","75%         13.000000\n","max         38.000000\n","Name: tokenized_question, dtype: float64"]},"metadata":{"tags":[]},"execution_count":64}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5meQ8RGM_2dO","executionInfo":{"status":"ok","timestamp":1618314760357,"user_tz":-120,"elapsed":363254,"user":{"displayName":"Nicola Poggialini","photoUrl":"","userId":"11981468023328037551"}},"outputId":"7ecd4648-9469-44d9-ea51-a8e5c0e5b96c"},"source":["print(tr_df['tokenized_question'].str.len().quantile(0.99))\n","vl_df['tokenized_question'].str.len().quantile(0.99)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["23.0\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["22.0"]},"metadata":{"tags":[]},"execution_count":65}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fXJYFE_o_8nu","executionInfo":{"status":"ok","timestamp":1618314760359,"user_tz":-120,"elapsed":363224,"user":{"displayName":"Nicola Poggialini","photoUrl":"","userId":"11981468023328037551"}},"outputId":"e21ec98d-7f16-495b-d77f-26d8e6e8819e"},"source":["print(tr_df['tokenized_context'].str.len().describe())\n","vl_df['tokenized_context'].str.len().describe()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["count    68384.000000\n","mean       136.110464\n","std         55.493847\n","min         22.000000\n","25%        101.000000\n","50%        125.000000\n","75%        162.000000\n","max        766.000000\n","Name: tokenized_context, dtype: float64\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["count    19131.000000\n","mean       143.389838\n","std         60.636169\n","min         22.000000\n","25%        103.000000\n","50%        130.000000\n","75%        170.000000\n","max        638.000000\n","Name: tokenized_context, dtype: float64"]},"metadata":{"tags":[]},"execution_count":66}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W1gRHAlT__MF","executionInfo":{"status":"ok","timestamp":1618314760360,"user_tz":-120,"elapsed":363194,"user":{"displayName":"Nicola Poggialini","photoUrl":"","userId":"11981468023328037551"}},"outputId":"538d21d1-e9b7-4948-e15c-b37a87c23b20"},"source":["print(tr_df['tokenized_context'].str.len().quantile(0.99))\n","vl_df['tokenized_context'].str.len().quantile(0.99)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["318.0\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["350.0"]},"metadata":{"tags":[]},"execution_count":67}]},{"cell_type":"code","metadata":{"id":"Sd9n1S44ADKc"},"source":["def len_words(dataset):\n","\n","  \"\"\"\n","  return the word's length\n","  \"\"\"\n","\n","  count_q = []\n","  count_c = []\n","\n","  for idx, row in dataset.iterrows():\n","    for w in row['char_tokenized_question']:\n","      l = len(w)\n","      count_q.append(l)\n","      \n","    for w in row['char_tokenized_context']:\n","      m = len(w)\n","      count_c.append(m)\n","  \n","  return pd.Series(count_q), pd.Series(count_c)\n","\n","t_q,t_c = len_words(tr_df)\n","v_q,v_c = len_words(vl_df)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5jOKaeTOAHg-","executionInfo":{"status":"ok","timestamp":1618314777003,"user_tz":-120,"elapsed":379782,"user":{"displayName":"Nicola Poggialini","photoUrl":"","userId":"11981468023328037551"}},"outputId":"725f1491-e277-430c-aaff-ceb9da7cb0da"},"source":["print(t_q.describe())\n","t_c.describe()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["count    772016.000000\n","mean          4.464509\n","std           2.697089\n","min           1.000000\n","25%           2.000000\n","50%           4.000000\n","75%           6.000000\n","max          26.000000\n","dtype: float64\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["count    9.307778e+06\n","mean     4.637862e+00\n","std      2.981265e+00\n","min      1.000000e+00\n","25%      2.000000e+00\n","50%      4.000000e+00\n","75%      7.000000e+00\n","max      3.700000e+01\n","dtype: float64"]},"metadata":{"tags":[]},"execution_count":69}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Iw5IXEnUAJyZ","executionInfo":{"status":"ok","timestamp":1618314777005,"user_tz":-120,"elapsed":379753,"user":{"displayName":"Nicola Poggialini","photoUrl":"","userId":"11981468023328037551"}},"outputId":"c50f6479-da10-4aa7-cea7-cf5aac6d5af5"},"source":["print(v_q.describe())\n","v_c.describe()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["count    215812.000000\n","mean          4.393486\n","std           2.614453\n","min           1.000000\n","25%           2.000000\n","50%           4.000000\n","75%           6.000000\n","max          30.000000\n","dtype: float64\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["count    2.743191e+06\n","mean     4.589248e+00\n","std      2.932105e+00\n","min      1.000000e+00\n","25%      2.000000e+00\n","50%      4.000000e+00\n","75%      7.000000e+00\n","max      3.300000e+01\n","dtype: float64"]},"metadata":{"tags":[]},"execution_count":70}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UnS_Pjc1AOnj","executionInfo":{"status":"ok","timestamp":1618314777710,"user_tz":-120,"elapsed":380426,"user":{"displayName":"Nicola Poggialini","photoUrl":"","userId":"11981468023328037551"}},"outputId":"5c7845c1-7568-4457-a0c1-99fd1f565bbc"},"source":["print(t_q.quantile(0.99))\n","t_c.quantile(0.99)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["12.0\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["13.0"]},"metadata":{"tags":[]},"execution_count":71}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pD-phrUHARwx","executionInfo":{"status":"ok","timestamp":1618314777716,"user_tz":-120,"elapsed":380401,"user":{"displayName":"Nicola Poggialini","photoUrl":"","userId":"11981468023328037551"}},"outputId":"12c1d5cc-2799-4070-9606-95fab7ad147f"},"source":["print(v_q.quantile(0.99))\n","v_c.quantile(0.99)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["12.0\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["13.0"]},"metadata":{"tags":[]},"execution_count":72}]},{"cell_type":"markdown","metadata":{"id":"h7omCoMOAbp2"},"source":["There are obviously some outliers. We are compeled to get rid of some samples because of memory issues.\n","\n","We will get rid of contexts that have more than 400 words and questions that have more than 25 words.\n","\n","We will set the length of a word to 15 characters.\n","\n","**EDIT :** These numbers are huge but we won't get out of memory errors if we build a sequence generator. If you don't want to use the sequence generator, you should reduce these numbers.\n","\n","**EDIT :** Now that we use a sequence generator, we could define `*_MAXLEN` variables according to the stats provided by the training set."]},{"cell_type":"code","metadata":{"id":"kJmPyiBUAff-"},"source":["QUESTION_MAXLEN = 25\n","CONTEXT_MAXLEN = 400\n","WORD_MAXLEN = 15\n","BATCH_SIZE = 10"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gv0gCARdAh0p","executionInfo":{"status":"ok","timestamp":1618314777719,"user_tz":-120,"elapsed":380352,"user":{"displayName":"Nicola Poggialini","photoUrl":"","userId":"11981468023328037551"}},"outputId":"f139b6a1-ec2c-4b53-e441-7b05e1a9c524"},"source":["tr_df.shape, vl_df.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((68384, 13), (19131, 13))"]},"metadata":{"tags":[]},"execution_count":74}]},{"cell_type":"code","metadata":{"id":"Kucor8ZZAoPC"},"source":["tr_df = tr_df[(tr_df['tokenized_question'].str.len() <= QUESTION_MAXLEN) & (tr_df['tokenized_context'].str.len() <= CONTEXT_MAXLEN) & (tr_df['start'] <= CONTEXT_MAXLEN) & (tr_df['end'] <= CONTEXT_MAXLEN) ].reset_index(drop = True)\n","vl_df = vl_df[(vl_df['tokenized_question'].str.len() <= QUESTION_MAXLEN) & (vl_df['tokenized_context'].str.len() <= CONTEXT_MAXLEN) & (vl_df['start'] <= CONTEXT_MAXLEN) & (vl_df['end'] <= CONTEXT_MAXLEN) ].reset_index(drop = True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nRMRb3S-ArMp","executionInfo":{"status":"ok","timestamp":1618314777732,"user_tz":-120,"elapsed":380308,"user":{"displayName":"Nicola Poggialini","photoUrl":"","userId":"11981468023328037551"}},"outputId":"46152351-0819-4e93-a689-1a17a80f40f5"},"source":["tr_df.shape[0], vl_df.shape[0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(68001, 19018)"]},"metadata":{"tags":[]},"execution_count":76}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9ORxQae6AuMS","executionInfo":{"status":"ok","timestamp":1618314777738,"user_tz":-120,"elapsed":380280,"user":{"displayName":"Nicola Poggialini","photoUrl":"","userId":"11981468023328037551"}},"outputId":"1f6c7d8c-37a7-49a2-c73c-e333396793b6"},"source":[" print(f' we get rid of : {SAMPLES - (tr_df.shape[0] + vl_df.shape[0])} samples')"],"execution_count":null,"outputs":[{"output_type":"stream","text":[" we get rid of : 580 samples\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"9KHbmu_wA8D-"},"source":["# Save datasets in json format"]},{"cell_type":"code","metadata":{"id":"i4AW5LcPA6nn"},"source":["def df_to_json(df, path, with_answer = True):\n","\n","  \"\"\"\n","  parse the given dataframe into the SQUAD json format and\n","  save it\n","  \"\"\"\n","  \n","  data = []\n","\n","  for title, articles in df.groupby('title'):\n","    chapter = {'title': title}\n","    paragraphs = []\n","    for context, contents in articles.groupby('context'):\n","      paragraph = {'context': context}\n","      qas = []\n","      for i, content in contents.iterrows():\n","        if with_answer:\n","          qa = {'answers': [{'answer_start': content['answer_start'], 'text': content['text']}], 'question': content['question'], 'id': content['id']}\n","        else:\n","          qa = {'question': content['question'], 'id': content['id']}\n","        qas.append(qa)\n","      paragraph.update({'qas': qas})\n","      paragraphs.append(paragraph)\n","    chapter.update({'paragraphs': paragraphs})\n","    data.append(chapter)\n","  raw_data = {'data': data}\n","\n","  with open(path, 'w') as handle:\n","    json.dump(raw_data, handle)\n","\n","  print(f'dataset saved in {path}')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XOBC-cLjBLSL","executionInfo":{"status":"ok","timestamp":1618314811101,"user_tz":-120,"elapsed":19504,"user":{"displayName":"Nicola Poggialini","photoUrl":"","userId":"11981468023328037551"}},"outputId":"e11cbd3e-1b0a-43a4-e493-e24ed4487505"},"source":["# save datasets in json format\n","train_set_path = '/content/drive/MyDrive/SQUAD_project/train_set.json'\n","path_to_train_set = os.path.join(os.getcwd(), train_set_path)\n","df_to_json(tr_df, path_to_train_set)\n","\n","val_set_path = '/content/drive/MyDrive/SQUAD_project/val_set.json'\n","path_to_valid_set = os.path.join(os.getcwd(), val_set_path)\n","df_to_json(vl_df, path_to_valid_set)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["dataset saved in /content/drive/MyDrive/SQUAD_project/train_set.json\n","dataset saved in /content/drive/MyDrive/SQUAD_project/val_set.json\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KNUw9hUpFNxR"},"source":["# we save both tokenizers\n","tokenizers_folder = os.path.join(os.getcwd(), '/content/drive/MyDrive/SQUAD_project/', 'tokenizers')\n","if not os.path.exists(tokenizers_folder):\n","  os.makedirs(tokenizers_folder)\n","\n","path_word_tokenizer = os.path.join(tokenizers_folder, 'word_tokenizer.pkl')\n","with open(path_word_tokenizer, 'wb') as handle:\n","    pickle.dump(tokenizer, handle, protocol = pickle.HIGHEST_PROTOCOL)\n","\n","path_char_tokenizer = os.path.join(tokenizers_folder, 'char_tokenizer.pkl')\n","with open(path_char_tokenizer, 'wb') as handle:\n","    pickle.dump(char_tokenizer, handle, protocol = pickle.HIGHEST_PROTOCOL)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jS83dK8yFgvy"},"source":["# Iterator"]},{"cell_type":"markdown","metadata":{"id":"_BPZemTxFk4N"},"source":["We create the iterator. The iterator allows us to work with much bigger data, because it is loaded into memory only when we need them."]},{"cell_type":"code","metadata":{"id":"3-0PqB0VFYoc"},"source":["# utils/datasets/dataset.py\n","class SQUAD_dataset(tf.keras.utils.Sequence):\n","\n","  \"\"\"\n","  utility class to create a working dataset that\n","  can be given to a neural network\n","  \"\"\"\n","\n","  def __init__(self, data, question_maxlen, context_maxlen, word_maxlen, batch_size, with_answer = True):\n","    self.QUESTION_MAXLEN = question_maxlen\n","    self.CONTEXT_MAXLEN = context_maxlen\n","    self.WORD_MAXLEN = word_maxlen\n","    self.batch_size = batch_size\n","    self.with_answer = with_answer\n","    self.__get_batches(data)\n","\n","  def __len__(self):\n","    return len(self.batches)\n","\n","  def __get_batches(self, data):\n","    batches = [data[i:i+self.batch_size] for i in range(0, len(data), self.batch_size)]\n","    self.batches = batches\n","\n","  def __repr__(self):\n","    template = '''SQUAD_dataset : questions : ({0}, {1}), contexts : ({0}, {2}), char_questions : ({0}, {1}, {3}), char_contexts : ({0}, {2}, {3}), id : ({0}, 1)'''.format(self.batch_size, self.QUESTION_MAXLEN, self.CONTEXT_MAXLEN, self.WORD_MAXLEN)\n","    return template\n","\n","  @classmethod\n","  def from_file(cls, path):\n","    path = os.path.join(os.getcwd(), path)\n","    with open(path, 'rb') as handle:\n","      dataset = pickle.load(handle)\n","    return dataset\n","\n","  def to_pickle(self, path):\n","    path = os.path.join(os.getcwd(), path)\n","    folder = os.path.dirname(path)\n","\n","    if not os.path.exists(folder):\n","      os.makedirs(folder)\n","\n","    with open(path, 'wb') as handle:\n","      pickle.dump(self, handle, protocol = pickle.HIGHEST_PROTOCOL)\n","\n","  def __getitem__(self, idx):\n","    batch = self.batches[idx].reset_index(drop = True)\n","\n","    id = np.asarray(batch['id'])\n","\n","    # questions and contexts words padding\n","    q_w = tf.keras.preprocessing.sequence.pad_sequences(batch['tokenized_question'], padding = 'post', maxlen = self.QUESTION_MAXLEN)\n","    c_w = tf.keras.preprocessing.sequence.pad_sequences(batch['tokenized_context'], padding = 'post', maxlen = self.CONTEXT_MAXLEN)\n","\n","    # question_char padding\n","    q_c = np.zeros((q_w.shape[0], self.QUESTION_MAXLEN, self.WORD_MAXLEN), dtype = np.int32)\n","\n","    for i, value in batch['char_tokenized_question'].iteritems():\n","      v = tf.keras.preprocessing.sequence.pad_sequences(value, padding = 'post', maxlen = self.WORD_MAXLEN, truncating = 'post')\n","      to_add = self.QUESTION_MAXLEN - v.shape[0]\n","      add = np.zeros((to_add, self.WORD_MAXLEN))\n","      arr = np.vstack([v,add])\n","      q_c[i] = arr\n","\n","    # context_char padding\n","    c_c = np.zeros((q_w.shape[0], self.CONTEXT_MAXLEN, self.WORD_MAXLEN), dtype = np.int32)\n","\n","    for i, value in batch['char_tokenized_context'].iteritems():\n","      v = tf.keras.preprocessing.sequence.pad_sequences(value, padding = 'post', maxlen = self.WORD_MAXLEN, truncating = 'post')\n","      to_add = self.CONTEXT_MAXLEN - v.shape[0]\n","      add = np.zeros((to_add, self.WORD_MAXLEN))\n","      arr = np.vstack([v,add])\n","      c_c[i] = arr\n","\n","    # one hot encode start and end\n","    if self.with_answer:\n","      y_start = tf.keras.utils.to_categorical(batch['start'].values, self.CONTEXT_MAXLEN)\n","      y_end = tf.keras.utils.to_categorical(batch['end'].values, self.CONTEXT_MAXLEN)\n","\n","      # (inputs), (outputs), (id)\n","      return (q_w, c_w, q_c, c_c), (y_start, y_end), (id,)\n","    return (q_c, c_w, q_c, c_c), (id,)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bpFLoWkQFwgO"},"source":["tr_data = SQUAD_dataset(tr_df, batch_size = BATCH_SIZE, question_maxlen = QUESTION_MAXLEN, context_maxlen = CONTEXT_MAXLEN, word_maxlen = WORD_MAXLEN)\n","vl_data = SQUAD_dataset(vl_df, batch_size = BATCH_SIZE, question_maxlen = QUESTION_MAXLEN, context_maxlen = CONTEXT_MAXLEN, word_maxlen = WORD_MAXLEN)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SmOaYIlEFy8C","executionInfo":{"status":"ok","timestamp":1618315013699,"user_tz":-120,"elapsed":861,"user":{"displayName":"Nicola Poggialini","photoUrl":"","userId":"11981468023328037551"}},"outputId":"8190b2f2-2957-43f2-d968-f0cfd243f7ea"},"source":["# number of batches\n","print(len(tr_data))\n","len(vl_data)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["6801\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["1902"]},"metadata":{"tags":[]},"execution_count":85}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z6Sf_gcQF0W0","executionInfo":{"status":"ok","timestamp":1618315012417,"user_tz":-120,"elapsed":712,"user":{"displayName":"Nicola Poggialini","photoUrl":"","userId":"11981468023328037551"}},"outputId":"f42d72cf-b501-4967-cb61-332b159d4b3a"},"source":["tr_data"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["SQUAD_dataset : questions : (10, 25), contexts : (10, 400), char_questions : (10, 25, 15), char_contexts : (10, 400, 15), id : (10, 1)"]},"metadata":{"tags":[]},"execution_count":84}]},{"cell_type":"code","metadata":{"id":"bHBGzg1qF5Lz"},"source":["tr_data.to_pickle('/content/drive/MyDrive/SQUAD_project/train_dataset.pkl')\n","vl_data.to_pickle('/content/drive/MyDrive/SQUAD_project/valid_dataset.pkl')"],"execution_count":null,"outputs":[]}]}