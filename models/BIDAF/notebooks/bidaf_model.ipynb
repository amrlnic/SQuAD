{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hvai4UxyMH1S"
   },
   "source": [
    "a very good explanation of the BIDAF architecture : \n",
    "\n",
    "https://towardsdatascience.com/the-definitive-guide-to-bi-directional-attention-flow-d0e96e9e666b\n",
    "\n",
    "character embedding with CNN :\n",
    "\n",
    "https://towardsdatascience.com/besides-word-embedding-why-you-need-to-know-character-embedding-6096a34a3b10\n",
    "https://github.com/makcedward/nlp/blob/master/sample/nlp-character_embedding.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AcJDYwmpauq3"
   },
   "source": [
    "To run this notebook you should have run the bidaf_preprocessing one.  \n",
    "You should as well modify all paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6825,
     "status": "ok",
     "timestamp": 1613672531639,
     "user": {
      "displayName": "corentin magyar",
      "photoUrl": "",
      "userId": "04135457892709862388"
     },
     "user_tz": -60
    },
    "id": "u8tAi2rmMp_h",
    "outputId": "96e1e78a-ac2b-409d-b60d-7025c2970bca"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\coren\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\coren\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import os\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, Bidirectional, TimeDistributed, Layer, Softmax, Concatenate, Dropout, Conv1D, GlobalMaxPooling1D\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pickle\n",
    "import nltk\n",
    "import json\n",
    "nltk.download('punkt')\n",
    "from nltk import word_tokenize\n",
    "import gensim.downloader as gloader\n",
    "import math\n",
    "\n",
    "import sys\n",
    "sys.path.append(os.path.abspath('../'))\n",
    "from utils.datasets import SQUAD_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 4215,
     "status": "ok",
     "timestamp": 1613672535862,
     "user": {
      "displayName": "corentin magyar",
      "photoUrl": "",
      "userId": "04135457892709862388"
     },
     "user_tz": -60
    },
    "id": "bJiZBATwVsHR"
   },
   "outputs": [],
   "source": [
    "path_word_tokenizer = os.path.abspath('../utils/tokenizers/word_tokenizer.pkl')\n",
    "with open(path_word_tokenizer, 'rb') as handle:\n",
    "  tokenizer = pickle.load(handle)\n",
    "\n",
    "path_char_tokenizer = os.path.abspath('../utils/tokenizers/char_tokenizer.pkl')\n",
    "with open(path_char_tokenizer, 'rb') as char_handle:\n",
    "  char_tokenizer = pickle.load(char_handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 24747,
     "status": "ok",
     "timestamp": 1613672556403,
     "user": {
      "displayName": "corentin magyar",
      "photoUrl": "",
      "userId": "04135457892709862388"
     },
     "user_tz": -60
    },
    "id": "JNsokDe-Lk6f"
   },
   "outputs": [],
   "source": [
    "train_dataset = SQUAD_dataset.from_file('../utils/datasets/train_dataset.pkl')\n",
    "valid_dataset = SQUAD_dataset.from_file('../utils/datasets/valid_dataset.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24747,
     "status": "ok",
     "timestamp": 1613672556408,
     "user": {
      "displayName": "corentin magyar",
      "photoUrl": "",
      "userId": "04135457892709862388"
     },
     "user_tz": -60
    },
    "id": "jzg9Gv7gMcIe",
    "outputId": "c352cc5b-8b6f-4880-885c-c99bad7947b6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SQUAD_dataset : questions : (10, 25), contexts : (10, 400), char_questions : (10, 25, 15), char_contexts : (10, 400, 15), index : (10, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24741,
     "status": "ok",
     "timestamp": 1613672556409,
     "user": {
      "displayName": "corentin magyar",
      "photoUrl": "",
      "userId": "04135457892709862388"
     },
     "user_tz": -60
    },
    "id": "ANbgqwjsMfBK",
    "outputId": "90767529-3025-4dac-e8a4-9d94401d01f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6961\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1742"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(train_dataset))\n",
    "len(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 24732,
     "status": "ok",
     "timestamp": 1613672556409,
     "user": {
      "displayName": "corentin magyar",
      "photoUrl": "",
      "userId": "04135457892709862388"
     },
     "user_tz": -60
    },
    "id": "68jUK4Q-M8ur"
   },
   "outputs": [],
   "source": [
    "# globals variables\n",
    "QUESTION_MAXLEN = 25\n",
    "CONTEXT_MAXLEN = 400\n",
    "EMBEDDING_SIZE = 300 # we can try different embedding size (50, 100, 300) or even try word2vec or fastext instead of glove\n",
    "WORD_VOCAB_LEN = len(tokenizer.word_index) + 1 # +1 for the pad token\n",
    "BATCH_SIZE = 10\n",
    "EPOCHS = 10\n",
    "CHAR_VOCAB_LEN = char_tokenizer.num_words # PAD token and UNK token included\n",
    "WORD_MAXLEN = 15\n",
    "LR = 0.0005\n",
    "N_FILTERS = EMBEDDING_SIZE\n",
    "FILTER_SIZE = 3\n",
    "CHAR_EMBEDDING_SIZE = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 24103,
     "status": "ok",
     "timestamp": 1613672556410,
     "user": {
      "displayName": "corentin magyar",
      "photoUrl": "",
      "userId": "04135457892709862388"
     },
     "user_tz": -60
    },
    "id": "HeQ8obxkV-y1"
   },
   "outputs": [],
   "source": [
    "def download_glove_embedding(embedding_dimension = 50):\n",
    "\n",
    "  \"\"\"\n",
    "  download glove model\n",
    "  \"\"\"\n",
    "\n",
    "  download_path = 'glove-wiki-gigaword-{}'.format(embedding_dimension)\n",
    "  try:\n",
    "    emb_model = gloader.load(download_path)\n",
    "  except ValueError as e:\n",
    "      print('Glove: 50, 100, 200, 300')\n",
    "      raise e\n",
    "  return emb_model\n",
    "\n",
    "def build_embedding_matrix(tokenizer, path_embedding_matrix, glove_model = None):\n",
    "\n",
    "  \"\"\"\n",
    "  build the word embedding matrix based on the glove vocabulary\n",
    "  \"\"\"\n",
    "\n",
    "  if os.path.exists(path_embedding_matrix):\n",
    "\n",
    "    embedding_matrix = np.load(path_embedding_matrix)\n",
    "    return embedding_matrix\n",
    "\n",
    "  else:\n",
    "\n",
    "    if glove_model == None:\n",
    "      glove_model = download_glove_embedding(EMBEDDING_SIZE)\n",
    "\n",
    "    embedding_matrix = np.zeros((WORD_VOCAB_LEN, EMBEDDING_SIZE))\n",
    "\n",
    "    for w,i in tokenizer.word_index.items():\n",
    "\n",
    "      if w in glove_model.vocab:\n",
    "        embedding_matrix[i,:] = glove_model.get_vector(w)\n",
    "      else:\n",
    "        embedding_matrix[i,:] = np.random.randn(1, EMBEDDING_SIZE)\n",
    "\n",
    "    del glove_model # we don't need it anymore\n",
    "\n",
    "    np.save(path_embedding_matrix, embedding_matrix)\n",
    "\n",
    "    return embedding_matrix\n",
    "\n",
    "def build_char_embedding_matrix(char_tokenizer):\n",
    "\n",
    "  \"\"\"\n",
    "  build the character embedding matrix\n",
    "  \"\"\"\n",
    "\n",
    "  char_embedding_matrix = np.zeros((CHAR_VOCAB_LEN,CHAR_VOCAB_LEN - 1))  # we have 199 characters that we have to one hot so each character has 199 dimensions\n",
    "\n",
    "  for char, i in char_tokenizer.word_index.items():\n",
    "    if i <= 199:\n",
    "      char_embedding_matrix[i][i - 1] = 1\n",
    "    else:\n",
    "      break\n",
    "  return char_embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DTL3GYD2TsrV"
   },
   "source": [
    "We build the embedding matrix.  \n",
    "We can also initialize a char_embedding_matrix, or we can let the model learn these embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 206825,
     "status": "ok",
     "timestamp": 1613672741580,
     "user": {
      "displayName": "corentin magyar",
      "photoUrl": "",
      "userId": "04135457892709862388"
     },
     "user_tz": -60
    },
    "id": "4Jkm1-RRXypA",
    "outputId": "be85dd81-c73a-428a-d3c6-4ec395b0297b"
   },
   "outputs": [],
   "source": [
    "path_embedding_matrix = os.path.abspath('../utils/data/embedding.npy')\n",
    "embedding_matrix = build_embedding_matrix(tokenizer, path_embedding_matrix)\n",
    "\n",
    "# instead of one hot encode char tokens maybe we can use glove or randomly fill the matrix\n",
    "# these embeddings should be trainable\n",
    "# https://github.com/minimaxir/char-embeddings\n",
    "#char_embedding_matrix = build_char_embedding_matrix(char_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R2rjHEDUTvv4"
   },
   "source": [
    "Then we define all layers of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 204875,
     "status": "ok",
     "timestamp": 1613672741589,
     "user": {
      "displayName": "corentin magyar",
      "photoUrl": "",
      "userId": "04135457892709862388"
     },
     "user_tz": -60
    },
    "id": "00VHMy09YmOp"
   },
   "outputs": [],
   "source": [
    "# utils/layers\n",
    "class WordEmbedding(Layer):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, input_len, embedding_matrix, trainable = False, mask_zero = True, **kwargs):\n",
    "        \n",
    "        super(WordEmbedding, self).__init__(**kwargs)\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.input_len = input_len\n",
    "        self.embedding_matrix = embedding_matrix\n",
    "        self.trainable = trainable\n",
    "        self.mask_zero = mask_zero\n",
    "\n",
    "        self.word_embed = Embedding(\n",
    "            input_dim = self.input_dim,\n",
    "            output_dim = self.output_dim,\n",
    "            weights = [self.embedding_matrix],\n",
    "            trainable = self.trainable,\n",
    "            input_length = self.input_len,\n",
    "            mask_zero = self.mask_zero,\n",
    "        )\n",
    "\n",
    "    def build(self, input_shape):\n",
    "      self.built = True\n",
    "\n",
    "    def call(self, inputs):\n",
    "        input = inputs\n",
    "        return self.word_embed(input) \n",
    "    \n",
    "    # inplement this method in order to get a serializable layer as part of a Functional model\n",
    "    def get_config(self):\n",
    "        # the base Layer class takes some keywords arguments like name and dtype, it is good to include \n",
    "        # them in the config (so we call the parent method and use the update method)\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'input_dim': self.input_dim,\n",
    "            'output_dim': self.output_dim,\n",
    "            'input_len': self.input_len, \n",
    "            'trainable': self.trainable,\n",
    "            'mask_zero': self.mask_zero\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "      return cls(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 202154,
     "status": "ok",
     "timestamp": 1613672741589,
     "user": {
      "displayName": "corentin magyar",
      "photoUrl": "",
      "userId": "04135457892709862388"
     },
     "user_tz": -60
    },
    "id": "ueuhbaJjsYVe"
   },
   "outputs": [],
   "source": [
    "# utils/layers\n",
    "class CharEmbedding(Layer):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, input_len, **kwargs):\n",
    "        \n",
    "        super(CharEmbedding, self).__init__(**kwargs)\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.input_len = input_len\n",
    "        self.char_embed = Embedding(\n",
    "            input_dim = self.input_dim, \n",
    "            output_dim = self.output_dim,  \n",
    "            input_length = self.input_len\n",
    "        )\n",
    "        # This wrapper allows to apply a layer to every temporal slice of an input.\n",
    "        # so we apply the same Embedding to every timestep (index 1) independently\n",
    "        self.timed = TimeDistributed(self.char_embed)\n",
    "        \n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.timed(inputs)\n",
    "            \n",
    "    def get_config(self):\n",
    "\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'input_dim': self.input_dim,\n",
    "            'output_dim': self.output_dim,\n",
    "            'input_len': self.input_len, \n",
    "        })\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "      return cls(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 200421,
     "status": "ok",
     "timestamp": 1613672741593,
     "user": {
      "displayName": "corentin magyar",
      "photoUrl": "",
      "userId": "04135457892709862388"
     },
     "user_tz": -60
    },
    "id": "p8R7tBTZskqP"
   },
   "outputs": [],
   "source": [
    "# utils/layers\n",
    "class CharCNN(Layer):\n",
    "    \n",
    "    def __init__(self, n_filters, filter_width, **kwargs):\n",
    "        \n",
    "        super(CharCNN, self).__init__(**kwargs)\n",
    "        self.n_filters = n_filters\n",
    "        self.filter_width = filter_width\n",
    "        self.conv = Conv1D(self.n_filters, self.filter_width)\n",
    "        self.pool = GlobalMaxPooling1D()\n",
    "        self.timed = TimeDistributed(self.pool)\n",
    "        ## add ReLU activation before max-pooling ?\n",
    "    def build(self, input_shape):\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.timed(self.conv(inputs))\n",
    "    \n",
    "    def get_config(self):\n",
    "\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'n_filters': self.n_filters,\n",
    "            'filter_width': self.filter_width, \n",
    "        })\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "      return cls(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 199067,
     "status": "ok",
     "timestamp": 1613672741594,
     "user": {
      "displayName": "corentin magyar",
      "photoUrl": "",
      "userId": "04135457892709862388"
     },
     "user_tz": -60
    },
    "id": "p36emvbtYtj-"
   },
   "outputs": [],
   "source": [
    "# utils/layers\n",
    "class HighwayNetwork(Layer):\n",
    "    \n",
    "    def __init__(self, hidden_size, **kwargs):\n",
    "        \n",
    "        super(HighwayNetwork, self).__init__(**kwargs)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.normal = Dense(self.hidden_size, activation = 'relu') \n",
    "        self.transform_gate = Dense(self.hidden_size, activation = 'sigmoid')\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs):        \n",
    "        \n",
    "        n = self.normal(inputs)\n",
    "        g = self.transform_gate(inputs)\n",
    "        x = g*n + (1-g)*inputs \n",
    "        return x\n",
    "\n",
    "    def get_config(self):\n",
    "\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'hidden_size': self.hidden_size, \n",
    "        })\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "      return cls(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 197566,
     "status": "ok",
     "timestamp": 1613672741595,
     "user": {
      "displayName": "corentin magyar",
      "photoUrl": "",
      "userId": "04135457892709862388"
     },
     "user_tz": -60
    },
    "id": "2Ci1m5TSYvjQ"
   },
   "outputs": [],
   "source": [
    "# utils/layers\n",
    "class ContextualEmbedding(Layer):\n",
    "    \n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        \n",
    "        super(ContextualEmbedding, self).__init__(**kwargs)\n",
    "        self.output_dim = output_dim\n",
    "        self.contextual = Bidirectional(LSTM(self.output_dim, return_sequences = True, dropout = 0.2))\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.built = True \n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.contextual(inputs)\n",
    "    \n",
    "    def get_config(self):\n",
    "\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'output_dim': self.output_dim,\n",
    "        })\n",
    "        return config\n",
    "    \n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "      return cls(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 196191,
     "status": "ok",
     "timestamp": 1613672741598,
     "user": {
      "displayName": "corentin magyar",
      "photoUrl": "",
      "userId": "04135457892709862388"
     },
     "user_tz": -60
    },
    "id": "58s56NipY-s-"
   },
   "outputs": [],
   "source": [
    "# utils/layers\n",
    "class Modelling(Layer):\n",
    "    \n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        \n",
    "        super(Modelling, self).__init__(**kwargs)\n",
    "        self.output_dim = output_dim\n",
    "        self.modelling1 = Bidirectional(LSTM(self.output_dim, return_sequences = True, dropout = 0.2))\n",
    "        self.modelling2 = Bidirectional(LSTM(self.output_dim, return_sequences = True, dropout = 0.2))\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.modelling2(self.modelling1(inputs))\n",
    "    \n",
    "    def get_config(self):\n",
    "\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'output_dim': self.output_dim,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "      return cls(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 194758,
     "status": "ok",
     "timestamp": 1613672741598,
     "user": {
      "displayName": "corentin magyar",
      "photoUrl": "",
      "userId": "04135457892709862388"
     },
     "user_tz": -60
    },
    "id": "PPIvap9IZCMS"
   },
   "outputs": [],
   "source": [
    "# utils/layers\n",
    "class Start(Layer):\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        \n",
    "        super(Start, self).__init__(**kwargs)\n",
    "        self.dense = Dense(1, activation = 'linear', use_bias = False)\n",
    "        self.dropout = Dropout(0.2)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \n",
    "        GM = inputs\n",
    "        start = self.dense(GM)\n",
    "        start = self.dropout(start)\n",
    "        p1 = tf.nn.softmax(tf.squeeze(start, axis = 2))\n",
    "        return p1\n",
    "\n",
    "    def get_config(self):\n",
    "      \n",
    "      config = super().get_config().copy()\n",
    "      return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "      return cls(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 193396,
     "status": "ok",
     "timestamp": 1613672741599,
     "user": {
      "displayName": "corentin magyar",
      "photoUrl": "",
      "userId": "04135457892709862388"
     },
     "user_tz": -60
    },
    "id": "JumPbG69ZEPf"
   },
   "outputs": [],
   "source": [
    "# utils/layers\n",
    "class ModellingEnd(Layer):\n",
    "    \n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        \n",
    "        super(ModellingEnd, self).__init__(**kwargs)\n",
    "        self.output_dim = output_dim\n",
    "        self.end = Bidirectional(LSTM(self.output_dim, return_sequences = True, dropout = 0.2))\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \n",
    "        G, M = inputs\n",
    "        M2 = self.end(M)\n",
    "        GM2 = tf.concat([G, M2], axis = 2)\n",
    "        return GM2\n",
    "    \n",
    "    def get_config(self):\n",
    "\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'output_dim': self.output_dim,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "      return cls(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 751,
     "status": "ok",
     "timestamp": 1613672742366,
     "user": {
      "displayName": "corentin magyar",
      "photoUrl": "",
      "userId": "04135457892709862388"
     },
     "user_tz": -60
    },
    "id": "4HdLF0UTZGRx"
   },
   "outputs": [],
   "source": [
    "# utils/layers\n",
    "class End(Layer):\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        \n",
    "        super(End, self).__init__(**kwargs)\n",
    "        self.dense = Dense(1, activation = 'linear', use_bias = False)\n",
    "        self.dropout = Dropout(0.2)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \n",
    "        GM2 = inputs\n",
    "        end = self.dense(GM2)\n",
    "        end = self.dropout(end)\n",
    "        p2 = tf.nn.softmax(tf.squeeze(end, axis = 2))\n",
    "        \n",
    "        return p2\n",
    "\n",
    "\n",
    "    def get_config(self):\n",
    "\n",
    "      config = super().get_config().copy()\n",
    "\n",
    "      return config\n",
    "    \n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "      return cls(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 1315,
     "status": "ok",
     "timestamp": 1613672742938,
     "user": {
      "displayName": "corentin magyar",
      "photoUrl": "",
      "userId": "04135457892709862388"
     },
     "user_tz": -60
    },
    "id": "3oj0lLdm_FmX"
   },
   "outputs": [],
   "source": [
    "# utils/models\n",
    "class BIDAF(Model):\n",
    "\n",
    "  \"\"\"\n",
    "  the BIDAF model\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, \n",
    "               question_maxlen, \n",
    "               context_maxlen, \n",
    "               word_vocab_len, \n",
    "               embedding_size, \n",
    "               embedding_matrix, \n",
    "               char_vocab_len,\n",
    "               word_maxlen, \n",
    "               n_filters, \n",
    "               filter_size, \n",
    "               char_embedding_size,\n",
    "               word_tokenizer_path,\n",
    "               char_tokenizer_path,\n",
    "               **kwargs):\n",
    "    \n",
    "    \n",
    "    super(BIDAF, self).__init__(name = 'BIDAF', **kwargs)\n",
    "\n",
    "    self.question_maxlen = question_maxlen\n",
    "    self.context_maxlen = context_maxlen\n",
    "    self.word_vocab_len = word_vocab_len\n",
    "    self.embedding_size = embedding_size\n",
    "    self.embedding_matrix = embedding_matrix\n",
    "    self.char_vocab_len = char_vocab_len\n",
    "    self.char_embedding_size = char_embedding_size\n",
    "    self.word_maxlen = word_maxlen\n",
    "    self.n_filters = n_filters\n",
    "    self.filter_size = filter_size\n",
    "\n",
    "    with open(word_tokenizer_path, 'rb') as handle:\n",
    "      self.word_tokenizer = pickle.load(handle)\n",
    "\n",
    "    with open(char_tokenizer_path, 'rb') as handle:\n",
    "      self.char_tokenizer = pickle.load(handle)\n",
    "\n",
    "    self.similarity_weights = Dense(1, use_bias = False)\n",
    "\n",
    "    # layers\n",
    "    self.word_embedding = WordEmbedding(self.word_vocab_len, self.embedding_size, self.question_maxlen, self.embedding_matrix)\n",
    "    self.char_embedding = CharEmbedding(self.char_vocab_len, self.char_embedding_size, self.word_maxlen)\n",
    "    self.cnn = CharCNN(self.n_filters, self.filter_size)\n",
    "    self.highway = HighwayNetwork(hidden_size = self.embedding_size + self.n_filters)\n",
    "    self.contextual = ContextualEmbedding(self.embedding_size)\n",
    "    self.modelling = Modelling(self.embedding_size)\n",
    "    self.modelling_end = ModellingEnd(self.embedding_size)\n",
    "    self.output_start = Start()\n",
    "    self.ouput_end = End()\n",
    "\n",
    "  def _get_tokens(self):\n",
    "\n",
    "    self.question = self.word_tokenizer.texts_to_sequences([self._question])\n",
    "    self.context = self.word_tokenizer.texts_to_sequences([self._context])\n",
    "    self.context_ids = self.context\n",
    "\n",
    "  def _get_padded_sequences(self):\n",
    "\n",
    "    self.question = tf.keras.preprocessing.sequence.pad_sequences(self.question, maxlen = self.question_maxlen, padding = 'post', truncating = 'post')\n",
    "    self.context = tf.keras.preprocessing.sequence.pad_sequences(self.context, maxlen = self.context_maxlen, padding = 'post', truncating = 'post')\n",
    "\n",
    "  def make_prediction(self, question, context):\n",
    "\n",
    "    self._question = word_tokenize(question)\n",
    "    self._context = word_tokenize(context)\n",
    "\n",
    "    self._get_tokens()\n",
    "    self._get_padded_sequences()\n",
    "\n",
    "    self.__get_tokens()\n",
    "    self.__get_padded_sequences()\n",
    "\n",
    "    start, end = self.predict([\n",
    "                      self.question,\n",
    "                      self.context,\n",
    "                      self.question_char,\n",
    "                      self.context_char\n",
    "                ])\n",
    "    \n",
    "    start = start.argmax()\n",
    "    end = end.argmax() + 1\n",
    "\n",
    "    if start > end:\n",
    "      start = end\n",
    "      end = start\n",
    "\n",
    "    answer = ''\n",
    "\n",
    "    for i in range(start, end):\n",
    "      answer += self.word_tokenizer.index_word[self.context_ids[0][i]] + ' '\n",
    "    return answer.strip()\n",
    "\n",
    "  def multi_predictions(self, datasets, path):\n",
    "    predictions = {}\n",
    "\n",
    "    for dataset in datasets:\n",
    "\n",
    "      for batch in dataset:\n",
    "\n",
    "        if len(batch) == 3:\n",
    "          sequences = batch[0]\n",
    "          id = batch[2][0].tolist()\n",
    "        else:\n",
    "          sequences = batch[0]\n",
    "          id = batch[1][0].tolist()          \n",
    "\n",
    "        qw, cw, qc, cc = sequences\n",
    "\n",
    "        start, end = self.predict([qw, cw, qc, cc])\n",
    "\n",
    "        start = start.argmax(axis = 1)\n",
    "        end = end.argmax(axis = 1)\n",
    "\n",
    "        answers = []\n",
    "\n",
    "        for idx, (s, e) in enumerate(zip(start, end)):\n",
    "          if s > e:\n",
    "            s = e\n",
    "            e = s\n",
    "\n",
    "          answer = ''\n",
    "          for i in range(s,e):\n",
    "            answer += self.word_tokenizer.index_word[cw[idx][i]] + ' '\n",
    "          answers.append(answer.strip())\n",
    "        \n",
    "        predictions.update({i.strip(): a for i,a in zip(id, answers)})\n",
    "    \n",
    "    with open(path, 'w') as handle:\n",
    "      json.dump(predictions, handle)\n",
    "\n",
    "    print(f' the file containing the predictions has been created in {path}')\n",
    "    \n",
    "  def __get_tokens(self):\n",
    "\n",
    "    self.question_char = self.char_tokenizer.texts_to_sequences(self._question)\n",
    "    self.context_char = self.char_tokenizer.texts_to_sequences(self._context)\n",
    "\n",
    "  def __get_padded_sequences(self):\n",
    "\n",
    "    # pad question at the character level\n",
    "    v = tf.keras.preprocessing.sequence.pad_sequences(self.question_char, padding = 'post', truncating = 'post', maxlen = self.word_maxlen)\n",
    "    to_add = self.question_maxlen - v.shape[0]\n",
    "    add = np.zeros((to_add, self.word_maxlen))\n",
    "    arr = np.vstack([v,add])\n",
    "    self.question_char = arr\n",
    "\n",
    "    # pad context at the character level\n",
    "    v = tf.keras.preprocessing.sequence.pad_sequences(self.context_char, padding = 'post', truncating = 'post', maxlen = self.word_maxlen)\n",
    "    to_add = self.context_maxlen - v.shape[0]\n",
    "    add = np.zeros((to_add, self.word_maxlen))\n",
    "    arr = np.vstack([v,add])\n",
    "    self.context_char = arr\n",
    "\n",
    "    self.question_char = tf.expand_dims(self.question_char, axis = 0)\n",
    "    self.context_char = tf.expand_dims(self.context_char, axis = 0)\n",
    "\n",
    "\n",
    "  def call(self, inputs, training = True):\n",
    "    qw, cw, qc, cc = inputs  # (bs, q_len), (bs, ctx_len), (bs, q_len, w_len), (bs, ctx_len, w_len)\n",
    "\n",
    "    # embedding always non-trainable\n",
    "    qw = self.word_embedding(qw) # (bs, q_len, emb)\n",
    "    cw = self.word_embedding(cw) # (bs, ctx_len, emb)\n",
    "\n",
    "    qc = self.char_embedding(qc) # (bs, q_len, w_len, char_emb)\n",
    "    cc = self.char_embedding(cc) # (bs, ctx_len, w_len, char_emb)\n",
    "\n",
    "    qc = self.cnn(qc) # (bs, q_len, n_filters)\n",
    "    cc = self.cnn(cc) # (bs, ctx_len, n_filters)\n",
    "\n",
    "    H = tf.concat([cw, cc], axis = 2) # (bs, ctx_len, emb + n_filters)\n",
    "    U = tf.concat([qw, qc], axis = 2) # (bs, q_len, emb + n_filters)\n",
    "\n",
    "    # highway\n",
    "    H = self.highway(H) # (bs, ctx_len, emb + n_filters)\n",
    "    U = self.highway(U) # (bs, q_len, emb + n_filters)\n",
    "\n",
    "    # contextual embedding\n",
    "    H = self.contextual(H) # (bs, ctx_len, emb + n_filters)\n",
    "    U = self.contextual(U) # (bs, q_len, emb + n_filters)\n",
    "\n",
    "    # similarity matrix\n",
    "    expand_h = tf.concat([[1, 1], [tf.shape(U)[1]], [1]], axis = 0) # [1, 1, q_len, 1]\n",
    "    expand_u = tf.concat([[1], [tf.shape(H)[1]], [1, 1]], axis = 0) # [1, ctx_len, 1, 1]\n",
    "\n",
    "    h = tf.tile(tf.expand_dims(H, axis = 2), expand_h) # (bs, ctx_len, q_len, emb + n_filters)\n",
    "    u = tf.tile(tf.expand_dims(U, axis = 1), expand_u) # (bs, ctx_len, q_len, emb + n_filters)\n",
    "    h_u = h * u # (bs, ctx_len, q_len, emb + n_filters)\n",
    "\n",
    "    alpha = tf.concat([h, u, h_u], axis = -1) # (bs, ctx_len, q_len, 3 * (emb + n_filters))\n",
    "    \n",
    "    similarity_matrix = self.similarity_weights(alpha) # (bs, ctx_len, q_len, 1)\n",
    "    similarity_matrix = tf.squeeze(similarity_matrix, 3) # (bs, ctx_len, q_len)\n",
    "\n",
    "    # context to query attention\n",
    "    attention_weights = tf.nn.softmax(similarity_matrix, axis = -1) # (bs, ctx_len, q_len)\n",
    "    C2Q = K.batch_dot(attention_weights, U) # (bs, ctx_len, emb + n_filters)\n",
    "\n",
    "    # query to context attention\n",
    "    attention_weights = tf.nn.softmax(tf.math.reduce_max(similarity_matrix, axis = 2), axis = -1) # (bs, ctx_len)\n",
    "    attention_weights = tf.expand_dims(attention_weights, axis = 1) # (bs, 1, ctx_len)\n",
    "    Q2C = K.batch_dot(attention_weights, H) # (bs, 1, emb + n_filters)\n",
    "    Q2C = tf.tile(Q2C, [1, tf.shape(H)[1], 1]) # (bs, ctx_len, emb + n_filters)\n",
    "\n",
    "    # query aware representation\n",
    "    G = tf.concat([H, C2Q, (H * C2Q), (H * Q2C)], axis = 2) # (bs, ctx_len, 4 * (emb + n_filters) )\n",
    "\n",
    "    # modelling\n",
    "    M = self.modelling(G) # (bs, ctx_len, emb + n_filters)\n",
    "\n",
    "    # output\n",
    "    M2 = self.modelling_end([G,M]) # (bs, ctx_len, emb + n_filters)\n",
    "\n",
    "    # start prediction\n",
    "    start = self.output_start(tf.concat([G, M], axis = 2)) # (bs, ctx_len)\n",
    "\n",
    "    # end prediction\n",
    "    end = self.ouput_end(M2) # (bs, ctx_len)\n",
    "\n",
    "    return start, end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 7339,
     "status": "ok",
     "timestamp": 1613672748969,
     "user": {
      "displayName": "corentin magyar",
      "photoUrl": "",
      "userId": "04135457892709862388"
     },
     "user_tz": -60
    },
    "id": "QKicRfIW6tUm"
   },
   "outputs": [],
   "source": [
    "bidaf_model = BIDAF(\n",
    "    QUESTION_MAXLEN,\n",
    "    CONTEXT_MAXLEN,\n",
    "    WORD_VOCAB_LEN,\n",
    "    EMBEDDING_SIZE,\n",
    "    embedding_matrix,\n",
    "    CHAR_VOCAB_LEN,\n",
    "    WORD_MAXLEN,\n",
    "    N_FILTERS,\n",
    "    FILTER_SIZE,\n",
    "    CHAR_EMBEDDING_SIZE,\n",
    "    path_word_tokenizer,\n",
    "    path_char_tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 7343,
     "status": "ok",
     "timestamp": 1613672748977,
     "user": {
      "displayName": "corentin magyar",
      "photoUrl": "",
      "userId": "04135457892709862388"
     },
     "user_tz": -60
    },
    "id": "krJp-At4b0y_"
   },
   "outputs": [],
   "source": [
    "loss_function = tf.keras.losses.CategoricalCrossentropy(reduction = 'auto')\n",
    "optimizer = tf.keras.optimizers.Nadam(learning_rate = LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 7338,
     "status": "ok",
     "timestamp": 1613672748977,
     "user": {
      "displayName": "corentin magyar",
      "photoUrl": "",
      "userId": "04135457892709862388"
     },
     "user_tz": -60
    },
    "id": "kg2EI6tBcbB8"
   },
   "outputs": [],
   "source": [
    "# https://udai.gitbook.io/practical-ml/nn/training-and-debugging-of-nn <- useful blog about machine learning / deep learning\n",
    "# steps to be performed in each training step\n",
    "@tf.function\n",
    "def train_step(model, input_vector, output_vector, loss_fn):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # forward propagation\n",
    "        output_predicted = model(input_vector, training = True)\n",
    "        # loss\n",
    "        loss_start = loss_function(output_vector[0], output_predicted[0])\n",
    "        loss_end = loss_function(output_vector[1], output_predicted[1])\n",
    "        loss_final = loss_start + loss_end\n",
    "    # getting gradients\n",
    "    gradients = tape.gradient(loss_final, model.trainable_variables)\n",
    "    # applying gradients\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss_start, loss_end, output_predicted, gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 7334,
     "status": "ok",
     "timestamp": 1613672748978,
     "user": {
      "displayName": "corentin magyar",
      "photoUrl": "",
      "userId": "04135457892709862388"
     },
     "user_tz": -60
    },
    "id": "hTNI3WKaciDu"
   },
   "outputs": [],
   "source": [
    "# https://udai.gitbook.io/practical-ml/nn/training-and-debugging-of-nn\n",
    "# steps to be performed in each validation step\n",
    "@tf.function\n",
    "def val_step(model, input_vector, output_vector, loss_fn):\n",
    "    # getting output of validation data\n",
    "    output_predicted = model(input_vector, training = False)\n",
    "    # loss calculation\n",
    "    loss_start = loss_function(output_vector[0], output_predicted[0])\n",
    "    loss_end = loss_function(output_vector[1], output_predicted[1])\n",
    "    return loss_start, loss_end, output_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 7329,
     "status": "ok",
     "timestamp": 1613672748978,
     "user": {
      "displayName": "corentin magyar",
      "photoUrl": "",
      "userId": "04135457892709862388"
     },
     "user_tz": -60
    },
    "id": "6on5ZTXwZING"
   },
   "outputs": [],
   "source": [
    "def f1_score(y_true, y_pred):    # taken from old keras source code\n",
    "    \n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    \n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    \n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    \n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    \n",
    "    f1_val = 2 * (precision * recall) / (precision + recall + K.epsilon())\n",
    "    \n",
    "    return f1_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 7326,
     "status": "ok",
     "timestamp": 1613672748979,
     "user": {
      "displayName": "corentin magyar",
      "photoUrl": "",
      "userId": "04135457892709862388"
     },
     "user_tz": -60
    },
    "id": "hmKsimsMcoHu"
   },
   "outputs": [],
   "source": [
    "# defining functions to compute the mean loss for each epoch\n",
    "train_start_loss = tf.keras.metrics.Mean(name = 'train_start_loss')\n",
    "train_end_loss = tf.keras.metrics.Mean(name = 'train_end_loss')\n",
    "val_start_loss = tf.keras.metrics.Mean(name = 'val_start_loss')\n",
    "val_end_loss = tf.keras.metrics.Mean(name = 'val_end_loss')\n",
    "train_start_f1 = tf.keras.metrics.Mean(name = 'train_start_f1')\n",
    "train_end_f1 = tf.keras.metrics.Mean(name = 'train_end_f1')\n",
    "val_start_f1 = tf.keras.metrics.Mean(name = 'val_start_f1')\n",
    "val_end_f1 = tf.keras.metrics.Mean(name = 'val_end_f1')\n",
    "train_start_acc = tf.keras.metrics.CategoricalAccuracy(name = 'train_start_acc')\n",
    "train_end_acc = tf.keras.metrics.CategoricalAccuracy(name = 'train_end_acc')\n",
    "val_start_acc = tf.keras.metrics.CategoricalAccuracy(name = 'val_start_acc')\n",
    "val_end_acc = tf.keras.metrics.CategoricalAccuracy(name = 'val_end_acc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "executionInfo": {
     "elapsed": 7323,
     "status": "ok",
     "timestamp": 1613672748981,
     "user": {
      "displayName": "corentin magyar",
      "photoUrl": "",
      "userId": "04135457892709862388"
     },
     "user_tz": -60
    },
    "id": "Qp9ojYdzUzwG"
   },
   "outputs": [],
   "source": [
    "best_loss = 100 # we initialize a loss value for model checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1836,
     "status": "ok",
     "timestamp": 1613672755922,
     "user": {
      "displayName": "corentin magyar",
      "photoUrl": "",
      "userId": "04135457892709862388"
     },
     "user_tz": -60
    },
    "id": "KGoXLG59p5Jy",
    "outputId": "3f6a7442-bcdd-4fb2-9e7b-f2226003776b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x19f743e62e0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# don't run the next cell if your model is already trained\n",
    "# don't run this cell if your model need to be trained but run the next one\n",
    "bidaf_model.load_weights('../utils/models/weights/bidaf_weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5fOKYFlzcwiW",
    "outputId": "25341793-fada-4de5-fb19-16f3af04aa6d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6961/6961 [44:21<00:00,  2.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Start Loss: 3.715324, Train Start Acc : 0.21323, Start F1 Score: 0.10070, Train End Loss: 3.507812, Train End Acc : 0.23734, End F1 Score: 0.11095,\n",
      "    Val Start Loss: 2.235232, Val Start Acc : 0.43508, Val Start F1 Score: 0.28522, Val End Loss: 2.044395, Val End Acc : 0.46603, Val End F1 Score: 0.31757\n",
      "Saving weights...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/6961 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Done !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6961/6961 [43:44<00:00,  2.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Train Start Loss: 2.681711, Train Start Acc : 0.42797, Start F1 Score: 0.37533, Train End Loss: 2.526541, Train End Acc : 0.45785, End F1 Score: 0.41619,\n",
      "    Val Start Loss: 1.780379, Val Start Acc : 0.53696, Val Start F1 Score: 0.44636, Val End Loss: 1.579908, Val End Acc : 0.57331, Val End F1 Score: 0.52164\n",
      "Saving weights...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/6961 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Done !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6961/6961 [43:42<00:00,  2.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Train Start Loss: 2.409225, Train Start Acc : 0.48303, Start F1 Score: 0.45983, Train End Loss: 2.260953, Train End Acc : 0.51763, End F1 Score: 0.50738,\n",
      "    Val Start Loss: 1.675886, Val Start Acc : 0.55200, Val Start F1 Score: 0.49601, Val End Loss: 1.510953, Val End Acc : 0.58657, Val End F1 Score: 0.55100\n",
      "Saving weights...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/6961 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Done !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6961/6961 [43:38<00:00,  2.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Train Start Loss: 2.222225, Train Start Acc : 0.52494, Start F1 Score: 0.51462, Train End Loss: 2.092869, Train End Acc : 0.55336, End F1 Score: 0.55884,\n",
      "    Val Start Loss: 1.646435, Val Start Acc : 0.55774, Val Start F1 Score: 0.52987, Val End Loss: 1.514827, Val End Acc : 0.59433, Val End F1 Score: 0.58174\n",
      "Saving weights...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/6961 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Done !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6961/6961 [43:52<00:00,  2.64it/s]\n",
      "  0%|          | 0/6961 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Train Start Loss: 2.065901, Train Start Acc : 0.55566, Start F1 Score: 0.55514, Train End Loss: 1.943553, Train End Acc : 0.58259, End F1 Score: 0.59834,\n",
      "    Val Start Loss: 1.676296, Val Start Acc : 0.55545, Val Start F1 Score: 0.53732, Val End Loss: 1.542550, Val End Acc : 0.58778, Val End F1 Score: 0.57692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6961/6961 [43:41<00:00,  2.66it/s]\n",
      "  0%|          | 0/6961 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Train Start Loss: 1.927224, Train Start Acc : 0.58334, Start F1 Score: 0.59266, Train End Loss: 1.814614, Train End Acc : 0.61016, End F1 Score: 0.63235,\n",
      "    Val Start Loss: 1.717615, Val Start Acc : 0.55085, Val Start F1 Score: 0.54641, Val End Loss: 1.594208, Val End Acc : 0.59122, Val End F1 Score: 0.58632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6961/6961 [43:36<00:00,  2.66it/s]\n",
      "  0%|          | 0/6961 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Train Start Loss: 1.831880, Train Start Acc : 0.60262, Start F1 Score: 0.61802, Train End Loss: 1.711476, Train End Acc : 0.63170, End F1 Score: 0.65915,\n",
      "    Val Start Loss: 1.758022, Val Start Acc : 0.55453, Val Start F1 Score: 0.55285, Val End Loss: 1.607760, Val End Acc : 0.58761, Val End F1 Score: 0.58862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▋        | 1146/6961 [07:11<36:28,  2.66it/s]"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    # resetting the states of the loss and metrics\n",
    "    train_start_loss.reset_states()\n",
    "    train_end_loss.reset_states()\n",
    "    val_start_loss.reset_states()\n",
    "    val_end_loss.reset_states()\n",
    "    train_start_f1.reset_states()\n",
    "    train_end_f1.reset_states()\n",
    "    val_start_f1.reset_states()\n",
    "    val_end_f1.reset_states()\n",
    "    train_start_acc.reset_states()\n",
    "    train_end_acc.reset_states()\n",
    "    val_start_acc.reset_states()\n",
    "    val_end_acc.reset_states()\n",
    "    \n",
    "    # iterating over train data batch by batch\n",
    "    for text_seq, label_seq, _ in tqdm(iterable = train_dataset, total = len(train_dataset)):\n",
    "        # train step\n",
    "        loss_start_, loss_end_, pred_out, gradients = train_step(bidaf_model, text_seq, label_seq, loss_function)\n",
    "        # adding loss to train loss\n",
    "        train_start_loss(loss_start_)\n",
    "        train_end_loss(loss_end_)\n",
    "        \n",
    "        # calculating f1 for batch\n",
    "        f1_start = f1_score(label_seq[0], pred_out[0])\n",
    "        f1_end = f1_score(label_seq[1], pred_out[1])\n",
    "        train_start_f1(f1_start)\n",
    "        train_end_f1(f1_end)\n",
    "        train_start_acc(label_seq[0], pred_out[0])\n",
    "        train_end_acc(label_seq[1], pred_out[1])\n",
    "    \n",
    "    # validation data\n",
    "    for text_seq_val, label_seq_val, _ in valid_dataset:\n",
    "        # getting val output\n",
    "        loss_val_start, loss_val_end, pred_out_val = val_step(bidaf_model, text_seq_val, label_seq_val, loss_function)\n",
    "        \n",
    "        val_start_loss(loss_val_start)\n",
    "        val_end_loss(loss_val_end)\n",
    "        \n",
    "        # calculating metric\n",
    "        f1_start_val = f1_score(label_seq_val[0], pred_out_val[0])\n",
    "        f1_end_val = f1_score(label_seq_val[1], pred_out_val[1])\n",
    "        val_start_f1(f1_start_val)\n",
    "        val_end_f1(f1_end_val)\n",
    "        val_start_acc(label_seq_val[0], pred_out_val[0])\n",
    "        val_end_acc(label_seq_val[1], pred_out_val[1])\n",
    "    \n",
    "   \n",
    "    # printing\n",
    "    template = '''Epoch {}, Train Start Loss: {:0.6f}, Train Start Acc : {:0.5f}, Start F1 Score: {:0.5f}, Train End Loss: {:0.6f}, Train End Acc : {:0.5f}, End F1 Score: {:0.5f},\n",
    "    Val Start Loss: {:0.6f}, Val Start Acc : {:0.5f}, Val Start F1 Score: {:0.5f}, Val End Loss: {:0.6f}, Val End Acc : {:0.5f}, Val End F1 Score: {:0.5f}'''\n",
    "\n",
    "    print(template.format(epoch + 1, train_start_loss.result(), train_start_acc.result(), train_start_f1.result(), \n",
    "                          train_end_loss.result(), train_end_acc.result(), train_end_f1.result(),\n",
    "                          val_start_loss.result(), val_start_acc.result(), val_start_f1.result(),\n",
    "                          val_end_loss.result(), val_end_acc.result(), val_end_f1.result()))\n",
    "\n",
    "\n",
    "    if (val_start_loss.result() + val_end_loss.result()) < best_loss:\n",
    "      print('Saving weights...')\n",
    "      bidaf_model.save_weights('../utils/models/weights/bidaf_weights')\n",
    "      print('\\n Done !')\n",
    "      best_loss = (val_start_loss.result() + val_end_loss.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "executionInfo": {
     "elapsed": 545,
     "status": "ok",
     "timestamp": 1613672763217,
     "user": {
      "displayName": "corentin magyar",
      "photoUrl": "",
      "userId": "04135457892709862388"
     },
     "user_tz": -60
    },
    "id": "wygC27fedajx"
   },
   "outputs": [],
   "source": [
    "def print_predictions(batch):\n",
    "\n",
    "  \"\"\"\n",
    "  utility function to visualize some predictions\n",
    "  \"\"\"\n",
    "\n",
    "  idx = np.random.randint(BATCH_SIZE)\n",
    "  samples = valid_dataset[batch]\n",
    "\n",
    "  sequences, labels, _ = samples\n",
    "\n",
    "  qw = sequences[0][idx]\n",
    "  cw = sequences[1][idx]\n",
    "  qc = sequences[2][idx]\n",
    "  cc = sequences[3][idx]\n",
    "\n",
    "  real_start = labels[0][idx]\n",
    "  real_end = labels[1][idx]\n",
    "\n",
    "  \"\"\"\n",
    "  Function that takes record numbers as input and predicts the answer for that record\n",
    "  \"\"\"\n",
    "\n",
    "  print('Question:')\n",
    "  for i in qw:\n",
    "    if i == 0:\n",
    "      break\n",
    "    else:\n",
    "      print(tokenizer.index_word[i], end = ' ')\n",
    "\n",
    "  print('\\nContext:')\n",
    "  for i in cw:\n",
    "    if i == 0:\n",
    "      break\n",
    "    else:\n",
    "      print(tokenizer.index_word[i], end = ' ')\n",
    "      \n",
    "  print('\\nPredicted Answer:')\n",
    "  _qw = qw.reshape(1, qw.shape[0])\n",
    "  _cw = cw.reshape(1, cw.shape[0])\n",
    "  _qc = np.expand_dims(qc, axis = 0)\n",
    "  _cc = np.expand_dims(cc, axis = 0)\n",
    "  start, end = bidaf_model.predict((_qw, _cw, _qc, _cc))\n",
    "  start = start.argmax()\n",
    "  end = end.argmax() + 1\n",
    "\n",
    "  if start > end:\n",
    "    start = end\n",
    "    end = start\n",
    "\n",
    "  for i in range(start, end ):\n",
    "    print(tokenizer.index_word[cw[i]], end = ' ')\n",
    "  print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20716,
     "status": "ok",
     "timestamp": 1613672785174,
     "user": {
      "displayName": "corentin magyar",
      "photoUrl": "",
      "userId": "04135457892709862388"
     },
     "user_tz": -60
    },
    "id": "2TOn5DDbddfq",
    "outputId": "3af5ead4-ffe5-4d1e-b7dd-07f9dda0229a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\n",
      "in how many scenarios will sydney remain higher than melbourne in population beyond 2056 ? \n",
      "Context:\n",
      "in recent years , melton , wyndham and casey , part of the melbourne statistical division , have recorded the highest growth rate of all local government areas in australia . melbourne could overtake sydney in population by 2028 , the abs has projected in two scenarios that sydney will remain larger than melbourne beyond 2056 , albeit by a margin of less than 3 % compared to a margin of 12 % today . melbourne 's population could overtake that of sydney by 2037 or 2039 , according to the first scenario projected by the abs ; primarily due to larger levels of internal migration losses assumed for sydney . another study claims that melbourne will surpass sydney in population by 2040 . \n",
      "Predicted Answer:\n",
      "two scenarios \n",
      "\n",
      "Question:\n",
      "who owned the rights to oswald ? \n",
      "Context:\n",
      "universal owned the rights to the `` oswald the lucky rabbit '' character , although walt disney and ub iwerks had created oswald , and their films had enjoyed a successful theatrical run . after charles mintz had unsuccessfully demanded that disney accept a lower fee for producing the property , mintz produced the films with his own group of animators . instead , disney and iwerks created mickey mouse who in 1928 stared in the first `` sync '' sound animated short , steamboat willie . this moment effectively launched walt disney studios ' foothold , while universal became a minor player in film animation . universal subsequently severed its link to mintz and formed its own in-house animation studio to produce oswald cartoons headed by walter lantz . \n",
      "Predicted Answer:\n",
      "universal owned \n",
      "\n",
      "Question:\n",
      "in what month each year is the southampton boat show held ? \n",
      "Context:\n",
      "the annual southampton boat show is held in september each year , with over 600 exhibitors present . it runs for just over a week at mayflower park on the city 's waterfront , where it has been held since 1968. the boat show itself is the climax of sea city , which runs from april to september each year to celebrate southampton 's links with the sea . \n",
      "Predicted Answer:\n",
      "september each \n",
      "\n",
      "Question:\n",
      "the foremost part of the brain in mammals is known as what ? \n",
      "Context:\n",
      "the first vertebrates appeared over 500 million years ago ( mya ) , during the cambrian period , and may have resembled the modern hagfish in form . sharks appeared about 450 mya , amphibians about 400 mya , reptiles about 350 mya , and mammals about 200 mya . each species has an equally long evolutionary history , but the brains of modern hagfishes , lampreys , sharks , amphibians , reptiles , and mammals show a gradient of size and complexity that roughly follows the evolutionary sequence . all of these brains contain the same set of basic anatomical components , but many are rudimentary in the hagfish , whereas in mammals the foremost part ( the telencephalon ) is greatly elaborated and expanded . \n",
      "Predicted Answer:\n",
      "the telencephalon ) \n",
      "\n",
      "Question:\n",
      "how many awards was beyonce nominated for at the 52nd grammy awards ? \n",
      "Context:\n",
      "at the 52nd annual grammy awards , beyoncé received ten nominations , including album of the year for i am ... sasha fierce , record of the year for `` halo '' , and song of the year for `` single ladies ( put a ring on it ) '' , among others . she tied with lauryn hill for most grammy nominations in a single year by a female artist . in 2010 , beyoncé was featured on lady gaga 's single `` telephone '' and its music video . the song topped the us pop songs chart , becoming the sixth number-one for both beyoncé and gaga , tying them with mariah carey for most number-ones since the nielsen top 40 airplay chart launched in 1992 . `` telephone '' received a grammy award nomination for best pop collaboration with vocals . \n",
      "Predicted Answer:\n",
      "ten nominations \n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_points = [8,15,52,152,332]\n",
    "for i in data_points:\n",
    "  print_predictions(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "executionInfo": {
     "elapsed": 643,
     "status": "ok",
     "timestamp": 1613672785828,
     "user": {
      "displayName": "corentin magyar",
      "photoUrl": "",
      "userId": "04135457892709862388"
     },
     "user_tz": -60
    },
    "id": "E3IzPCNqEO0j"
   },
   "outputs": [],
   "source": [
    "question = 'In what country is Normandy located?'\n",
    "context = \"The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse ('Norman' comes from 'Norseman') raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "executionInfo": {
     "elapsed": 3634,
     "status": "ok",
     "timestamp": 1613672788849,
     "user": {
      "displayName": "corentin magyar",
      "photoUrl": "",
      "userId": "04135457892709862388"
     },
     "user_tz": -60
    },
    "id": "DlbVjaSzbXH4",
    "outputId": "166e6438-0307-4b8e-eb93-a0f4cc75879c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'france .'"
      ]
     },
     "execution_count": 31,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bidaf_model.make_prediction(question,context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 302369,
     "status": "ok",
     "timestamp": 1613673093998,
     "user": {
      "displayName": "corentin magyar",
      "photoUrl": "",
      "userId": "04135457892709862388"
     },
     "user_tz": -60
    },
    "id": "O5yHWVgHzBA9",
    "outputId": "be4af163-d8ff-46f2-be81-3f036c4a58bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the file containing the predictions has been created in /content/drive/MyDrive/NLP/BIDAF/utils/data/predictions.json\n"
     ]
    }
   ],
   "source": [
    "path_save_validation = os.path.abspath('../utils/data/predictions.json')\n",
    "bidaf_model.multi_predictions([valid_dataset], path_save_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4663,
     "status": "ok",
     "timestamp": 1613673111360,
     "user": {
      "displayName": "corentin magyar",
      "photoUrl": "",
      "userId": "04135457892709862388"
     },
     "user_tz": -60
    },
    "id": "saD8cnRcA6tR",
    "outputId": "7f6fd844-5a3a-43dd-ed42-f9d319046d9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"exact\": 51.07678171481077,\n",
      "  \"f1\": 65.8105272912672,\n",
      "  \"total\": 17413,\n",
      "  \"HasAns_exact\": 51.07678171481077,\n",
      "  \"HasAns_f1\": 65.8105272912672,\n",
      "  \"HasAns_total\": 17413\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# the evaluate.py file can be downloaded on the SQUAD website https://rajpurkar.github.io/SQuAD-explorer/\n",
    "!python3 drive/MyDrive/NLP/BIDAF/utils/data/evaluate.py drive/MyDrive/NLP/BIDAF/utils/data/valid_set.json drive/MyDrive/NLP/BIDAF/utils/data/predictions.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OiPqRy_BVoBg"
   },
   "source": [
    "With the (mock) unseen dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "executionInfo": {
     "elapsed": 17649,
     "status": "ok",
     "timestamp": 1613673140166,
     "user": {
      "displayName": "corentin magyar",
      "photoUrl": "",
      "userId": "04135457892709862388"
     },
     "user_tz": -60
    },
    "id": "tL5IXVJXMEr2"
   },
   "outputs": [],
   "source": [
    "unseen_data = SQUAD_dataset.from_file('../utils/datasets/unseen_dataset.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1515246,
     "status": "ok",
     "timestamp": 1613674661570,
     "user": {
      "displayName": "corentin magyar",
      "photoUrl": "",
      "userId": "04135457892709862388"
     },
     "user_tz": -60
    },
    "id": "0HfleTvjMYR_",
    "outputId": "8dfff964-45aa-4314-cb61-6e7d3c99c7b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the file containing the predictions has been created in /content/drive/MyDrive/NLP/BIDAF/utils/data/unseen_predictions.json\n"
     ]
    }
   ],
   "source": [
    "path_save_unseen = os.path.abspath('../utils/data/unseen_predictions.json')\n",
    "bidaf_model.multi_predictions([unseen_data], path_save_unseen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F9Sxh-VBWXOj"
   },
   "source": [
    "**FURTHER WORK**:\n",
    "* try with GRU instead of LSTM (GRU are usually faster)\n",
    "* make batches with different padding size (so far, `CONTEXT_MAXLEN`, `WORD_MAXLEN` and `QUESTION_MAXLEN` are the same for each batch, while we could create local variables for each batch )\n",
    "* try others models (QANet, BERT,  Multi-Perspective Context Matching, ... )\n",
    "* try different initialization methods of the char_embedding matrix"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyP36zn/4V7bID8INhJa5sbF",
   "collapsed_sections": [],
   "mount_file_id": "1E8UcRYR1BlmgRH_ipWm2paY2qj-IJ_6Q",
   "name": "bidaf_model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
