{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of bidaf_preprocessing.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"VrXpyz2vZhs1"},"source":["In order to run this notebook, first things you should do are :\r\n","* mount your drive endpoint\r\n","* go at the end and modify paths ( i don't know if numpy.save() also create missing folders )\r\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TYSqjlxxzWK8","executionInfo":{"status":"ok","timestamp":1611834186743,"user_tz":-60,"elapsed":28748,"user":{"displayName":"Nicola Amoriello","photoUrl":"","userId":"12946999478101005836"}},"outputId":"18a1833f-a278-453d-fc14-431f40c061c0"},"source":["from google.colab import drive\r\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hETK3Axg43yB","executionInfo":{"status":"ok","timestamp":1611834203335,"user_tz":-60,"elapsed":4752,"user":{"displayName":"Nicola Amoriello","photoUrl":"","userId":"12946999478101005836"}},"outputId":"8ef0965c-1bf7-402b-c10a-e969dfd3bdd9"},"source":["import json\r\n","import os\r\n","import pandas as pd\r\n","import numpy as np\r\n","import tensorflow as tf\r\n","import nltk\r\n","from nltk import word_tokenize\r\n","nltk.download('punkt')\r\n","import gensim.downloader as gloader\r\n","from sklearn.model_selection import train_test_split\r\n","import re\r\n","import pickle"],"execution_count":2,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"VqLS5R574tdD","executionInfo":{"status":"ok","timestamp":1611827646443,"user_tz":-60,"elapsed":6170,"user":{"displayName":"Nicola Amoriello","photoUrl":"","userId":"12946999478101005836"}}},"source":["EMBEDDING_SIZE = 300\r\n","\r\n","def load_dataset(path, record_path = ['data', 'paragraphs', 'qas', 'answers'], verbose = True):\r\n","\r\n","  if verbose:\r\n","      print(\"Reading the json file\")\r\n","  # if the file encoding is not UTF8 an exception should be raised    \r\n","  file = json.loads(open(path).read())\r\n","\r\n","  if verbose:\r\n","      print(\"[INFO] processing...\")\r\n","\r\n","  # parsing different level's in the json file\r\n","  js = pd.json_normalize(file , record_path )\r\n","  m = pd.json_normalize(file, record_path[:-1] )\r\n","  r = pd.json_normalize(file,record_path[:-2])\r\n","  t = pd.json_normalize(file,record_path[0])\r\n","\r\n","  #combining it into single dataframe\r\n","  idx = np.repeat(r['context'].values, r.qas.str.len())\r\n","  ndx  = np.repeat(m['id'].values,m['answers'].str.len())\r\n","  m['context'] = idx\r\n","  js['q_idx'] = ndx\r\n","  main = pd.concat([ m[['id','question','context']].set_index('id'), js.set_index('q_idx')],1,sort = False).reset_index()\r\n","  main['c_id'] = main['context'].factorize()[0]\r\n","  if verbose:\r\n","      print(f\"[INFO] there are {main.shape[0]} questions with single answer\")\r\n","      print(f\"[INFO] there are {main.groupby('c_id').sum().shape[0]} different contexts\")\r\n","      print(f\"[INFO] there are {len(t)} unrelated subjects\")\r\n","      print(\"[INFO] Done\")\r\n","  return main\r\n","\r\n","def download_glove_model(embedding_dimension = 50):\r\n","  download_path = \"glove-wiki-gigaword-{}\".format(embedding_dimension)\r\n","  try:\r\n","    print('[INFO] downloading glove {}'.format(embedding_dimension))\r\n","    emb_model = gloader.load(download_path)\r\n","    print('[INFO] done !')\r\n","  except ValueError as e:\r\n","      print(\"Glove: 50, 100, 200, 300\")\r\n","      raise e\r\n","  return emb_model"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":184},"id":"2virMOF85A8W","executionInfo":{"status":"error","timestamp":1611834311194,"user_tz":-60,"elapsed":827,"user":{"displayName":"Nicola Amoriello","photoUrl":"","userId":"12946999478101005836"}},"outputId":"2c944d85-4550-4a61-edac-da0b43612725"},"source":["dataset_path = os.path.join(os.getcwd(),'drive/SQUAD_project/data/training_set.json')\r\n","squad_dataset = load_dataset(dataset_path)"],"execution_count":6,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-dd0f29fd5e1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdataset_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'drive/SQUAD_project/data/training_set.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msquad_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'load_dataset' is not defined"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"lzHPUWA45dzO","executionInfo":{"status":"ok","timestamp":1611649489353,"user_tz":-60,"elapsed":9813,"user":{"displayName":"corentin magyar","photoUrl":"","userId":"04135457892709862388"}},"outputId":"5f3884a5-2a92-4871-a69c-b02a851f76f0"},"source":["squad_dataset.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>index</th>\n","      <th>question</th>\n","      <th>context</th>\n","      <th>answer_start</th>\n","      <th>text</th>\n","      <th>c_id</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>5733be284776f41900661182</td>\n","      <td>To whom did the Virgin Mary allegedly appear i...</td>\n","      <td>Architecturally, the school has a Catholic cha...</td>\n","      <td>515</td>\n","      <td>Saint Bernadette Soubirous</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>5733be284776f4190066117f</td>\n","      <td>What is in front of the Notre Dame Main Building?</td>\n","      <td>Architecturally, the school has a Catholic cha...</td>\n","      <td>188</td>\n","      <td>a copper statue of Christ</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>5733be284776f41900661180</td>\n","      <td>The Basilica of the Sacred heart at Notre Dame...</td>\n","      <td>Architecturally, the school has a Catholic cha...</td>\n","      <td>279</td>\n","      <td>the Main Building</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>5733be284776f41900661181</td>\n","      <td>What is the Grotto at Notre Dame?</td>\n","      <td>Architecturally, the school has a Catholic cha...</td>\n","      <td>381</td>\n","      <td>a Marian place of prayer and reflection</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5733be284776f4190066117e</td>\n","      <td>What sits on top of the Main Building at Notre...</td>\n","      <td>Architecturally, the school has a Catholic cha...</td>\n","      <td>92</td>\n","      <td>a golden statue of the Virgin Mary</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                      index  ... c_id\n","0  5733be284776f41900661182  ...    0\n","1  5733be284776f4190066117f  ...    0\n","2  5733be284776f41900661180  ...    0\n","3  5733be284776f41900661181  ...    0\n","4  5733be284776f4190066117e  ...    0\n","\n","[5 rows x 6 columns]"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"oCWKKV2RAhML"},"source":["SAMPLES = squad_dataset.shape[0]\r\n","\r\n","def preprocess_sentence(text):\r\n","  text = text.lower()\r\n","  #text = re.sub(r'(\\.)',r' \\1 ', text)\r\n","  text = text.strip()\r\n","  return text\r\n","\r\n","def clean_dataset(dataset):\r\n","\r\n","  _dataset = dataset.copy()\r\n","\r\n","  cleaned_questions = _dataset['question'].apply(preprocess_sentence)\r\n","  cleaned_texts = _dataset['text'].apply(preprocess_sentence)\r\n","\r\n","  # we process only different contexts and then we duplicate them\r\n","  unique_context = pd.Series(_dataset['context'].unique())\r\n","  count_c = _dataset.groupby('c_id').count()['text']\r\n","  cleaned_contexts = unique_context.apply(preprocess_sentence)\r\n","\r\n","  _dataset['question'] = cleaned_questions\r\n","  _dataset['text'] = cleaned_texts\r\n","  _dataset['context'] = pd.Series(np.repeat(cleaned_contexts, count_c).tolist())\r\n","\r\n","  return _dataset"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HSVZ45xxqqqz"},"source":["squad_dataset = clean_dataset(squad_dataset)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Rq8B58osDLvX"},"source":["def get_tokenizer(dataset, glove_model = None):\r\n","\r\n","  tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token = 'UNK', filters = '')\r\n","\r\n","  # we will only keep the 200 - 1 most frequent characters (otherwise oom issue)\r\n","  # others tokens are replaced by UNK token \r\n","  # we keep 1 - 199 and 1 is UNK token (so we keep 198 tokens)\r\n","  char_tokenizer = tf.keras.preprocessing.text.Tokenizer(char_level = True, filters = '', oov_token = 'UNK', num_words = 200)\r\n","\r\n","  if glove_model == None:\r\n","    glove_model = download_glove_model(EMBEDDING_SIZE)\r\n","\r\n","  tokenized_questions = dataset['question'].apply(word_tokenize).to_list()\r\n","\r\n","  contexts = pd.Series(dataset['context'].unique())\r\n","  tokenized_contexts = contexts.apply(word_tokenize).to_list()\r\n","\r\n","  sequences = glove_model.index2entity + tokenized_questions + tokenized_contexts\r\n","\r\n","  del glove_model # we  don't need anymore the glove model\r\n","\r\n","  tokenizer.fit_on_texts(sequences)\r\n","  char_tokenizer.fit_on_texts(dataset['question'].to_list() + contexts.to_list())\r\n","\r\n","  return tokenizer, char_tokenizer\r\n","\r\n","\r\n","def update_tokenizer(dataset, tokenizer, char_tokenizer):\r\n","\r\n","  tokenized_questions = dataset['question'].apply(word_tokenize).to_list()\r\n","\r\n","  contexts = pd.Series(dataset['context'].unique())\r\n","  tokenized_contexts = contexts.apply(word_tokenize).to_list()\r\n","\r\n","  sequences = tokenized_questions + tokenized_contexts\r\n","  tokenizer.fit_on_texts(sequences)\r\n","\r\n","  char_tokenizer.fit_on_texts(dataset['question'].to_list() + contexts.to_list())\r\n","\r\n","def get_start_end(row):\r\n","\r\n","  context = row['context']\r\n","  answer = row['text']\r\n","  tok_answer = word_tokenize(answer)\r\n","\r\n","  _start = context.find(answer)\r\n","\r\n","  if _start == -1:\r\n","    # the answer is not in the context\r\n","    # maybe due to a typo\r\n","    row['start'] = -1\r\n","    row['end'] = -1\r\n","    return row\r\n","\r\n","  lc = context[:_start]\r\n","  lc = word_tokenize(lc)\r\n","\r\n","  start = len(lc)\r\n","  end = start + len(tok_answer)\r\n","\r\n","  row['start'] = start\r\n","  row['end'] = end\r\n","\r\n","  return row\r\n","\r\n","def tokenize(dataset, tokenizer, char_tokenizer):\r\n","\r\n","  _dataset = dataset.copy()\r\n","\r\n","  tokenized_questions = _dataset['question'].apply(word_tokenize).to_list()\r\n","  tokenized_contexts = _dataset['context'].apply(word_tokenize).to_list()\r\n","\r\n","  t_q = tokenizer.texts_to_sequences(tokenized_questions)\r\n","  t_c = tokenizer.texts_to_sequences(tokenized_contexts)\r\n","\r\n","  c_q = []\r\n","  c_c = []\r\n","\r\n","  for question, context in zip(tokenized_questions, tokenized_contexts):\r\n","    _q = char_tokenizer.texts_to_sequences(question)\r\n","    _c = char_tokenizer.texts_to_sequences(context)\r\n","    c_q.append(_q)\r\n","    c_c.append(_c)\r\n","\r\n","  _dataset['tokenized_question'] = t_q\r\n","  _dataset['tokenized_context'] = t_c\r\n","\r\n","  _dataset['char_tokenized_question'] = c_q\r\n","  _dataset['char_tokenized_context'] = c_c\r\n","\r\n","  return _dataset\r\n","\r\n","def split(dataset, test_size = 0.2, random_state = 42):\r\n","\r\n","  # random_state for deterministic state\r\n","\r\n","  tr, vl = train_test_split(dataset, test_size = test_size, random_state = random_state)\r\n","  tr.reset_index(drop = True, inplace = True)\r\n","  vl.reset_index(drop = True, inplace = True)\r\n","\r\n","  return tr,vl\r\n","\r\n","def convert(context , coord = None, tokenizer = None):\r\n","  if coord:\r\n","    start = coord[0]\r\n","    end = coord[1]\r\n","    if type(context) == str:\r\n","      context = word_tokenize(context)\r\n","      answer = context[start:end]\r\n","      return ' '.join(answer).strip()\r\n","    else:\r\n","      answer = ''\r\n","      for i in range(start, end):\r\n","        t = context[i]\r\n","        answer+= tokenizer.index_word[t] + ' '\r\n","      return answer.strip()\r\n","  else:\r\n","    if type(context) == str:\r\n","      return context\r\n","    else:\r\n","      c = ''\r\n","      for t in context:\r\n","        c += tokenizer.index_word[t] + ' '\r\n","      return c.strip()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M8pXuOXlpLsx"},"source":["tr_df, vl_df = split(squad_dataset)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BGk_DpS3RS32","executionInfo":{"status":"ok","timestamp":1611649489788,"user_tz":-60,"elapsed":10198,"user":{"displayName":"corentin magyar","photoUrl":"","userId":"04135457892709862388"}},"outputId":"a83f8879-b8b5-44aa-fab0-22f698e3c7e4"},"source":["tr_df.shape[0],vl_df.shape[0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(70079, 17520)"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"v0VbHyp4YSUx"},"source":["Our vocabulary is based on the Glove vocabulary, and we add terms from the training set"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8hY8Z7HQFKjo","executionInfo":{"status":"ok","timestamp":1611649649911,"user_tz":-60,"elapsed":170308,"user":{"displayName":"corentin magyar","photoUrl":"","userId":"04135457892709862388"}},"outputId":"d29e0541-2b64-4633-983c-d330e56bf0a7"},"source":["tokenizer, char_tokenizer = get_tokenizer(tr_df)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[INFO] downloading glove 300\n","[INFO] done !\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ILsFqw2bKf5L","executionInfo":{"status":"ok","timestamp":1611649649917,"user_tz":-60,"elapsed":170301,"user":{"displayName":"corentin magyar","photoUrl":"","userId":"04135457892709862388"}},"outputId":"f79cd4b4-d17d-4f5c-be6c-491636179784"},"source":["print(len(tokenizer.word_index))\r\n","len(char_tokenizer.word_index)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["429064\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["1263"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"markdown","metadata":{"id":"wWoZPgxdYbkz"},"source":["We then update our vocabulary with terms from the validation set"]},{"cell_type":"code","metadata":{"id":"Xzv_ZS3Zqifn"},"source":["update_tokenizer(vl_df, tokenizer, char_tokenizer)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gLwNWJsorIae","executionInfo":{"status":"ok","timestamp":1611649667214,"user_tz":-60,"elapsed":187586,"user":{"displayName":"corentin magyar","photoUrl":"","userId":"04135457892709862388"}},"outputId":"62c1229a-cf87-466c-8062-ea97410d53e3"},"source":["print(len(tokenizer.word_index))\r\n","len(char_tokenizer.word_index)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["429758\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["1265"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"id":"fM8oYX_SHkni"},"source":["# take a while\r\n","tr_df = tr_df.apply(get_start_end, axis = 1)\r\n","vl_df = vl_df.apply(get_start_end, axis = 1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ox1J7Z8YhVcv"},"source":["we get rid of samples where the answer doesn't match the context (maybe there is a typo in the answer or the context).  \r\n","To avoid to discard many samples, we could lemmatize / stem the text.   \r\n","Obviously, lemmatization is a better choice for our task, but if we want a really accurate lemmatization processing, we need to do POS tagging."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8eTsENy6jU-t","executionInfo":{"status":"ok","timestamp":1611649874438,"user_tz":-60,"elapsed":394802,"user":{"displayName":"corentin magyar","photoUrl":"","userId":"04135457892709862388"}},"outputId":"871d5ab1-9378-4444-a208-f50a8d2d130c"},"source":["tr_df[tr_df['start'] == -1].shape[0], vl_df[vl_df['start'] == -1].shape[0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(69, 15)"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"id":"RfAls7CF683U","executionInfo":{"status":"ok","timestamp":1611649874445,"user_tz":-60,"elapsed":394804,"user":{"displayName":"corentin magyar","photoUrl":"","userId":"04135457892709862388"}},"outputId":"136b3123-e834-48bf-a2f9-940b655f2ce8"},"source":["tr_df[tr_df['start'] == -1]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>index</th>\n","      <th>question</th>\n","      <th>context</th>\n","      <th>answer_start</th>\n","      <th>text</th>\n","      <th>c_id</th>\n","      <th>start</th>\n","      <th>end</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>87</th>\n","      <td>56de31984396321400ee2672</td>\n","      <td>on what date was the 2013 human development re...</td>\n","      <td>some countries were not included for various r...</td>\n","      <td>92</td>\n","      <td>march 14, 2013</td>\n","      <td>2185</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>3133</th>\n","      <td>56e17f5de3433e1400422f8c</td>\n","      <td>what field studies the placement of catalan in...</td>\n","      <td>in central catalan, unstressed vowels reduce t...</td>\n","      <td>0</td>\n","      <td>catalan sociolinguistics</td>\n","      <td>3470</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>3983</th>\n","      <td>56e1b97fcd28a01900c67ad8</td>\n","      <td>what is the official regulating body of  valen...</td>\n","      <td>valencian is classified as a western dialect, ...</td>\n","      <td>168</td>\n","      <td>the valencian academy of language</td>\n","      <td>3488</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>6198</th>\n","      <td>56e1b4decd28a01900c67a91</td>\n","      <td>what language is the regulator meant to standa...</td>\n","      <td>in alghero, the iec has adapted its standard t...</td>\n","      <td>103</td>\n","      <td>catalan</td>\n","      <td>3486</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>6994</th>\n","      <td>56e1b738cd28a01900c67aae</td>\n","      <td>where are the provinces of lleida and tarragona?</td>\n","      <td>in 2011, the aragonese government passed a dec...</td>\n","      <td>94</td>\n","      <td>western catalonia</td>\n","      <td>3487</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>66889</th>\n","      <td>572e8003c246551400ce425f</td>\n","      <td>what did great britain gain in the west indies...</td>\n","      <td>many middle and small powers in europe, unlike...</td>\n","      <td>113</td>\n","      <td>some individual caribbean islands in the west ...</td>\n","      <td>15282</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>66972</th>\n","      <td>572e81f2cb0c0d14000f1206</td>\n","      <td>what is the precedent for the \"second hundred ...</td>\n","      <td>the war was successful for great britain, whic...</td>\n","      <td>446</td>\n","      <td>reminiscent of the more famous and compact str...</td>\n","      <td>15283</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>67376</th>\n","      <td>572e8578c246551400ce42bd</td>\n","      <td>who would sicily and savoy normally align with?</td>\n","      <td>realizing that war was imminent, prussia preem...</td>\n","      <td>434</td>\n","      <td>sicily, and savoy, although sided with franco-...</td>\n","      <td>15281</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>69867</th>\n","      <td>56e180f5e3433e1400422f96</td>\n","      <td>what do the dialects of catalan feature?</td>\n","      <td>catalan sociolinguistics studies the situation...</td>\n","      <td>56</td>\n","      <td>uniformity</td>\n","      <td>3471</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>69923</th>\n","      <td>56e17f5de3433e1400422f90</td>\n","      <td>what outside affects does this study include?</td>\n","      <td>in central catalan, unstressed vowels reduce t...</td>\n","      <td>331</td>\n","      <td>other languages in contact</td>\n","      <td>3470</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>69 rows Ã— 8 columns</p>\n","</div>"],"text/plain":["                          index  ... end\n","87     56de31984396321400ee2672  ...  -1\n","3133   56e17f5de3433e1400422f8c  ...  -1\n","3983   56e1b97fcd28a01900c67ad8  ...  -1\n","6198   56e1b4decd28a01900c67a91  ...  -1\n","6994   56e1b738cd28a01900c67aae  ...  -1\n","...                         ...  ...  ..\n","66889  572e8003c246551400ce425f  ...  -1\n","66972  572e81f2cb0c0d14000f1206  ...  -1\n","67376  572e8578c246551400ce42bd  ...  -1\n","69867  56e180f5e3433e1400422f96  ...  -1\n","69923  56e17f5de3433e1400422f90  ...  -1\n","\n","[69 rows x 8 columns]"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":520},"id":"o7uz1Odj7fAS","executionInfo":{"status":"ok","timestamp":1611649874451,"user_tz":-60,"elapsed":394807,"user":{"displayName":"corentin magyar","photoUrl":"","userId":"04135457892709862388"}},"outputId":"3cd055db-9fb0-4eab-a658-1f61febd6cc0"},"source":["vl_df[vl_df['start'] == -1]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>index</th>\n","      <th>question</th>\n","      <th>context</th>\n","      <th>answer_start</th>\n","      <th>text</th>\n","      <th>c_id</th>\n","      <th>start</th>\n","      <th>end</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>171</th>\n","      <td>56e18a90e3433e1400422fac</td>\n","      <td>in what densely populated area is it spoken?</td>\n","      <td>western catalan comprises the two dialects of ...</td>\n","      <td>166</td>\n","      <td>barcelona province</td>\n","      <td>3474</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>1536</th>\n","      <td>56e1a3cbe3433e1400423066</td>\n","      <td>where is iec's standard used?</td>\n","      <td>standard catalan, virtually accepted by all sp...</td>\n","      <td>3</td>\n","      <td>the balearic islands</td>\n","      <td>3484</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>3007</th>\n","      <td>56e18710cd28a01900c679b9</td>\n","      <td>what have a and e done in eastern dialects?</td>\n","      <td>the dialects of the catalan language feature a...</td>\n","      <td>162</td>\n","      <td>merged</td>\n","      <td>3472</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>4925</th>\n","      <td>56e1b4decd28a01900c67a8e</td>\n","      <td>where is the catalan speaking part of aragon?</td>\n","      <td>in alghero, the iec has adapted its standard t...</td>\n","      <td>114</td>\n","      <td>la franja</td>\n","      <td>3486</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>5782</th>\n","      <td>56e18bfbe3433e1400422fb5</td>\n","      <td>how many stressed phonemes are there in catalan?</td>\n","      <td>central catalan is considered the standard pro...</td>\n","      <td>69</td>\n","      <td>seven</td>\n","      <td>3468</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>5897</th>\n","      <td>56e18710cd28a01900c679b7</td>\n","      <td>what is the major difference between the two b...</td>\n","      <td>the dialects of the catalan language feature a...</td>\n","      <td>118</td>\n","      <td>treatment of unstressed a and e</td>\n","      <td>3472</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>5937</th>\n","      <td>56e18bfbe3433e1400422fb4</td>\n","      <td>what is the vowel system of catalan?</td>\n","      <td>western catalan comprises the two dialects of ...</td>\n","      <td>50</td>\n","      <td>vulgar latin</td>\n","      <td>3468</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>6561</th>\n","      <td>56e1b264e3433e14004230a6</td>\n","      <td>where has the iec adapted its standard to the ...</td>\n","      <td>the most notable difference between both stand...</td>\n","      <td>3</td>\n","      <td>alghero</td>\n","      <td>3485</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>7378</th>\n","      <td>572e81f2cb0c0d14000f1207</td>\n","      <td>what was a later conflict that some considered...</td>\n","      <td>the war was successful for great britain, whic...</td>\n","      <td>246</td>\n","      <td>to later conflicts like the napoleonic wars</td>\n","      <td>15283</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>11252</th>\n","      <td>56e17b08cd28a01900c679af</td>\n","      <td>where do you find dialectic vowel reductions?</td>\n","      <td>catalan has inherited the typical vowel system...</td>\n","      <td>176</td>\n","      <td>section pronunciation</td>\n","      <td>3469</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>11511</th>\n","      <td>56de31f34396321400ee2680</td>\n","      <td>does the ihdi measure the \"average\" or the \"po...</td>\n","      <td>the 2013 human development report by the unite...</td>\n","      <td>72</td>\n","      <td>the average level</td>\n","      <td>2182</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>12438</th>\n","      <td>56e188e4cd28a01900c679c0</td>\n","      <td>how many dialects are in the eastern group?</td>\n","      <td>the dialects of the catalan language feature a...</td>\n","      <td>110</td>\n","      <td>four</td>\n","      <td>3473</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>16218</th>\n","      <td>56e180f5e3433e1400422f97</td>\n","      <td>in comparison to what are the dialects uniform?</td>\n","      <td>catalan sociolinguistics studies the situation...</td>\n","      <td>96</td>\n","      <td>other romance languages</td>\n","      <td>3471</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>16474</th>\n","      <td>56e1b738cd28a01900c67aaf</td>\n","      <td>what forms are mutually intelligible?</td>\n","      <td>in 2011, the aragonese government passed a dec...</td>\n","      <td>190</td>\n","      <td>catalan and valencian</td>\n","      <td>3487</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>17282</th>\n","      <td>56e18bfbe3433e1400422fb6</td>\n","      <td>where is this system common?</td>\n","      <td>central catalan is considered the standard pro...</td>\n","      <td>131</td>\n","      <td>western romance</td>\n","      <td>3468</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                          index  ... end\n","171    56e18a90e3433e1400422fac  ...  -1\n","1536   56e1a3cbe3433e1400423066  ...  -1\n","3007   56e18710cd28a01900c679b9  ...  -1\n","4925   56e1b4decd28a01900c67a8e  ...  -1\n","5782   56e18bfbe3433e1400422fb5  ...  -1\n","5897   56e18710cd28a01900c679b7  ...  -1\n","5937   56e18bfbe3433e1400422fb4  ...  -1\n","6561   56e1b264e3433e14004230a6  ...  -1\n","7378   572e81f2cb0c0d14000f1207  ...  -1\n","11252  56e17b08cd28a01900c679af  ...  -1\n","11511  56de31f34396321400ee2680  ...  -1\n","12438  56e188e4cd28a01900c679c0  ...  -1\n","16218  56e180f5e3433e1400422f97  ...  -1\n","16474  56e1b738cd28a01900c67aaf  ...  -1\n","17282  56e18bfbe3433e1400422fb6  ...  -1\n","\n","[15 rows x 8 columns]"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"code","metadata":{"id":"pZ3CXWdL8CNk"},"source":["# we get rid of samples where the answer doesn't match the context\r\n","tr_df = tr_df[tr_df['start'] != -1]\r\n","vl_df = vl_df[vl_df['start'] != -1]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M_KJeyPxVZG6"},"source":["tr_df = tokenize(tr_df, tokenizer, char_tokenizer)\r\n","vl_df = tokenize(vl_df, tokenizer, char_tokenizer)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":363},"id":"VJd7p05zjg0e","executionInfo":{"status":"ok","timestamp":1611650015811,"user_tz":-60,"elapsed":536155,"user":{"displayName":"corentin magyar","photoUrl":"","userId":"04135457892709862388"}},"outputId":"59feac1d-5afd-404a-ee9a-3c866b325ead"},"source":["tr_df.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>index</th>\n","      <th>question</th>\n","      <th>context</th>\n","      <th>answer_start</th>\n","      <th>text</th>\n","      <th>c_id</th>\n","      <th>start</th>\n","      <th>end</th>\n","      <th>tokenized_question</th>\n","      <th>tokenized_context</th>\n","      <th>char_tokenized_question</th>\n","      <th>char_tokenized_context</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>572667e6708984140094c4f9</td>\n","      <td>what team had dallas green managed in 1980?</td>\n","      <td>after over a dozen more subpar seasons, in 198...</td>\n","      <td>154</td>\n","      <td>phillies</td>\n","      <td>8880</td>\n","      <td>29</td>\n","      <td>30</td>\n","      <td>[11, 309, 49, 11808, 646, 2132, 6, 2627, 9]</td>\n","      <td>[61, 83, 10, 6737, 62, 70020, 1740, 3, 6, 3372...</td>\n","      <td>[[20, 11, 5, 4], [4, 3, 5, 16], [11, 5, 13], [...</td>\n","      <td>[[5, 17, 4, 3, 10], [8, 24, 3, 10], [5], [13, ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>56dec2483277331400b4d712</td>\n","      <td>which candidate withdrew from the presidential...</td>\n","      <td>schwarzenegger's endorsement in the republican...</td>\n","      <td>156</td>\n","      <td>rudy giuliani</td>\n","      <td>2311</td>\n","      <td>23</td>\n","      <td>25</td>\n","      <td>[27, 2789, 4161, 23, 2, 1534, 698, 6, 417, 4, ...</td>\n","      <td>[1084, 19, 9106, 6, 2, 1467, 477, 4, 2, 420, 1...</td>\n","      <td>[[20, 11, 6, 14, 11], [14, 5, 7, 13, 6, 13, 5,...</td>\n","      <td>[[9, 14, 11, 20, 5, 10, 39, 3, 7, 3, 19, 19, 3...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>5726e5995951b619008f81bb</td>\n","      <td>captive animals can distinguish co-inhabitats ...</td>\n","      <td>it has been observed that well-fed predator an...</td>\n","      <td>224</td>\n","      <td>wild ones outside the area</td>\n","      <td>9822</td>\n","      <td>38</td>\n","      <td>43</td>\n","      <td>[11888, 727, 65, 3733, 419169, 23, 11, 48, 136...</td>\n","      <td>[30, 40, 59, 2316, 20, 63225, 4421, 727, 6, 10...</td>\n","      <td>[[14, 5, 18, 4, 6, 24, 3], [5, 7, 6, 16, 5, 12...</td>\n","      <td>[[6, 4], [11, 5, 9], [22, 3, 3, 7], [8, 22, 9,...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>5726486f708984140094c157</td>\n","      <td>the results of which battle allowed the britis...</td>\n","      <td>after returning from egypt, napoleon engineere...</td>\n","      <td>919</td>\n","      <td>the battle of trafalgar</td>\n","      <td>8418</td>\n","      <td>158</td>\n","      <td>162</td>\n","      <td>[2, 1324, 4, 27, 326, 495, 2, 132, 8, 6280, 15...</td>\n","      <td>[61, 3986, 23, 598, 3, 545, 9789, 10, 2313, 6,...</td>\n","      <td>[[4, 11, 3], [10, 3, 9, 15, 12, 4, 9], [8, 17]...</td>\n","      <td>[[5, 17, 4, 3, 10], [10, 3, 4, 15, 10, 7, 6, 7...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5730299db2c2fd14005689a7</td>\n","      <td>how was vesey executed in 1822?</td>\n","      <td>by 1820, charleston's population had grown to ...</td>\n","      <td>382</td>\n","      <td>hanged</td>\n","      <td>15719</td>\n","      <td>74</td>\n","      <td>75</td>\n","      <td>[44, 13, 25121, 2181, 6, 10202, 9]</td>\n","      <td>[18, 9015, 3, 1909, 19, 104, 49, 2555, 8, 2106...</td>\n","      <td>[[11, 8, 20], [20, 5, 9], [24, 3, 9, 3, 21], [...</td>\n","      <td>[[22, 21], [28, 40, 31, 29], [23], [14, 11, 5,...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                      index  ...                             char_tokenized_context\n","0  572667e6708984140094c4f9  ...  [[5, 17, 4, 3, 10], [8, 24, 3, 10], [5], [13, ...\n","1  56dec2483277331400b4d712  ...  [[9, 14, 11, 20, 5, 10, 39, 3, 7, 3, 19, 19, 3...\n","2  5726e5995951b619008f81bb  ...  [[6, 4], [11, 5, 9], [22, 3, 3, 7], [8, 22, 9,...\n","3  5726486f708984140094c157  ...  [[5, 17, 4, 3, 10], [10, 3, 4, 15, 10, 7, 6, 7...\n","4  5730299db2c2fd14005689a7  ...  [[22, 21], [28, 40, 31, 29], [23], [14, 11, 5,...\n","\n","[5 rows x 12 columns]"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2kw5Pk4c6I3S","executionInfo":{"status":"ok","timestamp":1611650016236,"user_tz":-60,"elapsed":536576,"user":{"displayName":"corentin magyar","photoUrl":"","userId":"04135457892709862388"}},"outputId":"18473e87-e9cb-4f86-d1f8-f3cb6f5b2774"},"source":["print(tr_df['tokenized_question'].str.len().describe())\r\n","vl_df['tokenized_question'].str.len().describe()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["count    70010.000000\n","mean        11.275532\n","std          3.715821\n","min          1.000000\n","25%          9.000000\n","50%         11.000000\n","75%         13.000000\n","max         60.000000\n","Name: tokenized_question, dtype: float64\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["count    17505.000000\n","mean        11.335504\n","std          3.754207\n","min          1.000000\n","25%          9.000000\n","50%         11.000000\n","75%         13.000000\n","max         38.000000\n","Name: tokenized_question, dtype: float64"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Dk5TG_Yy9crW","executionInfo":{"status":"ok","timestamp":1611650016239,"user_tz":-60,"elapsed":536573,"user":{"displayName":"corentin magyar","photoUrl":"","userId":"04135457892709862388"}},"outputId":"6af392cf-365e-4d8f-e056-c0e8ab501414"},"source":["print(tr_df['tokenized_question'].str.len().quantile(0.99))\r\n","vl_df['tokenized_question'].str.len().quantile(0.99)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["22.0\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["23.0"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bsKYKsgR6-UK","executionInfo":{"status":"ok","timestamp":1611650016241,"user_tz":-60,"elapsed":536571,"user":{"displayName":"corentin magyar","photoUrl":"","userId":"04135457892709862388"}},"outputId":"25835967-c2f6-47f8-e878-2dd802534a21"},"source":["print(tr_df['tokenized_context'].str.len().describe())\r\n","vl_df['tokenized_context'].str.len().describe()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["count    70010.000000\n","mean       137.824439\n","std         56.941382\n","min         22.000000\n","25%        102.000000\n","50%        127.000000\n","75%        164.000000\n","max        766.000000\n","Name: tokenized_context, dtype: float64\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["count    17505.000000\n","mean       137.211083\n","std         55.912622\n","min         22.000000\n","25%        102.000000\n","50%        126.000000\n","75%        163.000000\n","max        766.000000\n","Name: tokenized_context, dtype: float64"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Cyp5MUtj9woq","executionInfo":{"status":"ok","timestamp":1611650016243,"user_tz":-60,"elapsed":536569,"user":{"displayName":"corentin magyar","photoUrl":"","userId":"04135457892709862388"}},"outputId":"ef58613f-c0b9-4562-a474-775c8dbe130d"},"source":["print(tr_df['tokenized_context'].str.len().quantile(0.99))\r\n","vl_df['tokenized_context'].str.len().quantile(0.99)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["324.0\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["324.0"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"code","metadata":{"id":"fE3J6xJZ7Sqe"},"source":["def len_words(dataset):\r\n","  count_q = []\r\n","  count_c = []\r\n","\r\n","  for idx, row in dataset.iterrows():\r\n","    for w in row['char_tokenized_question']:\r\n","      l = len(w)\r\n","      count_q.append(l)\r\n","      \r\n","    for w in row['char_tokenized_context']:\r\n","      m = len(w)\r\n","      count_c.append(m)\r\n","  \r\n","  return pd.Series(count_q), pd.Series(count_c)\r\n","\r\n","t_q,t_c = len_words(tr_df)\r\n","v_q,v_c = len_words(vl_df)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i8R_-bUv9HGQ","executionInfo":{"status":"ok","timestamp":1611650029529,"user_tz":-60,"elapsed":549844,"user":{"displayName":"corentin magyar","photoUrl":"","userId":"04135457892709862388"}},"outputId":"7a9038c6-19b4-4926-aabf-fe217941e1cf"},"source":["print(t_q.describe())\r\n","t_c.describe()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["count    789400.000000\n","mean          4.447926\n","std           2.677579\n","min           1.000000\n","25%           2.000000\n","50%           4.000000\n","75%           6.000000\n","max          30.000000\n","dtype: float64\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["count    9.649089e+06\n","mean     4.626070e+00\n","std      2.969605e+00\n","min      1.000000e+00\n","25%      2.000000e+00\n","50%      4.000000e+00\n","75%      7.000000e+00\n","max      3.700000e+01\n","dtype: float64"]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R64fJS7DxkuN","executionInfo":{"status":"ok","timestamp":1611650029538,"user_tz":-60,"elapsed":549850,"user":{"displayName":"corentin magyar","photoUrl":"","userId":"04135457892709862388"}},"outputId":"d774b4bf-2255-4dc5-e532-9ad1872878ce"},"source":["print(v_q.describe())\r\n","v_c.describe()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["count    198428.000000\n","mean          4.453232\n","std           2.686696\n","min           1.000000\n","25%           2.000000\n","50%           4.000000\n","75%           6.000000\n","max          24.000000\n","dtype: float64\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["count    2.401880e+06\n","mean     4.629710e+00\n","std      2.972670e+00\n","min      1.000000e+00\n","25%      2.000000e+00\n","50%      4.000000e+00\n","75%      7.000000e+00\n","max      3.700000e+01\n","dtype: float64"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5wxf3tS7xylx","executionInfo":{"status":"ok","timestamp":1611650029844,"user_tz":-60,"elapsed":550153,"user":{"displayName":"corentin magyar","photoUrl":"","userId":"04135457892709862388"}},"outputId":"97273789-75d0-4698-c2fd-dcba8d340acf"},"source":["print(t_q.quantile(0.99))\r\n","t_c.quantile(0.99)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["12.0\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["13.0"]},"metadata":{"tags":[]},"execution_count":28}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x6F-O0qjx4Gu","executionInfo":{"status":"ok","timestamp":1611650029848,"user_tz":-60,"elapsed":550152,"user":{"displayName":"corentin magyar","photoUrl":"","userId":"04135457892709862388"}},"outputId":"3d97ef17-184d-43d2-9112-ee943961cac1"},"source":["print(v_q.quantile(0.99))\r\n","v_c.quantile(0.99)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["12.0\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["13.0"]},"metadata":{"tags":[]},"execution_count":29}]},{"cell_type":"markdown","metadata":{"id":"LC1K9nYs9ht2"},"source":["There are obviously some outliers. We are compeled to get rid of some samples because of memory issues.\r\n","\r\n","We will get rid of contexts that have more than 325 words and questions that have more than 23 words.\r\n","\r\n","We will set the length of a word to 13 characters"]},{"cell_type":"code","metadata":{"id":"3WiAI_oA_3lY"},"source":["QUESTION_MAXLEN = 23\r\n","CONTEXT_MAXLEN = 325\r\n","WORD_MAXLEN = 13"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5fZMoWqTL2Wz","executionInfo":{"status":"ok","timestamp":1611650029856,"user_tz":-60,"elapsed":550151,"user":{"displayName":"corentin magyar","photoUrl":"","userId":"04135457892709862388"}},"outputId":"63863178-bc2f-4d41-e759-eccb078b2c1b"},"source":["tr_df.shape, vl_df.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((70010, 12), (17505, 12))"]},"metadata":{"tags":[]},"execution_count":31}]},{"cell_type":"code","metadata":{"id":"ibILzhoh-Mf8"},"source":["tr_df = tr_df[(tr_df['tokenized_question'].str.len() <= QUESTION_MAXLEN) & (tr_df['tokenized_context'].str.len() <= CONTEXT_MAXLEN) & (tr_df['start'] <= CONTEXT_MAXLEN) & (tr_df['end'] <= CONTEXT_MAXLEN) ].reset_index(drop = True)\r\n","vl_df = vl_df[(vl_df['tokenized_question'].str.len() <= QUESTION_MAXLEN) & (vl_df['tokenized_context'].str.len() <= CONTEXT_MAXLEN) & (vl_df['start'] <= CONTEXT_MAXLEN) & (vl_df['end'] <= CONTEXT_MAXLEN) ].reset_index(drop = True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qQTuvAo7oGcA","executionInfo":{"status":"ok","timestamp":1611650029862,"user_tz":-60,"elapsed":550148,"user":{"displayName":"corentin magyar","photoUrl":"","userId":"04135457892709862388"}},"outputId":"493da891-8eb9-4d19-def3-63dae3937ee6"},"source":["tr_df.shape[0], vl_df.shape[0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(68842, 17199)"]},"metadata":{"tags":[]},"execution_count":33}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"93mWQxFLG3z9","executionInfo":{"status":"ok","timestamp":1611650030098,"user_tz":-60,"elapsed":550380,"user":{"displayName":"corentin magyar","photoUrl":"","userId":"04135457892709862388"}},"outputId":"a8221249-0f70-4879-d090-07a37e57d618"},"source":[" print(f' we get rid of : {SAMPLES - (tr_df.shape[0] + vl_df.shape[0])} samples')"],"execution_count":null,"outputs":[{"output_type":"stream","text":[" we get rid of : 1558 samples\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Owvocjs5JCPa"},"source":["with open('tokenizer.pickle', 'wb') as handle:\r\n","    pickle.dump(tokenizer, handle, protocol = pickle.HIGHEST_PROTOCOL)\r\n","%mv tokenizer.pickle 'drive/MyDrive/NLP/data/' "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UO51HjxuOb85"},"source":["with open('char_tokenizer.pickle', 'wb') as handle:\r\n","    pickle.dump(char_tokenizer, handle, protocol = pickle.HIGHEST_PROTOCOL)\r\n","%mv char_tokenizer.pickle 'drive/MyDrive/NLP/data/' "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A0O23CFlJInL"},"source":["tr_padded_questions = tf.keras.preprocessing.sequence.pad_sequences(tr_df['tokenized_question'], padding = 'post', maxlen = QUESTION_MAXLEN)\r\n","tr_padded_contexts = tf.keras.preprocessing.sequence.pad_sequences(tr_df['tokenized_context'], padding = 'post', maxlen = CONTEXT_MAXLEN)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EjvD-ReFpsK9"},"source":["np.save('drive/MyDrive/NLP/data/tr_padded_questions__{}.npy'.format(QUESTION_MAXLEN), tr_padded_questions)\r\n","np.save('drive/MyDrive/NLP/data/tr_padded_contexts__{}.npy'.format(CONTEXT_MAXLEN), tr_padded_contexts)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mT8IxOUEJOX9"},"source":["vl_padded_questions = tf.keras.preprocessing.sequence.pad_sequences(vl_df['tokenized_question'], padding = 'post', maxlen = QUESTION_MAXLEN)\r\n","vl_padded_contexts = tf.keras.preprocessing.sequence.pad_sequences(vl_df['tokenized_context'], padding = 'post', maxlen = CONTEXT_MAXLEN)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VLXyLCkwpwNC"},"source":["np.save('drive/MyDrive/NLP/data/vl_padded_questions__{}.npy'.format(QUESTION_MAXLEN), vl_padded_questions)\r\n","np.save('drive/MyDrive/NLP/data/vl_padded_contexts__{}.npy'.format(CONTEXT_MAXLEN), vl_padded_contexts)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yAc0OvPaQm61"},"source":["pad_char_c = np.zeros((vl_df.shape[0], QUESTION_MAXLEN, WORD_MAXLEN), dtype = np.int32)\r\n","\r\n","for i, value in vl_df['char_tokenized_question'].iteritems():\r\n","  v = tf.keras.preprocessing.sequence.pad_sequences(value, padding = 'post', maxlen = WORD_MAXLEN, truncating = 'post')\r\n","  to_add = QUESTION_MAXLEN - v.shape[0]\r\n","  add = np.zeros((to_add, WORD_MAXLEN))\r\n","  arr = np.vstack([v,add])\r\n","  pad_char_c[i] = arr\r\n","\r\n","np.save('drive/MyDrive/NLP/data/vl_char_padded_questions__{}.npy'.format(WORD_MAXLEN), pad_char_c)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M7TPzCCzjDoX"},"source":["pad_char_c = np.zeros((tr_df.shape[0], QUESTION_MAXLEN, WORD_MAXLEN), dtype = np.int32)\r\n","\r\n","for i, value in tr_df['char_tokenized_question'].iteritems():\r\n","  v = tf.keras.preprocessing.sequence.pad_sequences(value, padding = 'post', maxlen = WORD_MAXLEN, truncating = 'post')\r\n","  to_add = QUESTION_MAXLEN - v.shape[0]\r\n","  add = np.zeros((to_add, WORD_MAXLEN))\r\n","  arr = np.vstack([v,add])\r\n","  pad_char_c[i] = arr\r\n","\r\n","np.save('drive/MyDrive/NLP/data/tr_char_padded_questions__{}.npy'.format(WORD_MAXLEN), pad_char_c)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AoG-TYzd6saB"},"source":["del pad_char_c # to free memory"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vvSDAazhjPpt"},"source":["pad_char_q = np.zeros((vl_df.shape[0], CONTEXT_MAXLEN, WORD_MAXLEN), dtype = np.int32)\r\n","\r\n","for i, value in vl_df['char_tokenized_context'].iteritems():\r\n","  v = tf.keras.preprocessing.sequence.pad_sequences(value, padding = 'post', maxlen = WORD_MAXLEN, truncating = 'post')\r\n","  to_add = CONTEXT_MAXLEN - v.shape[0]\r\n","  add = np.zeros((to_add, WORD_MAXLEN))\r\n","  arr = np.vstack([v,add])\r\n","  pad_char_q[i] = arr\r\n","\r\n","np.save('drive/MyDrive/NLP/data/vl_char_padded_contexts__{}.npy'.format(WORD_MAXLEN), pad_char_q)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z5wdii1-ln-o"},"source":["pad_char_q =  np.zeros((tr_df.shape[0], CONTEXT_MAXLEN, WORD_MAXLEN))\r\n","\r\n","for i, value in tr_df['char_tokenized_context'].iteritems():\r\n","  v = tf.keras.preprocessing.sequence.pad_sequences(value, padding = 'post', maxlen = WORD_MAXLEN, truncating = 'post')\r\n","  to_add = CONTEXT_MAXLEN - v.shape[0]\r\n","  add = np.zeros((to_add, WORD_MAXLEN))\r\n","  arr = np.vstack([v,add])\r\n","  pad_char_q[i] = arr\r\n","  \r\n","np.save('drive/MyDrive/NLP/data/tr_char_padded_contexts__{}.npy'.format(WORD_MAXLEN), pad_char_q)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"k1JBEoSr6w6I"},"source":["del pad_char_q # to free memory"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xNvYtxrbPKJs"},"source":["num_classes = CONTEXT_MAXLEN\r\n","y_start_train = tf.keras.utils.to_categorical(tr_df['start'].values, num_classes)\r\n","y_end_train = tf.keras.utils.to_categorical(tr_df['end'].values, num_classes)\r\n","\r\n","y_start_valid = tf.keras.utils.to_categorical(vl_df['start'].values, num_classes)\r\n","y_end_valid = tf.keras.utils.to_categorical(vl_df['end'].values, num_classes)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ND79b1GXPSql"},"source":["np.save('drive/MyDrive/NLP/data/tr_y_start.npy', y_start_train)\r\n","np.save('drive/MyDrive/NLP/data/tr_y_end.npy', y_end_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MGvrV6EnPj4g"},"source":["np.save('drive/MyDrive/NLP/data/vl_y_start.npy', y_start_valid)\r\n","np.save('drive/MyDrive/NLP/data/vl_y_end.npy', y_end_valid)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vGAUj_UvNKSp"},"source":["In fact we have created a character tokenizer but we won't work at the character level.  \r\n","Indeed, we run oom when we try to work at this level.\r\n","\r\n","**EDIT** : it seems we can work at the character level if we significantly reduce `WORD_MAXLEN` (from 37 to 13) and only treat the first 199 tokens as real tokens and the others as UNK tokens"]}]}