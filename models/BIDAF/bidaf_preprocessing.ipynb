{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bidaf_preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrXpyz2vZhs1"
      },
      "source": [
        "In order to run this notebook, the first things you should do are :\n",
        "* pip install pandas numpy tensorflow nltk gensim sklearn\n",
        "* modify the `SQUAD_PATH` variable (path to squad file)\n",
        "* modify all others paths (where to save datasets, tokenizers...)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hETK3Axg43yB",
        "outputId": "9f955e8f-029e-45e4-e6c2-b2adafb0349d"
      },
      "source": [
        "import json\n",
        "import os\n",
        "import random\n",
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "nltk.download('punkt')\n",
        "import gensim.downloader as gloader\n",
        "from sklearn.model_selection import train_test_split\n",
        "import re\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "\n",
        "random.seed(42)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqLS5R574tdD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79691f70-05ef-4dc6-ee93-5237f955226e"
      },
      "source": [
        "EMBEDDING_SIZE = 300\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/amrlnic/SQuAD/main/data/training_set.json\" \n",
        "download = requests.get(url).content\n",
        "data = json.loads(download)\n",
        "\n",
        "def load_dataset(file, record_path = ['data', 'paragraphs', 'qas', 'answers'], verbose = True):\n",
        "\n",
        "  \"\"\"\n",
        "  parse the SQUAD dataset into a dataframe\n",
        "  \"\"\"\n",
        "\n",
        "  if verbose:\n",
        "      print(\"Reading the json file\")\n",
        "\n",
        "  if verbose:\n",
        "      print(\"[INFO] processing...\")\n",
        "\n",
        "  # parsing different level's in the json file\n",
        "  js = pd.json_normalize(file , record_path )\n",
        "  m = pd.json_normalize(file, record_path[:-1] )\n",
        "  r = pd.json_normalize(file, record_path[:-2])\n",
        "  t = pd.json_normalize(file, record_path[0])\n",
        "\n",
        "  title = pd.json_normalize(file['data'], record_path = ['paragraphs'], meta = 'title')\n",
        "\n",
        "  #combining it into single dataframe\n",
        "  idx = np.repeat(r['context'].values, r.qas.str.len())\n",
        "  ndx  = np.repeat(m['id'].values, m['answers'].str.len())\n",
        "  m['context'] = idx\n",
        "  m['title'] = np.repeat(title['title'].values, r.qas.str.len())\n",
        "  js['q_idx'] = ndx\n",
        "  main = pd.concat([ m[['id','question','context', 'title']].set_index('id'), js.set_index('q_idx')], 1, sort = False).reset_index()\n",
        "  main['c_id'] = main['context'].factorize()[0]\n",
        "  if verbose:\n",
        "      print(f\"[INFO] there are {main.shape[0]} questions with single answer\")\n",
        "      print(f\"[INFO] there are {main.groupby('c_id').sum().shape[0]} different contexts\")\n",
        "      print(f\"[INFO] there are {len(t)} unrelated subjects\")\n",
        "      print(\"[INFO] Done\")\n",
        "  return main\n",
        "\n",
        "\n",
        "# Load data\n",
        "squad_dataset = load_dataset(data)\n",
        "def download_glove_model(embedding_dimension = 50):\n",
        "\n",
        "  \"\"\"\n",
        "  download glove model\n",
        "  \"\"\"\n",
        "\n",
        "  download_path = \"glove-wiki-gigaword-{}\".format(embedding_dimension)\n",
        "  try:\n",
        "    print('[INFO] downloading glove {}'.format(embedding_dimension))\n",
        "    emb_model = gloader.load(download_path)\n",
        "    print('[INFO] done !')\n",
        "  except ValueError as e:\n",
        "      print(\"Glove: 50, 100, 200, 300\")\n",
        "      raise e\n",
        "  return emb_model"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading the json file\n",
            "[INFO] processing...\n",
            "[INFO] there are 87599 questions with single answer\n",
            "[INFO] there are 18891 different contexts\n",
            "[INFO] there are 442 unrelated subjects\n",
            "[INFO] Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2virMOF85A8W",
        "outputId": "72ebb6b6-a619-4b99-c520-49581a14aa1e"
      },
      "source": [
        "squad_dataset = load_dataset(data)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading the json file\n",
            "[INFO] processing...\n",
            "[INFO] there are 87599 questions with single answer\n",
            "[INFO] there are 18891 different contexts\n",
            "[INFO] there are 442 unrelated subjects\n",
            "[INFO] Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "id": "lzHPUWA45dzO",
        "outputId": "f520cfaa-a932-40fb-d8b2-93a28938e908"
      },
      "source": [
        "squad_dataset.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>question</th>\n",
              "      <th>context</th>\n",
              "      <th>title</th>\n",
              "      <th>answer_start</th>\n",
              "      <th>text</th>\n",
              "      <th>c_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5733be284776f41900661182</td>\n",
              "      <td>To whom did the Virgin Mary allegedly appear i...</td>\n",
              "      <td>Architecturally, the school has a Catholic cha...</td>\n",
              "      <td>University_of_Notre_Dame</td>\n",
              "      <td>515</td>\n",
              "      <td>Saint Bernadette Soubirous</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5733be284776f4190066117f</td>\n",
              "      <td>What is in front of the Notre Dame Main Building?</td>\n",
              "      <td>Architecturally, the school has a Catholic cha...</td>\n",
              "      <td>University_of_Notre_Dame</td>\n",
              "      <td>188</td>\n",
              "      <td>a copper statue of Christ</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5733be284776f41900661180</td>\n",
              "      <td>The Basilica of the Sacred heart at Notre Dame...</td>\n",
              "      <td>Architecturally, the school has a Catholic cha...</td>\n",
              "      <td>University_of_Notre_Dame</td>\n",
              "      <td>279</td>\n",
              "      <td>the Main Building</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5733be284776f41900661181</td>\n",
              "      <td>What is the Grotto at Notre Dame?</td>\n",
              "      <td>Architecturally, the school has a Catholic cha...</td>\n",
              "      <td>University_of_Notre_Dame</td>\n",
              "      <td>381</td>\n",
              "      <td>a Marian place of prayer and reflection</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5733be284776f4190066117e</td>\n",
              "      <td>What sits on top of the Main Building at Notre...</td>\n",
              "      <td>Architecturally, the school has a Catholic cha...</td>\n",
              "      <td>University_of_Notre_Dame</td>\n",
              "      <td>92</td>\n",
              "      <td>a golden statue of the Virgin Mary</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                      index  ... c_id\n",
              "0  5733be284776f41900661182  ...    0\n",
              "1  5733be284776f4190066117f  ...    0\n",
              "2  5733be284776f41900661180  ...    0\n",
              "3  5733be284776f41900661181  ...    0\n",
              "4  5733be284776f4190066117e  ...    0\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCWKKV2RAhML"
      },
      "source": [
        "SAMPLES = squad_dataset.shape[0]\n",
        "\n",
        "def preprocess_sentence(text):\n",
        "\n",
        "  \"\"\"\n",
        "  lowercase and strip the given text\n",
        "  \"\"\"\n",
        "\n",
        "  text = text.lower()\n",
        "  text = text.strip()\n",
        "  return text\n",
        "\n",
        "def clean_dataset(dataset, with_answer = True):\n",
        "\n",
        "  \"\"\"\n",
        "  preprocess the dataset\n",
        "  \"\"\"\n",
        "\n",
        "  _dataset = dataset.copy()\n",
        "\n",
        "  cleaned_questions = _dataset['question'].apply(preprocess_sentence)\n",
        "\n",
        "  # we process only different contexts and then we duplicate them\n",
        "  unique_context = pd.Series(_dataset['context'].unique())\n",
        "  count_c = _dataset.groupby('c_id').size()\n",
        "  cleaned_contexts = unique_context.apply(preprocess_sentence)\n",
        "\n",
        "  _dataset['question'] = cleaned_questions\n",
        "\n",
        "  if with_answer:\n",
        "    cleaned_texts = _dataset['text'].apply(preprocess_sentence)\n",
        "    _dataset['text'] = cleaned_texts\n",
        "  _dataset['context'] = pd.Series(np.repeat(cleaned_contexts, count_c).tolist())\n",
        "\n",
        "  return _dataset"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSVZ45xxqqqz"
      },
      "source": [
        "squad_dataset = clean_dataset(squad_dataset)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rq8B58osDLvX"
      },
      "source": [
        "def get_tokenizer(dataset, glove_model = None):\n",
        "\n",
        "  \"\"\"\n",
        "  create the word and char tokenizers and feed them \n",
        "  on the given dataset and the glove vocabulary\n",
        "  \"\"\"\n",
        "\n",
        "  tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token = 'UNK', filters = '')\n",
        "\n",
        "  # we will only keep the 200 - 1 most frequent characters (otherwise oom issue)\n",
        "  # others tokens are replaced by UNK token \n",
        "  # we keep 199 most frequent tokens and indice 1 is UNK token (so we keep 198 tokens)\n",
        "  char_tokenizer = tf.keras.preprocessing.text.Tokenizer(char_level = True, filters = '', oov_token = 'UNK', num_words = 200)\n",
        "\n",
        "  if glove_model == None:\n",
        "    glove_model = download_glove_model(EMBEDDING_SIZE)\n",
        "\n",
        "  tokenized_questions = dataset['question'].apply(word_tokenize).to_list()\n",
        "\n",
        "  contexts = pd.Series(dataset['context'].unique())\n",
        "  tokenized_contexts = contexts.apply(word_tokenize).to_list()\n",
        "\n",
        "  sequences = glove_model.index2entity + tokenized_questions + tokenized_contexts\n",
        "\n",
        "  del glove_model # we  don't need anymore the glove model\n",
        "\n",
        "  tokenizer.fit_on_texts(sequences)\n",
        "  char_tokenizer.fit_on_texts(dataset['question'].to_list() + contexts.to_list())\n",
        "\n",
        "  return tokenizer, char_tokenizer\n",
        "  \n",
        "\n",
        "\n",
        "def update_tokenizer(dataset, tokenizer, char_tokenizer):\n",
        "\n",
        "  \"\"\"\n",
        "  update the existing word/char vocabulary on a new dataset\n",
        "  \"\"\"\n",
        "\n",
        "  tokenized_questions = dataset['question'].apply(word_tokenize).to_list()\n",
        "\n",
        "  contexts = pd.Series(dataset['context'].unique())\n",
        "  tokenized_contexts = contexts.apply(word_tokenize).to_list()\n",
        "\n",
        "  sequences = tokenized_questions + tokenized_contexts\n",
        "  tokenizer.fit_on_texts(sequences)\n",
        "\n",
        "  char_tokenizer.fit_on_texts(dataset['question'].to_list() + contexts.to_list())\n",
        "\n",
        "\n",
        "\n",
        "def get_start_end(row):\n",
        "\n",
        "  \"\"\"\n",
        "  get the start and end span for each sample,\n",
        "  if the span cannot be found return -1\n",
        "  \"\"\"\n",
        "\n",
        "  context = row['context']\n",
        "  answer = row['text']\n",
        "  tok_answer = word_tokenize(answer)\n",
        "\n",
        "  _start = context.find(answer)\n",
        "\n",
        "  if _start == -1:\n",
        "    # the answer is not in the context\n",
        "    # maybe due to a typo\n",
        "    row['start'] = -1\n",
        "    row['end'] = -1\n",
        "    return row\n",
        "\n",
        "  lc = context[:_start]\n",
        "  lc = word_tokenize(lc)\n",
        "\n",
        "  start = len(lc)\n",
        "  end = start + len(tok_answer)\n",
        "\n",
        "  row['start'] = start\n",
        "  row['end'] = end\n",
        "\n",
        "  return row\n",
        "\n",
        "def tokenize(dataset, tokenizer, char_tokenizer):\n",
        "\n",
        "  \"\"\"\n",
        "  tokenize the given dataset\n",
        "  \"\"\"\n",
        "\n",
        "  _dataset = dataset.copy()\n",
        "\n",
        "  tokenized_questions = _dataset['question'].apply(word_tokenize).to_list()\n",
        "  tokenized_contexts = _dataset['context'].apply(word_tokenize).to_list()\n",
        "\n",
        "  t_q = tokenizer.texts_to_sequences(tokenized_questions)\n",
        "  t_c = tokenizer.texts_to_sequences(tokenized_contexts)\n",
        "\n",
        "  c_q = []\n",
        "  c_c = []\n",
        "\n",
        "  for question, context in zip(tokenized_questions, tokenized_contexts):\n",
        "    _q = char_tokenizer.texts_to_sequences(question)\n",
        "    _c = char_tokenizer.texts_to_sequences(context)\n",
        "    c_q.append(_q)\n",
        "    c_c.append(_c)\n",
        "\n",
        "  _dataset['tokenized_question'] = t_q\n",
        "  _dataset['tokenized_context'] = t_c\n",
        "\n",
        "  _dataset['char_tokenized_question'] = c_q\n",
        "  _dataset['char_tokenized_context'] = c_c\n",
        "\n",
        "  return _dataset\n",
        "\n",
        "\n",
        "\n",
        "def split(dataset, train_size = 0.8):\n",
        "\n",
        "  \"\"\"\n",
        "  split the dataset in two part: the training and the validation based on titles\n",
        "  \"\"\"\n",
        "\n",
        "  # find unique titles\n",
        "  titles = squad_dataset['title']\n",
        "  unique_titles = titles.unique()\n",
        "\n",
        "\n",
        "  n_titles = len(unique_titles)\n",
        "  titles_seq = list(range(n_titles))\n",
        "\n",
        "  train_len = int(n_titles*train_size)\n",
        "\n",
        "  # sample train indexes\n",
        "  train_ind = random.sample(titles_seq, train_len)\n",
        "  test_ind = list(set(titles_seq) - set(train_ind))\n",
        "\n",
        "  train_titles = unique_titles[train_ind]\n",
        "  test_titles = unique_titles[test_ind]\n",
        "\n",
        "  squad_columns = list(squad_dataset.columns)\n",
        "\n",
        "  # initialize empty train and test df\n",
        "  train_data = pd.DataFrame(columns = squad_columns)\n",
        "  test_data = pd.DataFrame(columns = squad_columns)\n",
        "\n",
        "  for train_title in train_titles:\n",
        "\n",
        "    train_section = squad_dataset[squad_dataset['title'] == train_title]\n",
        "    train_data = train_data.append(train_section)\n",
        "\n",
        "  for test_title in test_titles:\n",
        "\n",
        "    test_section = squad_dataset[squad_dataset['title'] == test_title]\n",
        "    test_data = test_data.append(test_section)\n",
        "\n",
        "\n",
        "  return train_data, test_data\n",
        "\n",
        "\n",
        "\n",
        "def df_to_json(df, path, with_answer = True):\n",
        "\n",
        "  \"\"\"\n",
        "  parse the given dataframe into the SQUAD json format and\n",
        "  save it\n",
        "  \"\"\"\n",
        "  \n",
        "  data = []\n",
        "\n",
        "  for title, articles in df.groupby('title'):\n",
        "    chapter = {'title': title}\n",
        "    paragraphs = []\n",
        "    for context, contents in articles.groupby('context'):\n",
        "      paragraph = {'context': context}\n",
        "      qas = []\n",
        "      for i, content in contents.iterrows():\n",
        "        if with_answer:\n",
        "          qa = {'answers': [{'answer_start': content['answer_start'], 'text': content['text']}], 'question': content['question'], 'id': content['id']}\n",
        "        else:\n",
        "          qa = {'question': content['question'], 'id': content['id']}\n",
        "        qas.append(qa)\n",
        "      paragraph.update({'qas': qas})\n",
        "      paragraphs.append(paragraph)\n",
        "    chapter.update({'paragraphs': paragraphs})\n",
        "    data.append(chapter)\n",
        "  raw_data = {'data': data}\n",
        "\n",
        "  with open(path, 'w') as handle:\n",
        "    json.dump(raw_data, handle)\n",
        "\n",
        "  print(f'dataset saved in {path}')"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8pXuOXlpLsx"
      },
      "source": [
        "tr_df, vl_df = split(squad_dataset)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGk_DpS3RS32",
        "outputId": "91cc4c4d-0ba0-4aaf-b9ad-8056b9cb4d0b"
      },
      "source": [
        "tr_df.shape[0],vl_df.shape[0]"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(69129, 18470)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0VbHyp4YSUx"
      },
      "source": [
        "Our vocabulary is based on the Glove vocabulary, and we add terms from the training set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8hY8Z7HQFKjo",
        "outputId": "923e0247-2261-4edc-d463-7b9dec3634c3"
      },
      "source": [
        "tokenizer, char_tokenizer = get_tokenizer(tr_df)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] downloading glove 300\n",
            "[==================================================] 100.0% 376.1/376.1MB downloaded\n",
            "[INFO] done !\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILsFqw2bKf5L",
        "outputId": "2e5d6440-a3c9-4a2b-a468-236872ac7e8d"
      },
      "source": [
        "print(len(tokenizer.word_index))\n",
        "len(char_tokenizer.word_index)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "429064\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1263"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWoZPgxdYbkz"
      },
      "source": [
        "We then update our vocabulary with terms from the validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xzv_ZS3Zqifn"
      },
      "source": [
        "update_tokenizer(vl_df, tokenizer, char_tokenizer)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLwNWJsorIae",
        "outputId": "f2d46ba1-39d9-4014-82b6-72f3e989de7d"
      },
      "source": [
        "print(len(tokenizer.word_index))\n",
        "len(char_tokenizer.word_index)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "429758\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1265"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fM8oYX_SHkni"
      },
      "source": [
        "# take a while\n",
        "tr_df = tr_df.apply(get_start_end, axis = 1)\n",
        "vl_df = vl_df.apply(get_start_end, axis = 1)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ox1J7Z8YhVcv"
      },
      "source": [
        "we get rid of samples where the answer doesn't match the context (maybe there is a typo in the answer or the context).  \n",
        "To avoid to discard many samples, we could lemmatize / stem the text.   \n",
        "Obviously, lemmatization is a better choice for our task, but if we want a really accurate lemmatization processing, we need to do POS tagging."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZ3CXWdL8CNk"
      },
      "source": [
        "# we get rid of samples where the answer doesn't match the context\n",
        "tr_df = tr_df[tr_df['start'] != -1]\n",
        "vl_df = vl_df[vl_df['start'] != -1]"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_KJeyPxVZG6"
      },
      "source": [
        "tr_df = tokenize(tr_df, tokenizer, char_tokenizer)\n",
        "vl_df = tokenize(vl_df, tokenizer, char_tokenizer)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        },
        "id": "VJd7p05zjg0e",
        "outputId": "4a059861-8b92-47ad-f6e0-14c3d47028b3"
      },
      "source": [
        "tr_df.head()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>question</th>\n",
              "      <th>context</th>\n",
              "      <th>title</th>\n",
              "      <th>answer_start</th>\n",
              "      <th>text</th>\n",
              "      <th>c_id</th>\n",
              "      <th>start</th>\n",
              "      <th>end</th>\n",
              "      <th>tokenized_question</th>\n",
              "      <th>tokenized_context</th>\n",
              "      <th>char_tokenized_question</th>\n",
              "      <th>char_tokenized_context</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>65306</th>\n",
              "      <td>5728b2912ca10214002da5fa</td>\n",
              "      <td>what is the aggregate population of paris?</td>\n",
              "      <td>since the 19th century, the built-up area of p...</td>\n",
              "      <td>Paris</td>\n",
              "      <td>171</td>\n",
              "      <td>10,550,350</td>\n",
              "      <td>14151</td>\n",
              "      <td>30</td>\n",
              "      <td>31</td>\n",
              "      <td>[10, 12, 2, 9969, 101, 4, 376, 7]</td>\n",
              "      <td>[120, 2, 503, 83, 3, 2, 19402, 114, 4, 376, 40...</td>\n",
              "      <td>[[19, 11, 5, 4], [6, 9], [4, 11, 3], [5, 20, 2...</td>\n",
              "      <td>[[9, 6, 7, 14, 3], [4, 11, 3], [28, 36, 4, 11]...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65307</th>\n",
              "      <td>5728b2912ca10214002da5fb</td>\n",
              "      <td>from what census is this information from?</td>\n",
              "      <td>since the 19th century, the built-up area of p...</td>\n",
              "      <td>Paris</td>\n",
              "      <td>188</td>\n",
              "      <td>2012 census</td>\n",
              "      <td>14151</td>\n",
              "      <td>34</td>\n",
              "      <td>36</td>\n",
              "      <td>[25, 10, 799, 12, 42, 506, 25, 7]</td>\n",
              "      <td>[120, 2, 503, 83, 3, 2, 19402, 114, 4, 376, 40...</td>\n",
              "      <td>[[17, 10, 8, 16], [19, 11, 5, 4], [14, 3, 7, 9...</td>\n",
              "      <td>[[9, 6, 7, 14, 3], [4, 11, 3], [28, 36, 4, 11]...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65308</th>\n",
              "      <td>5728b2912ca10214002da5fc</td>\n",
              "      <td>what is the population of paris' metropolitan ...</td>\n",
              "      <td>since the 19th century, the built-up area of p...</td>\n",
              "      <td>Paris</td>\n",
              "      <td>282</td>\n",
              "      <td>12,341,418</td>\n",
              "      <td>14151</td>\n",
              "      <td>52</td>\n",
              "      <td>53</td>\n",
              "      <td>[10, 12, 2, 101, 4, 376, 98, 1172, 114, 7]</td>\n",
              "      <td>[120, 2, 503, 83, 3, 2, 19402, 114, 4, 376, 40...</td>\n",
              "      <td>[[19, 11, 5, 4], [6, 9], [4, 11, 3], [18, 8, 1...</td>\n",
              "      <td>[[9, 6, 7, 14, 3], [4, 11, 3], [28, 36, 4, 11]...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65309</th>\n",
              "      <td>5728b2912ca10214002da5fd</td>\n",
              "      <td>how many kilometers does the administrative re...</td>\n",
              "      <td>since the 19th century, the built-up area of p...</td>\n",
              "      <td>Paris</td>\n",
              "      <td>388</td>\n",
              "      <td>12,012 km²</td>\n",
              "      <td>14151</td>\n",
              "      <td>71</td>\n",
              "      <td>73</td>\n",
              "      <td>[36, 38, 3763, 56, 2, 1666, 221, 1437, 7]</td>\n",
              "      <td>[120, 2, 503, 83, 3, 2, 19402, 114, 4, 376, 40...</td>\n",
              "      <td>[[11, 8, 19], [16, 5, 7, 21], [27, 6, 12, 8, 1...</td>\n",
              "      <td>[[9, 6, 7, 14, 3], [4, 11, 3], [28, 36, 4, 11]...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65310</th>\n",
              "      <td>5728b2912ca10214002da5fe</td>\n",
              "      <td>as of 2014 how many inhabitants lived in the a...</td>\n",
              "      <td>since the 19th century, the built-up area of p...</td>\n",
              "      <td>Paris</td>\n",
              "      <td>431</td>\n",
              "      <td>12 million</td>\n",
              "      <td>14151</td>\n",
              "      <td>80</td>\n",
              "      <td>82</td>\n",
              "      <td>[14, 4, 500, 36, 38, 1976, 1064, 6, 2, 1666, 2...</td>\n",
              "      <td>[120, 2, 503, 83, 3, 2, 19402, 114, 4, 376, 40...</td>\n",
              "      <td>[[5, 9], [8, 17], [31, 29, 28, 44], [11, 8, 19...</td>\n",
              "      <td>[[9, 6, 7, 14, 3], [4, 11, 3], [28, 36, 4, 11]...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                          index  ...                             char_tokenized_context\n",
              "65306  5728b2912ca10214002da5fa  ...  [[9, 6, 7, 14, 3], [4, 11, 3], [28, 36, 4, 11]...\n",
              "65307  5728b2912ca10214002da5fb  ...  [[9, 6, 7, 14, 3], [4, 11, 3], [28, 36, 4, 11]...\n",
              "65308  5728b2912ca10214002da5fc  ...  [[9, 6, 7, 14, 3], [4, 11, 3], [28, 36, 4, 11]...\n",
              "65309  5728b2912ca10214002da5fd  ...  [[9, 6, 7, 14, 3], [4, 11, 3], [28, 36, 4, 11]...\n",
              "65310  5728b2912ca10214002da5fe  ...  [[9, 6, 7, 14, 3], [4, 11, 3], [28, 36, 4, 11]...\n",
              "\n",
              "[5 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRlxHm3xwnFn"
      },
      "source": [
        "We display some useful stats in order to define the padding size (at the word and character level, for both question and context)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2kw5Pk4c6I3S",
        "outputId": "cc7dd898-e57c-4bc3-f21e-091ee709a86b"
      },
      "source": [
        "print(tr_df['tokenized_question'].str.len().describe())\n",
        "vl_df['tokenized_question'].str.len().describe()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "count    69045.000000\n",
            "mean        11.337113\n",
            "std          3.746685\n",
            "min          1.000000\n",
            "25%          9.000000\n",
            "50%         11.000000\n",
            "75%         13.000000\n",
            "max         49.000000\n",
            "Name: tokenized_question, dtype: float64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    18470.000000\n",
              "mean        11.102166\n",
              "std          3.630045\n",
              "min          3.000000\n",
              "25%          9.000000\n",
              "50%         11.000000\n",
              "75%         13.000000\n",
              "max         60.000000\n",
              "Name: tokenized_question, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dk5TG_Yy9crW",
        "outputId": "55e24bbd-2c84-4f91-9f0a-f581f5f02382"
      },
      "source": [
        "print(tr_df['tokenized_question'].str.len().quantile(0.99))\n",
        "vl_df['tokenized_question'].str.len().quantile(0.99)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "23.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "22.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bsKYKsgR6-UK",
        "outputId": "dfaef739-b00d-4ee5-c289-47251a88c98b"
      },
      "source": [
        "print(tr_df['tokenized_context'].str.len().describe())\n",
        "vl_df['tokenized_context'].str.len().describe()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "count    69045.000000\n",
            "mean       138.315707\n",
            "std         56.355860\n",
            "min         22.000000\n",
            "25%        102.000000\n",
            "50%        127.000000\n",
            "75%        164.000000\n",
            "max        766.000000\n",
            "Name: tokenized_context, dtype: float64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    18470.000000\n",
              "mean       135.406659\n",
              "std         58.085212\n",
              "min         25.000000\n",
              "25%        100.000000\n",
              "50%        124.000000\n",
              "75%        161.000000\n",
              "max        638.000000\n",
              "Name: tokenized_context, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cyp5MUtj9woq",
        "outputId": "371b6cc9-591f-4cd0-90f8-2c7c171a7add"
      },
      "source": [
        "print(tr_df['tokenized_context'].str.len().quantile(0.99))\n",
        "vl_df['tokenized_context'].str.len().quantile(0.99)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "323.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "342.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fE3J6xJZ7Sqe"
      },
      "source": [
        "def len_words(dataset):\n",
        "\n",
        "  \"\"\"\n",
        "  return the word's length\n",
        "  \"\"\"\n",
        "\n",
        "  count_q = []\n",
        "  count_c = []\n",
        "\n",
        "  for idx, row in dataset.iterrows():\n",
        "    for w in row['char_tokenized_question']:\n",
        "      l = len(w)\n",
        "      count_q.append(l)\n",
        "      \n",
        "    for w in row['char_tokenized_context']:\n",
        "      m = len(w)\n",
        "      count_c.append(m)\n",
        "  \n",
        "  return pd.Series(count_q), pd.Series(count_c)\n",
        "\n",
        "t_q,t_c = len_words(tr_df)\n",
        "v_q,v_c = len_words(vl_df)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i8R_-bUv9HGQ",
        "outputId": "5183783c-4aa4-4bde-bb65-47efbd656325"
      },
      "source": [
        "print(t_q.describe())\n",
        "t_c.describe()"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "count    782771.000000\n",
            "mean          4.450666\n",
            "std           2.687285\n",
            "min           1.000000\n",
            "25%           2.000000\n",
            "50%           4.000000\n",
            "75%           6.000000\n",
            "max          30.000000\n",
            "dtype: float64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    9.550008e+06\n",
              "mean     4.623795e+00\n",
              "std      2.974525e+00\n",
              "min      1.000000e+00\n",
              "25%      2.000000e+00\n",
              "50%      4.000000e+00\n",
              "75%      7.000000e+00\n",
              "max      3.700000e+01\n",
              "dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R64fJS7DxkuN",
        "outputId": "ebee3e29-db48-44c2-d6b0-1aa9ca7c6bb2"
      },
      "source": [
        "print(v_q.describe())\n",
        "v_c.describe()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "count    205057.000000\n",
            "mean          4.442604\n",
            "std           2.649141\n",
            "min           1.000000\n",
            "25%           3.000000\n",
            "50%           4.000000\n",
            "75%           6.000000\n",
            "max          26.000000\n",
            "dtype: float64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    2.500961e+06\n",
              "mean     4.638254e+00\n",
              "std      2.953675e+00\n",
              "min      1.000000e+00\n",
              "25%      2.000000e+00\n",
              "50%      4.000000e+00\n",
              "75%      7.000000e+00\n",
              "max      3.200000e+01\n",
              "dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5wxf3tS7xylx",
        "outputId": "ebeb600d-a6fe-40b7-edd7-5f8cbffa962d"
      },
      "source": [
        "print(t_q.quantile(0.99))\n",
        "t_c.quantile(0.99)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6F-O0qjx4Gu",
        "outputId": "e8085d91-82e9-4668-a5c5-fdecb78e56ce"
      },
      "source": [
        "print(v_q.quantile(0.99))\n",
        "v_c.quantile(0.99)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LC1K9nYs9ht2"
      },
      "source": [
        "There are obviously some outliers. We are compeled to get rid of some samples because of memory issues.\n",
        "\n",
        "We will get rid of contexts that have more than 400 words and questions that have more than 25 words.\n",
        "\n",
        "We will set the length of a word to 15 characters\n",
        "\n",
        "**EDIT :** These numbers are huge but we won't get out of memory errors if we build a sequence generator. If you don't want to use the sequence generator, you should reduce these numbers.\n",
        "\n",
        "**EDIT :** Now that we use a sequence generator, we could define `*_MAXLEN` variables according to the stats provided by the training set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3WiAI_oA_3lY"
      },
      "source": [
        "QUESTION_MAXLEN = 25\n",
        "CONTEXT_MAXLEN = 400\n",
        "WORD_MAXLEN = 15\n",
        "BATCH_SIZE = 10"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fZMoWqTL2Wz",
        "outputId": "f4d195f3-6013-4300-82f0-2ba188591c97"
      },
      "source": [
        "tr_df.shape, vl_df.shape"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((69045, 13), (18470, 13))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibILzhoh-Mf8"
      },
      "source": [
        "tr_df = tr_df[(tr_df['tokenized_question'].str.len() <= QUESTION_MAXLEN) & (tr_df['tokenized_context'].str.len() <= CONTEXT_MAXLEN) & (tr_df['start'] <= CONTEXT_MAXLEN) & (tr_df['end'] <= CONTEXT_MAXLEN) ].reset_index(drop = True)\n",
        "vl_df = vl_df[(vl_df['tokenized_question'].str.len() <= QUESTION_MAXLEN) & (vl_df['tokenized_context'].str.len() <= CONTEXT_MAXLEN) & (vl_df['start'] <= CONTEXT_MAXLEN) & (vl_df['end'] <= CONTEXT_MAXLEN) ].reset_index(drop = True)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQTuvAo7oGcA",
        "outputId": "ae4a02df-b1b3-4f36-d902-8a0ff97b8574"
      },
      "source": [
        "tr_df.shape[0], vl_df.shape[0]"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(68630, 18389)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93mWQxFLG3z9",
        "outputId": "bce8fbe0-4630-4384-b8b3-bb3942d7ab8e"
      },
      "source": [
        " print(f' we get rid of : {SAMPLES - (tr_df.shape[0] + vl_df.shape[0])} samples')"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " we get rid of : 580 samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4lFv9wpDfDOj",
        "outputId": "ed0b4139-02d7-4f7f-f2b9-9ac2c9654289"
      },
      "source": [
        "# save datasets in json format\n",
        "path_to_train_set = os.path.join(os.getcwd(), 'drive/MyDrive/NLP/BIDAF/utils/data/train_set.json')\n",
        "df_to_json(tr_df, path_to_train_set)\n",
        "\n",
        "path_to_valid_set = os.path.join(os.getcwd(), 'drive/MyDrive/NLP/BIDAF/utils/data/valid_set.json')\n",
        "df_to_json(vl_df, path_to_valid_set)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dataset saved in /content/drive/MyDrive/NLP/BIDAF/utils/data/train_set.json\n",
            "dataset saved in /content/drive/MyDrive/NLP/BIDAF/utils/data/valid_set.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Owvocjs5JCPa"
      },
      "source": [
        "# we save both tokenizers\n",
        "tokenizers_folder = os.path.join(os.getcwd(), 'drive/MyDrive/NLP/BIDAF/utils', 'tokenizers')\n",
        "if not os.path.exists(tokenizers_folder):\n",
        "  os.makedirs(tokenizers_folder)\n",
        "\n",
        "path_word_tokenizer = os.path.join(tokenizers_folder, 'word_tokenizer.pkl')\n",
        "with open(path_word_tokenizer, 'wb') as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol = pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "path_char_tokenizer = os.path.join(tokenizers_folder, 'char_tokenizer.pkl')\n",
        "with open(path_char_tokenizer, 'wb') as handle:\n",
        "    pickle.dump(char_tokenizer, handle, protocol = pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XX5aJeXaqnZh"
      },
      "source": [
        "We create the iterator. The iterator allows us to work with much bigger data, because it is loaded into memory only when we need them"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMEr6c_hiv6Q"
      },
      "source": [
        "# utils/datasets/dataset.py\n",
        "class SQUAD_dataset(tf.keras.utils.Sequence):\n",
        "\n",
        "  \"\"\"\n",
        "  utility class to create a working dataset that\n",
        "  can be given to a neural network\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, data, question_maxlen, context_maxlen, word_maxlen, batch_size, with_answer = True):\n",
        "    self.QUESTION_MAXLEN = question_maxlen\n",
        "    self.CONTEXT_MAXLEN = context_maxlen\n",
        "    self.WORD_MAXLEN = word_maxlen\n",
        "    self.batch_size = batch_size\n",
        "    self.with_answer = with_answer\n",
        "    self.__get_batches(data)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.batches)\n",
        "\n",
        "  def __get_batches(self, data):\n",
        "    batches = [data[i:i+self.batch_size] for i in range(0, len(data), self.batch_size)]\n",
        "    self.batches = batches\n",
        "\n",
        "  def __repr__(self):\n",
        "    template = '''SQUAD_dataset : questions : ({0}, {1}), contexts : ({0}, {2}), char_questions : ({0}, {1}, {3}), char_contexts : ({0}, {2}, {3}), id : ({0}, 1)'''.format(self.batch_size, self.QUESTION_MAXLEN, self.CONTEXT_MAXLEN, self.WORD_MAXLEN)\n",
        "    return template\n",
        "\n",
        "  @classmethod\n",
        "  def from_file(cls, path):\n",
        "    path = os.path.join(os.getcwd(), path)\n",
        "    with open(path, 'rb') as handle:\n",
        "      dataset = pickle.load(handle)\n",
        "    return dataset\n",
        "\n",
        "  def to_pickle(self, path):\n",
        "    path = os.path.join(os.getcwd(), path)\n",
        "    folder = os.path.dirname(path)\n",
        "\n",
        "    if not os.path.exists(folder):\n",
        "      os.makedirs(folder)\n",
        "\n",
        "    with open(path, 'wb') as handle:\n",
        "      pickle.dump(self, handle, protocol = pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    batch = self.batches[idx].reset_index(drop = True)\n",
        "\n",
        "    id = np.asarray(batch['id'])\n",
        "\n",
        "    # questions and contexts words padding\n",
        "    q_w = tf.keras.preprocessing.sequence.pad_sequences(batch['tokenized_question'], padding = 'post', maxlen = self.QUESTION_MAXLEN)\n",
        "    c_w = tf.keras.preprocessing.sequence.pad_sequences(batch['tokenized_context'], padding = 'post', maxlen = self.CONTEXT_MAXLEN)\n",
        "\n",
        "    # question_char padding\n",
        "    q_c = np.zeros((q_w.shape[0], self.QUESTION_MAXLEN, self.WORD_MAXLEN), dtype = np.int32)\n",
        "\n",
        "    for i, value in batch['char_tokenized_question'].iteritems():\n",
        "      v = tf.keras.preprocessing.sequence.pad_sequences(value, padding = 'post', maxlen = self.WORD_MAXLEN, truncating = 'post')\n",
        "      to_add = self.QUESTION_MAXLEN - v.shape[0]\n",
        "      add = np.zeros((to_add, self.WORD_MAXLEN))\n",
        "      arr = np.vstack([v,add])\n",
        "      q_c[i] = arr\n",
        "\n",
        "    # context_char padding\n",
        "    c_c = np.zeros((q_w.shape[0], self.CONTEXT_MAXLEN, self.WORD_MAXLEN), dtype = np.int32)\n",
        "\n",
        "    for i, value in batch['char_tokenized_context'].iteritems():\n",
        "      v = tf.keras.preprocessing.sequence.pad_sequences(value, padding = 'post', maxlen = self.WORD_MAXLEN, truncating = 'post')\n",
        "      to_add = self.CONTEXT_MAXLEN - v.shape[0]\n",
        "      add = np.zeros((to_add, self.WORD_MAXLEN))\n",
        "      arr = np.vstack([v,add])\n",
        "      c_c[i] = arr\n",
        "\n",
        "    # one hot encode start and end\n",
        "    if self.with_answer:\n",
        "      y_start = tf.keras.utils.to_categorical(batch['start'].values, self.CONTEXT_MAXLEN)\n",
        "      y_end = tf.keras.utils.to_categorical(batch['end'].values, self.CONTEXT_MAXLEN)\n",
        "\n",
        "      # (inputs), (outputs), (id)\n",
        "      return (q_w, c_w, q_c, c_c), (y_start, y_end), (id,)\n",
        "    return (q_c, c_w, q_c, c_c), (id,)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGTS5gSunCRa"
      },
      "source": [
        "tr_data = SQUAD_dataset(tr_df, batch_size = BATCH_SIZE, question_maxlen = QUESTION_MAXLEN, context_maxlen = CONTEXT_MAXLEN, word_maxlen = WORD_MAXLEN)\n",
        "vl_data = SQUAD_dataset(vl_df, batch_size = BATCH_SIZE, question_maxlen = QUESTION_MAXLEN, context_maxlen = CONTEXT_MAXLEN, word_maxlen = WORD_MAXLEN)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9VciOKiQtyAx",
        "outputId": "c65b7825-086e-4967-8083-8196a32642cd"
      },
      "source": [
        "# number of batches\n",
        "print(len(tr_data))\n",
        "len(vl_data)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6863\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1839"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1GtQIVC9zZ5g",
        "outputId": "669e6009-9a5f-41a2-8b5b-0382a17d7048"
      },
      "source": [
        "tr_data"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SQUAD_dataset : questions : (10, 25), contexts : (10, 400), char_questions : (10, 25, 15), char_contexts : (10, 400, 15), id : (10, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etbiAh9h1-Fg"
      },
      "source": [
        "tr_data.to_pickle('drive/MyDrive/NLP/BIDAF/utils/datasets/train_dataset.pkl')\n",
        "vl_data.to_pickle('drive/MyDrive/NLP/BIDAF/utils/datasets/valid_dataset.pkl')"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSo5-GcJQe18"
      },
      "source": [
        "Now that the preprocessing is over we can preprocess a (mock) unseen dataset. It is basically the same that the one we have seen just before, but it does not contain the start and end span (text and answer_start fields)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8eli84fbubp",
        "outputId": "01f8d755-5f6a-4a8f-8641-19caf072ef2c"
      },
      "source": [
        "# with_answer = False to parse a dataset with no answer\n",
        "unseen_dataset = load_dataset(SQUAD_PATH, with_answer = False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading the json file\n",
            "[INFO] processing...\n",
            "[INFO] there are 87599 questions with single answer\n",
            "[INFO] there are 18891 different contexts\n",
            "[INFO] there are 442 unrelated subjects\n",
            "[INFO] Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "au8aZNFfde5I"
      },
      "source": [
        "s = unseen_dataset.shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "McWg6RJy-NOl",
        "outputId": "428c45c3-e90e-4d07-e499-8c05d68961c9"
      },
      "source": [
        "unseen_dataset = clean_dataset(unseen_dataset, with_answer = False)\n",
        "unseen_dataset = tokenize(unseen_dataset, tokenizer, char_tokenizer)\n",
        "unseen_dataset = unseen_dataset[(unseen_dataset['tokenized_question'].str.len() <= QUESTION_MAXLEN) & (unseen_dataset['tokenized_context'].str.len() <= CONTEXT_MAXLEN)].reset_index(drop = True)\n",
        "print(f' we get rid of : {s - (unseen_dataset.shape[0])} samples')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " we get rid of : 496 samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2oPFVcMAAIb-",
        "outputId": "e8ddb325-d2ff-4c16-8aeb-82b3bc57a435"
      },
      "source": [
        "unseen_path = os.path.join(os.getcwd(), 'drive/MyDrive/NLP/BIDAF/utils/data/unseen_set.json')\n",
        "df_to_json(unseen_dataset, unseen_path, with_answer = False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dataset saved in /content/drive/MyDrive/NLP/BIDAF/utils/data/unseen_set.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYsbL8bs_ZFf"
      },
      "source": [
        "unseen_data = SQUAD_dataset(unseen_dataset, batch_size = BATCH_SIZE, question_maxlen = QUESTION_MAXLEN, context_maxlen = CONTEXT_MAXLEN, word_maxlen = WORD_MAXLEN, with_answer = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "koM-e00x_wsM",
        "outputId": "840f4327-399a-4f3a-9553-884f20d6dab6"
      },
      "source": [
        "unseen_data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SQUAD_dataset : questions : (10, 25), contexts : (10, 400), char_questions : (10, 25, 15), char_contexts : (10, 400, 15), id : (10, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOWz8V-2_yze"
      },
      "source": [
        "unseen_data.to_pickle('drive/MyDrive/NLP/BIDAF/utils/datasets/unseen_dataset.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}