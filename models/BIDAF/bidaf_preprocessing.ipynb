{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VrXpyz2vZhs1"
   },
   "source": [
    "In order to run this notebook, the first things you should do are :\n",
    "* pip install pandas numpy tensorflow nltk gensim sklearn\n",
    "* modify the `SQUAD_PATH` variable (path to squad file)\n",
    "* modify all others paths (where to save datasets, tokenizers...)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6109,
     "status": "ok",
     "timestamp": 1613671332105,
     "user": {
      "displayName": "corentin magyar",
      "photoUrl": "",
      "userId": "04135457892709862388"
     },
     "user_tz": -60
    },
    "id": "hETK3Axg43yB",
    "outputId": "69633036-dab6-483e-a66e-b21745b0bbd1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "nltk.download('punkt')\n",
    "import gensim.downloader as gloader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import pickle\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 6107,
     "status": "ok",
     "timestamp": 1613671332106,
     "user": {
      "displayName": "corentin magyar",
      "photoUrl": "",
      "userId": "04135457892709862388"
     },
     "user_tz": -60
    },
    "id": "VqLS5R574tdD"
   },
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 300\n",
    "SQUAD_PATH = os.path.join(os.getcwd(), 'drive/MyDrive/NLP/BIDAF/utils/data/training_set.json')\n",
    "\n",
    "def load_dataset(path, record_path = ['data', 'paragraphs', 'qas', 'answers'], verbose = True, with_answer = True):\n",
    "\n",
    "  \"\"\"\n",
    "  parse the SQUAD dataset into a dataframe\n",
    "  \"\"\"\n",
    "\n",
    "  if verbose:\n",
    "      print(\"Reading the json file\")\n",
    "  # if the file encoding is not UTF8 an exception should be raised    \n",
    "  file = json.loads(open(path).read())\n",
    "\n",
    "  if verbose:\n",
    "      print(\"[INFO] processing...\")\n",
    "\n",
    "  # parsing different level's in the json file\n",
    "  if with_answer:\n",
    "    js = pd.json_normalize(file , record_path )\n",
    "  m = pd.json_normalize(file, record_path[:-1] )\n",
    "  r = pd.json_normalize(file, record_path[:-2])\n",
    "  title = pd.json_normalize(file['data'], record_path = ['paragraphs'], meta = 'title')\n",
    "  t = pd.json_normalize(file, record_path[0])\n",
    "\n",
    "  #combining it into single dataframe\n",
    "  contexts = np.repeat(r['context'].values, r['qas'].str.len())\n",
    "  m['context'] = contexts\n",
    "  m['title'] = np.repeat(title['title'].values, r['qas'].str.len())\n",
    "  m['c_id'] = m['context'].factorize()[0]\n",
    "  m = m.drop(['answers'], axis = 1)\n",
    "\n",
    "  if with_answer:\n",
    "    main = js.merge(m, left_index = True, right_index = True)\n",
    "  else:\n",
    "    main = m\n",
    "  if verbose:\n",
    "      print(f\"[INFO] there are {main.shape[0]} questions with single answer\")\n",
    "      print(f\"[INFO] there are {main.groupby('c_id').sum().shape[0]} different contexts\")\n",
    "      print(f\"[INFO] there are {len(t)} unrelated subjects\")\n",
    "      print(\"[INFO] Done\")\n",
    "  return main\n",
    "\n",
    "def download_glove_model(embedding_dimension = 50):\n",
    "\n",
    "  \"\"\"\n",
    "  download glove model\n",
    "  \"\"\"\n",
    "\n",
    "  download_path = \"glove-wiki-gigaword-{}\".format(embedding_dimension)\n",
    "  try:\n",
    "    print('[INFO] downloading glove {}'.format(embedding_dimension))\n",
    "    emb_model = gloader.load(download_path)\n",
    "    print('[INFO] done !')\n",
    "  except ValueError as e:\n",
    "      print(\"Glove: 50, 100, 200, 300\")\n",
    "      raise e\n",
    "  return emb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18926,
     "status": "ok",
     "timestamp": 1613671344934,
     "user": {
      "displayName": "corentin magyar",
      "photoUrl": "",
      "userId": "04135457892709862388"
     },
     "user_tz": -60
    },
    "id": "2virMOF85A8W",
    "outputId": "0b56ed21-c3a9-43b2-d471-35a6405b5390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the json file\n",
      "[INFO] processing...\n",
      "[INFO] there are 87599 questions with single answer\n",
      "[INFO] there are 18891 different contexts\n",
      "[INFO] there are 442 unrelated subjects\n",
      "[INFO] Done\n"
     ]
    }
   ],
   "source": [
    "squad_dataset = load_dataset(SQUAD_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "executionInfo": {
     "elapsed": 18905,
     "status": "ok",
     "timestamp": 1613671344936,
     "user": {
      "displayName": "corentin magyar",
      "photoUrl": "",
      "userId": "04135457892709862388"
     },
     "user_tz": -60
    },
    "id": "lzHPUWA45dzO",
    "outputId": "8e73a3f3-36d5-49b9-fd17-1356072813eb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer_start</th>\n",
       "      <th>text</th>\n",
       "      <th>question</th>\n",
       "      <th>id</th>\n",
       "      <th>context</th>\n",
       "      <th>title</th>\n",
       "      <th>c_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>515</td>\n",
       "      <td>Saint Bernadette Soubirous</td>\n",
       "      <td>To whom did the Virgin Mary allegedly appear i...</td>\n",
       "      <td>5733be284776f41900661182</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>188</td>\n",
       "      <td>a copper statue of Christ</td>\n",
       "      <td>What is in front of the Notre Dame Main Building?</td>\n",
       "      <td>5733be284776f4190066117f</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>279</td>\n",
       "      <td>the Main Building</td>\n",
       "      <td>The Basilica of the Sacred heart at Notre Dame...</td>\n",
       "      <td>5733be284776f41900661180</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>381</td>\n",
       "      <td>a Marian place of prayer and reflection</td>\n",
       "      <td>What is the Grotto at Notre Dame?</td>\n",
       "      <td>5733be284776f41900661181</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>92</td>\n",
       "      <td>a golden statue of the Virgin Mary</td>\n",
       "      <td>What sits on top of the Main Building at Notre...</td>\n",
       "      <td>5733be284776f4190066117e</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   answer_start  ... c_id\n",
       "0           515  ...    0\n",
       "1           188  ...    0\n",
       "2           279  ...    0\n",
       "3           381  ...    0\n",
       "4            92  ...    0\n",
       "\n",
       "[5 rows x 7 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 18901,
     "status": "ok",
     "timestamp": 1613671344938,
     "user": {
      "displayName": "corentin magyar",
      "photoUrl": "",
      "userId": "04135457892709862388"
     },
     "user_tz": -60
    },
    "id": "oCWKKV2RAhML"
   },
   "outputs": [],
   "source": [
    "SAMPLES = squad_dataset.shape[0]\n",
    "\n",
    "def preprocess_sentence(text):\n",
    "\n",
    "  \"\"\"\n",
    "  lowercase and strip the given text\n",
    "  \"\"\"\n",
    "\n",
    "  text = text.lower()\n",
    "  text = text.strip()\n",
    "  return text\n",
    "\n",
    "def clean_dataset(dataset, with_answer = True):\n",
    "\n",
    "  \"\"\"\n",
    "  preprocess the dataset\n",
    "  \"\"\"\n",
    "\n",
    "  _dataset = dataset.copy()\n",
    "\n",
    "  cleaned_questions = _dataset['question'].apply(preprocess_sentence)\n",
    "\n",
    "  # we process only different contexts and then we duplicate them\n",
    "  unique_context = pd.Series(_dataset['context'].unique())\n",
    "  count_c = _dataset.groupby('c_id').size()\n",
    "  cleaned_contexts = unique_context.apply(preprocess_sentence)\n",
    "\n",
    "  _dataset['question'] = cleaned_questions\n",
    "\n",
    "  if with_answer:\n",
    "    cleaned_texts = _dataset['text'].apply(preprocess_sentence)\n",
    "    _dataset['text'] = cleaned_texts\n",
    "  _dataset['context'] = pd.Series(np.repeat(cleaned_contexts, count_c).tolist())\n",
    "\n",
    "  return _dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 19191,
     "status": "ok",
     "timestamp": 1613671345232,
     "user": {
      "displayName": "corentin magyar",
      "photoUrl": "",
      "userId": "04135457892709862388"
     },
     "user_tz": -60
    },
    "id": "HSVZ45xxqqqz"
   },
   "outputs": [],
   "source": [
    "squad_dataset = clean_dataset(squad_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 19191,
     "status": "ok",
     "timestamp": 1613671345234,
     "user": {
      "displayName": "corentin magyar",
      "photoUrl": "",
      "userId": "04135457892709862388"
     },
     "user_tz": -60
    },
    "id": "Rq8B58osDLvX"
   },
   "outputs": [],
   "source": [
    "def get_tokenizer(dataset, glove_model = None):\n",
    "\n",
    "  \"\"\"\n",
    "  create the word and char tokenizers and feed them \n",
    "  on the given dataset and the glove vocabulary\n",
    "  \"\"\"\n",
    "\n",
    "  tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token = 'UNK', filters = '')\n",
    "\n",
    "  # we will only keep the 200 - 1 most frequent characters (otherwise oom issue)\n",
    "  # others tokens are replaced by UNK token \n",
    "  # we keep 199 most frequent tokens and indice 1 is UNK token (so we keep 198 tokens)\n",
    "  char_tokenizer = tf.keras.preprocessing.text.Tokenizer(char_level = True, filters = '', oov_token = 'UNK', num_words = 200)\n",
    "\n",
    "  if glove_model == None:\n",
    "    glove_model = download_glove_model(EMBEDDING_SIZE)\n",
    "\n",
    "  tokenized_questions = dataset['question'].apply(word_tokenize).to_list()\n",
    "\n",
    "  contexts = pd.Series(dataset['context'].unique())\n",
    "  tokenized_contexts = contexts.apply(word_tokenize).to_list()\n",
    "\n",
    "  sequences = glove_model.index2entity + tokenized_questions + tokenized_contexts\n",
    "\n",
    "  del glove_model # we  don't need anymore the glove model\n",
    "\n",
    "  tokenizer.fit_on_texts(sequences)\n",
    "  char_tokenizer.fit_on_texts(dataset['question'].to_list() + contexts.to_list())\n",
    "\n",
    "  return tokenizer, char_tokenizer\n",
    "\n",
    "\n",
    "def update_tokenizer(dataset, tokenizer, char_tokenizer):\n",
    "\n",
    "  \"\"\"\n",
    "  update the existing word/char vocabulary on a new dataset\n",
    "  \"\"\"\n",
    "\n",
    "  tokenized_questions = dataset['question'].apply(word_tokenize).to_list()\n",
    "\n",
    "  contexts = pd.Series(dataset['context'].unique())\n",
    "  tokenized_contexts = contexts.apply(word_tokenize).to_list()\n",
    "\n",
    "  sequences = tokenized_questions + tokenized_contexts\n",
    "  tokenizer.fit_on_texts(sequences)\n",
    "\n",
    "  char_tokenizer.fit_on_texts(dataset['question'].to_list() + contexts.to_list())\n",
    "\n",
    "def get_start_end(row):\n",
    "\n",
    "  \"\"\"\n",
    "  get the start and end span for each sample,\n",
    "  if the span cannot be found return -1\n",
    "  \"\"\"\n",
    "\n",
    "  context = row['context']\n",
    "  answer = row['text']\n",
    "  tok_answer = word_tokenize(answer)\n",
    "\n",
    "  _start = context.find(answer)\n",
    "\n",
    "  if _start == -1:\n",
    "    # the answer is not in the context\n",
    "    # maybe due to a typo\n",
    "    row['start'] = -1\n",
    "    row['end'] = -1\n",
    "    return row\n",
    "\n",
    "  lc = context[:_start]\n",
    "  lc = word_tokenize(lc)\n",
    "\n",
    "  start = len(lc)\n",
    "  end = start + len(tok_answer)\n",
    "\n",
    "  row['start'] = start\n",
    "  row['end'] = end\n",
    "\n",
    "  return row\n",
    "\n",
    "def tokenize(dataset, tokenizer, char_tokenizer):\n",
    "\n",
    "  \"\"\"\n",
    "  tokenize the given dataset\n",
    "  \"\"\"\n",
    "\n",
    "  _dataset = dataset.copy()\n",
    "\n",
    "  tokenized_questions = _dataset['question'].apply(word_tokenize).to_list()\n",
    "  tokenized_contexts = _dataset['context'].apply(word_tokenize).to_list()\n",
    "\n",
    "  t_q = tokenizer.texts_to_sequences(tokenized_questions)\n",
    "  t_c = tokenizer.texts_to_sequences(tokenized_contexts)\n",
    "\n",
    "  c_q = []\n",
    "  c_c = []\n",
    "\n",
    "  for question, context in zip(tokenized_questions, tokenized_contexts):\n",
    "    _q = char_tokenizer.texts_to_sequences(question)\n",
    "    _c = char_tokenizer.texts_to_sequences(context)\n",
    "    c_q.append(_q)\n",
    "    c_c.append(_c)\n",
    "\n",
    "  _dataset['tokenized_question'] = t_q\n",
    "  _dataset['tokenized_context'] = t_c\n",
    "\n",
    "  _dataset['char_tokenized_question'] = c_q\n",
    "  _dataset['char_tokenized_context'] = c_c\n",
    "\n",
    "  return _dataset\n",
    "\n",
    "def split(dataset, test_size = 0.2, random_state = 42):\n",
    "\n",
    "  \"\"\"\n",
    "  split the dataset in two part: the training and the validation\n",
    "  \"\"\"\n",
    "\n",
    "  # random_state for deterministic state\n",
    "  tr, vl = train_test_split(dataset, test_size = test_size, random_state = random_state)\n",
    "  tr.reset_index(drop = True, inplace = True)\n",
    "  vl.reset_index(drop = True, inplace = True)\n",
    "\n",
    "  return tr,vl\n",
    "\n",
    "def df_to_json(df, path, with_answer = True):\n",
    "\n",
    "  \"\"\"\n",
    "  parse the given dataframe into the SQUAD json format and\n",
    "  save it\n",
    "  \"\"\"\n",
    "  \n",
    "  data = []\n",
    "\n",
    "  for title, articles in df.groupby('title'):\n",
    "    chapter = {'title': title}\n",
    "    paragraphs = []\n",
    "    for context, contents in articles.groupby('context'):\n",
    "      paragraph = {'context': context}\n",
    "      qas = []\n",
    "      for i, content in contents.iterrows():\n",
    "        if with_answer:\n",
    "          qa = {'answers': [{'answer_start': content['answer_start'], 'text': content['text']}], 'question': content['question'], 'id': content['id']}\n",
    "        else:\n",
    "          qa = {'question': content['question'], 'id': content['id']}\n",
    "        qas.append(qa)\n",
    "      paragraph.update({'qas': qas})\n",
    "      paragraphs.append(paragraph)\n",
    "    chapter.update({'paragraphs': paragraphs})\n",
    "    data.append(chapter)\n",
    "  raw_data = {'data': data}\n",
    "\n",
    "  with open(path, 'w') as handle:\n",
    "    json.dump(raw_data, handle)\n",
    "\n",
    "  print(f'dataset saved in {path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 19188,
     "status": "ok",
     "timestamp": 1613671345235,
     "user": {
      "displayName": "corentin magyar",
      "photoUrl": "",
      "userId": "04135457892709862388"
     },
     "user_tz": -60
    },
    "id": "M8pXuOXlpLsx"
   },
   "outputs": [],
   "source": [
    "tr_df, vl_df = split(squad_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19183,
     "status": "ok",
     "timestamp": 1613671345237,
     "user": {
      "displayName": "corentin magyar",
      "photoUrl": "",
      "userId": "04135457892709862388"
     },
     "user_tz": -60
    },
    "id": "BGk_DpS3RS32",
    "outputId": "122cb393-201a-4fd4-eafb-6b182c059f05"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70079, 17520)"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_df.shape[0],vl_df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v0VbHyp4YSUx"
   },
   "source": [
    "Our vocabulary is based on the Glove vocabulary, and we add terms from the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 314465,
     "status": "ok",
     "timestamp": 1613671640527,
     "user": {
      "displayName": "corentin magyar",
      "photoUrl": "",
      "userId": "04135457892709862388"
     },
     "user_tz": -60
    },
    "id": "8hY8Z7HQFKjo",
    "outputId": "297e4500-f300-4936-bd47-8114c680f668"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] downloading glove 300\n",
      "[==================================================] 100.0% 376.1/376.1MB downloaded\n",
      "[INFO] done !\n"
     ]
    }
   ],
   "source": [
    "tokenizer, char_tokenizer = get_tokenizer(tr_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 314467,
     "status": "ok",
     "timestamp": 1613671640535,
     "user": {
      "displayName": "corentin magyar",
      "photoUrl": "",
      "userId": "04135457892709862388"
     },
     "user_tz": -60
    },
    "id": "ILsFqw2bKf5L",
    "outputId": "2e5d6440-a3c9-4a2b-a468-236872ac7e8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "429064\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1263"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(tokenizer.word_index))\n",
    "len(char_tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wWoZPgxdYbkz"
   },
   "source": [
    "We then update our vocabulary with terms from the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 338369,
     "status": "ok",
     "timestamp": 1613671664440,
     "user": {
      "displayName": "corentin magyar",
      "photoUrl": "",
      "userId": "04135457892709862388"
     },
     "user_tz": -60
    },
    "id": "Xzv_ZS3Zqifn"
   },
   "outputs": [],
   "source": [
    "update_tokenizer(vl_df, tokenizer, char_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 338372,
     "status": "ok",
     "timestamp": 1613671664449,
     "user": {
      "displayName": "corentin magyar",
      "photoUrl": "",
      "userId": "04135457892709862388"
     },
     "user_tz": -60
    },
    "id": "gLwNWJsorIae",
    "outputId": "f2d46ba1-39d9-4014-82b6-72f3e989de7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "429758\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1265"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(tokenizer.word_index))\n",
    "len(char_tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 593162,
     "status": "ok",
     "timestamp": 1613671919241,
     "user": {
      "displayName": "corentin magyar",
      "photoUrl": "",
      "userId": "04135457892709862388"
     },
     "user_tz": -60
    },
    "id": "fM8oYX_SHkni"
   },
   "outputs": [],
   "source": [
    "# take a while\n",
    "tr_df = tr_df.apply(get_start_end, axis = 1)\n",
    "vl_df = vl_df.apply(get_start_end, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ox1J7Z8YhVcv"
   },
   "source": [
    "we get rid of samples where the answer doesn't match the context (maybe there is a typo in the answer or the context).  \n",
    "To avoid to discard many samples, we could lemmatize / stem the text.   \n",
    "Obviously, lemmatization is a better choice for our task, but if we want a really accurate lemmatization processing, we need to do POS tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 593170,
     "status": "ok",
     "timestamp": 1613671919255,
     "user": {
      "displayName": "corentin magyar",
      "photoUrl": "",
      "userId": "04135457892709862388"
     },
     "user_tz": -60
    },
    "id": "8eTsENy6jU-t",
    "outputId": "78f1e933-1aaa-40cb-e8ac-31f3e135556a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(69, 15)"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_df[tr_df['start'] == -1].shape[0], vl_df[vl_df['start'] == -1].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 788
    },
    "executionInfo": {
     "elapsed": 593153,
     "status": "ok",
     "timestamp": 1613671919257,
     "user": {
      "displayName": "corentin magyar",
      "photoUrl": "",
      "userId": "04135457892709862388"
     },
     "user_tz": -60
    },
    "id": "RfAls7CF683U",
    "outputId": "83de30be-66bc-49e3-aec0-45edf99e85c7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer_start</th>\n",
       "      <th>text</th>\n",
       "      <th>question</th>\n",
       "      <th>id</th>\n",
       "      <th>context</th>\n",
       "      <th>title</th>\n",
       "      <th>c_id</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>92</td>\n",
       "      <td>march 14, 2013</td>\n",
       "      <td>on what date was the 2013 human development re...</td>\n",
       "      <td>56de31984396321400ee2672</td>\n",
       "      <td>some countries were not included for various r...</td>\n",
       "      <td>Human_Development_Index</td>\n",
       "      <td>2185</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3133</th>\n",
       "      <td>0</td>\n",
       "      <td>catalan sociolinguistics</td>\n",
       "      <td>what field studies the placement of catalan in...</td>\n",
       "      <td>56e17f5de3433e1400422f8c</td>\n",
       "      <td>in central catalan, unstressed vowels reduce t...</td>\n",
       "      <td>Catalan_language</td>\n",
       "      <td>3470</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3983</th>\n",
       "      <td>168</td>\n",
       "      <td>the valencian academy of language</td>\n",
       "      <td>what is the official regulating body of  valen...</td>\n",
       "      <td>56e1b97fcd28a01900c67ad8</td>\n",
       "      <td>valencian is classified as a western dialect, ...</td>\n",
       "      <td>Catalan_language</td>\n",
       "      <td>3488</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6198</th>\n",
       "      <td>103</td>\n",
       "      <td>catalan</td>\n",
       "      <td>what language is the regulator meant to standa...</td>\n",
       "      <td>56e1b4decd28a01900c67a91</td>\n",
       "      <td>in alghero, the iec has adapted its standard t...</td>\n",
       "      <td>Catalan_language</td>\n",
       "      <td>3486</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6994</th>\n",
       "      <td>94</td>\n",
       "      <td>western catalonia</td>\n",
       "      <td>where are the provinces of lleida and tarragona?</td>\n",
       "      <td>56e1b738cd28a01900c67aae</td>\n",
       "      <td>in 2011, the aragonese government passed a dec...</td>\n",
       "      <td>Catalan_language</td>\n",
       "      <td>3487</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66889</th>\n",
       "      <td>113</td>\n",
       "      <td>some individual caribbean islands in the west ...</td>\n",
       "      <td>what did great britain gain in the west indies...</td>\n",
       "      <td>572e8003c246551400ce425f</td>\n",
       "      <td>many middle and small powers in europe, unlike...</td>\n",
       "      <td>Seven_Years%27_War</td>\n",
       "      <td>15282</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66972</th>\n",
       "      <td>446</td>\n",
       "      <td>reminiscent of the more famous and compact str...</td>\n",
       "      <td>what is the precedent for the \"second hundred ...</td>\n",
       "      <td>572e81f2cb0c0d14000f1206</td>\n",
       "      <td>the war was successful for great britain, whic...</td>\n",
       "      <td>Seven_Years%27_War</td>\n",
       "      <td>15283</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67376</th>\n",
       "      <td>434</td>\n",
       "      <td>sicily, and savoy, although sided with franco-...</td>\n",
       "      <td>who would sicily and savoy normally align with?</td>\n",
       "      <td>572e8578c246551400ce42bd</td>\n",
       "      <td>realizing that war was imminent, prussia preem...</td>\n",
       "      <td>Seven_Years%27_War</td>\n",
       "      <td>15281</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69867</th>\n",
       "      <td>56</td>\n",
       "      <td>uniformity</td>\n",
       "      <td>what do the dialects of catalan feature?</td>\n",
       "      <td>56e180f5e3433e1400422f96</td>\n",
       "      <td>catalan sociolinguistics studies the situation...</td>\n",
       "      <td>Catalan_language</td>\n",
       "      <td>3471</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69923</th>\n",
       "      <td>331</td>\n",
       "      <td>other languages in contact</td>\n",
       "      <td>what outside affects does this study include?</td>\n",
       "      <td>56e17f5de3433e1400422f90</td>\n",
       "      <td>in central catalan, unstressed vowels reduce t...</td>\n",
       "      <td>Catalan_language</td>\n",
       "      <td>3470</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>69 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       answer_start  ... end\n",
       "87               92  ...  -1\n",
       "3133              0  ...  -1\n",
       "3983            168  ...  -1\n",
       "6198            103  ...  -1\n",
       "6994             94  ...  -1\n",
       "...             ...  ...  ..\n",
       "66889           113  ...  -1\n",
       "66972           446  ...  -1\n",
       "67376           434  ...  -1\n",
       "69867            56  ...  -1\n",
       "69923           331  ...  -1\n",
       "\n",
       "[69 rows x 9 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_df[tr_df['start'] == -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 593133,
     "status": "ok",
     "timestamp": 1613671919258,
     "user": {
      "displayName": "corentin magyar",
      "photoUrl": "",
      "userId": "04135457892709862388"
     },
     "user_tz": -60
    },
    "id": "o7uz1Odj7fAS",
    "outputId": "f52c2fee-e36b-4b2e-85c4-88c0602fbeb4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer_start</th>\n",
       "      <th>text</th>\n",
       "      <th>question</th>\n",
       "      <th>id</th>\n",
       "      <th>context</th>\n",
       "      <th>title</th>\n",
       "      <th>c_id</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>166</td>\n",
       "      <td>barcelona province</td>\n",
       "      <td>in what densely populated area is it spoken?</td>\n",
       "      <td>56e18a90e3433e1400422fac</td>\n",
       "      <td>western catalan comprises the two dialects of ...</td>\n",
       "      <td>Catalan_language</td>\n",
       "      <td>3474</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1536</th>\n",
       "      <td>3</td>\n",
       "      <td>the balearic islands</td>\n",
       "      <td>where is iec's standard used?</td>\n",
       "      <td>56e1a3cbe3433e1400423066</td>\n",
       "      <td>standard catalan, virtually accepted by all sp...</td>\n",
       "      <td>Catalan_language</td>\n",
       "      <td>3484</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3007</th>\n",
       "      <td>162</td>\n",
       "      <td>merged</td>\n",
       "      <td>what have a and e done in eastern dialects?</td>\n",
       "      <td>56e18710cd28a01900c679b9</td>\n",
       "      <td>the dialects of the catalan language feature a...</td>\n",
       "      <td>Catalan_language</td>\n",
       "      <td>3472</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4925</th>\n",
       "      <td>114</td>\n",
       "      <td>la franja</td>\n",
       "      <td>where is the catalan speaking part of aragon?</td>\n",
       "      <td>56e1b4decd28a01900c67a8e</td>\n",
       "      <td>in alghero, the iec has adapted its standard t...</td>\n",
       "      <td>Catalan_language</td>\n",
       "      <td>3486</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5782</th>\n",
       "      <td>69</td>\n",
       "      <td>seven</td>\n",
       "      <td>how many stressed phonemes are there in catalan?</td>\n",
       "      <td>56e18bfbe3433e1400422fb5</td>\n",
       "      <td>central catalan is considered the standard pro...</td>\n",
       "      <td>Catalan_language</td>\n",
       "      <td>3468</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5897</th>\n",
       "      <td>118</td>\n",
       "      <td>treatment of unstressed a and e</td>\n",
       "      <td>what is the major difference between the two b...</td>\n",
       "      <td>56e18710cd28a01900c679b7</td>\n",
       "      <td>the dialects of the catalan language feature a...</td>\n",
       "      <td>Catalan_language</td>\n",
       "      <td>3472</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5937</th>\n",
       "      <td>50</td>\n",
       "      <td>vulgar latin</td>\n",
       "      <td>what is the vowel system of catalan?</td>\n",
       "      <td>56e18bfbe3433e1400422fb4</td>\n",
       "      <td>western catalan comprises the two dialects of ...</td>\n",
       "      <td>Catalan_language</td>\n",
       "      <td>3468</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6561</th>\n",
       "      <td>3</td>\n",
       "      <td>alghero</td>\n",
       "      <td>where has the iec adapted its standard to the ...</td>\n",
       "      <td>56e1b264e3433e14004230a6</td>\n",
       "      <td>the most notable difference between both stand...</td>\n",
       "      <td>Catalan_language</td>\n",
       "      <td>3485</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7378</th>\n",
       "      <td>246</td>\n",
       "      <td>to later conflicts like the napoleonic wars</td>\n",
       "      <td>what was a later conflict that some considered...</td>\n",
       "      <td>572e81f2cb0c0d14000f1207</td>\n",
       "      <td>the war was successful for great britain, whic...</td>\n",
       "      <td>Seven_Years%27_War</td>\n",
       "      <td>15283</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11252</th>\n",
       "      <td>176</td>\n",
       "      <td>section pronunciation</td>\n",
       "      <td>where do you find dialectic vowel reductions?</td>\n",
       "      <td>56e17b08cd28a01900c679af</td>\n",
       "      <td>catalan has inherited the typical vowel system...</td>\n",
       "      <td>Catalan_language</td>\n",
       "      <td>3469</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11511</th>\n",
       "      <td>72</td>\n",
       "      <td>the average level</td>\n",
       "      <td>does the ihdi measure the \"average\" or the \"po...</td>\n",
       "      <td>56de31f34396321400ee2680</td>\n",
       "      <td>the 2013 human development report by the unite...</td>\n",
       "      <td>Human_Development_Index</td>\n",
       "      <td>2182</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12438</th>\n",
       "      <td>110</td>\n",
       "      <td>four</td>\n",
       "      <td>how many dialects are in the eastern group?</td>\n",
       "      <td>56e188e4cd28a01900c679c0</td>\n",
       "      <td>the dialects of the catalan language feature a...</td>\n",
       "      <td>Catalan_language</td>\n",
       "      <td>3473</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16218</th>\n",
       "      <td>96</td>\n",
       "      <td>other romance languages</td>\n",
       "      <td>in comparison to what are the dialects uniform?</td>\n",
       "      <td>56e180f5e3433e1400422f97</td>\n",
       "      <td>catalan sociolinguistics studies the situation...</td>\n",
       "      <td>Catalan_language</td>\n",
       "      <td>3471</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16474</th>\n",
       "      <td>190</td>\n",
       "      <td>catalan and valencian</td>\n",
       "      <td>what forms are mutually intelligible?</td>\n",
       "      <td>56e1b738cd28a01900c67aaf</td>\n",
       "      <td>in 2011, the aragonese government passed a dec...</td>\n",
       "      <td>Catalan_language</td>\n",
       "      <td>3487</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17282</th>\n",
       "      <td>131</td>\n",
       "      <td>western romance</td>\n",
       "      <td>where is this system common?</td>\n",
       "      <td>56e18bfbe3433e1400422fb6</td>\n",
       "      <td>central catalan is considered the standard pro...</td>\n",
       "      <td>Catalan_language</td>\n",
       "      <td>3468</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       answer_start                                         text  ... start end\n",
       "171             166                           barcelona province  ...    -1  -1\n",
       "1536              3                         the balearic islands  ...    -1  -1\n",
       "3007            162                                       merged  ...    -1  -1\n",
       "4925            114                                    la franja  ...    -1  -1\n",
       "5782             69                                        seven  ...    -1  -1\n",
       "5897            118              treatment of unstressed a and e  ...    -1  -1\n",
       "5937             50                                 vulgar latin  ...    -1  -1\n",
       "6561              3                                      alghero  ...    -1  -1\n",
       "7378            246  to later conflicts like the napoleonic wars  ...    -1  -1\n",
       "11252           176                        section pronunciation  ...    -1  -1\n",
       "11511            72                            the average level  ...    -1  -1\n",
       "12438           110                                         four  ...    -1  -1\n",
       "16218            96                      other romance languages  ...    -1  -1\n",
       "16474           190                        catalan and valencian  ...    -1  -1\n",
       "17282           131                              western romance  ...    -1  -1\n",
       "\n",
       "[15 rows x 9 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vl_df[vl_df['start'] == -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 593133,
     "status": "ok",
     "timestamp": 1613671919260,
     "user": {
      "displayName": "corentin magyar",
      "photoUrl": "",
      "userId": "04135457892709862388"
     },
     "user_tz": -60
    },
    "id": "pZ3CXWdL8CNk"
   },
   "outputs": [],
   "source": [
    "# we get rid of samples where the answer doesn't match the context\n",
    "tr_df = tr_df[tr_df['start'] != -1]\n",
    "vl_df = vl_df[vl_df['start'] != -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 773601,
     "status": "ok",
     "timestamp": 1613672099730,
     "user": {
      "displayName": "corentin magyar",
      "photoUrl": "",
      "userId": "04135457892709862388"
     },
     "user_tz": -60
    },
    "id": "M_KJeyPxVZG6"
   },
   "outputs": [],
   "source": [
    "tr_df = tokenize(tr_df, tokenizer, char_tokenizer)\n",
    "vl_df = tokenize(vl_df, tokenizer, char_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 538
    },
    "executionInfo": {
     "elapsed": 773590,
     "status": "ok",
     "timestamp": 1613672099740,
     "user": {
      "displayName": "corentin magyar",
      "photoUrl": "",
      "userId": "04135457892709862388"
     },
     "user_tz": -60
    },
    "id": "VJd7p05zjg0e",
    "outputId": "01358dfa-e67a-40fb-dd96-57af7067ccfe"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer_start</th>\n",
       "      <th>text</th>\n",
       "      <th>question</th>\n",
       "      <th>id</th>\n",
       "      <th>context</th>\n",
       "      <th>title</th>\n",
       "      <th>c_id</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>tokenized_question</th>\n",
       "      <th>tokenized_context</th>\n",
       "      <th>char_tokenized_question</th>\n",
       "      <th>char_tokenized_context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>154</td>\n",
       "      <td>phillies</td>\n",
       "      <td>what team had dallas green managed in 1980?</td>\n",
       "      <td>572667e6708984140094c4f9</td>\n",
       "      <td>after over a dozen more subpar seasons, in 198...</td>\n",
       "      <td>Chicago_Cubs</td>\n",
       "      <td>8880</td>\n",
       "      <td>29</td>\n",
       "      <td>30</td>\n",
       "      <td>[11, 309, 49, 11808, 646, 2132, 6, 2627, 9]</td>\n",
       "      <td>[61, 83, 10, 6737, 62, 70020, 1740, 3, 6, 3372...</td>\n",
       "      <td>[[20, 11, 5, 4], [4, 3, 5, 16], [11, 5, 13], [...</td>\n",
       "      <td>[[5, 17, 4, 3, 10], [8, 24, 3, 10], [5], [13, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>156</td>\n",
       "      <td>rudy giuliani</td>\n",
       "      <td>which candidate withdrew from the presidential...</td>\n",
       "      <td>56dec2483277331400b4d712</td>\n",
       "      <td>schwarzenegger's endorsement in the republican...</td>\n",
       "      <td>Arnold_Schwarzenegger</td>\n",
       "      <td>2311</td>\n",
       "      <td>23</td>\n",
       "      <td>25</td>\n",
       "      <td>[27, 2789, 4161, 23, 2, 1534, 698, 6, 417, 4, ...</td>\n",
       "      <td>[1084, 19, 9106, 6, 2, 1467, 477, 4, 2, 420, 1...</td>\n",
       "      <td>[[20, 11, 6, 14, 11], [14, 5, 7, 13, 6, 13, 5,...</td>\n",
       "      <td>[[9, 14, 11, 20, 5, 10, 39, 3, 7, 3, 19, 19, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>224</td>\n",
       "      <td>wild ones outside the area</td>\n",
       "      <td>captive animals can distinguish co-inhabitats ...</td>\n",
       "      <td>5726e5995951b619008f81bb</td>\n",
       "      <td>it has been observed that well-fed predator an...</td>\n",
       "      <td>Predation</td>\n",
       "      <td>9822</td>\n",
       "      <td>38</td>\n",
       "      <td>43</td>\n",
       "      <td>[11888, 727, 65, 3733, 419169, 23, 11, 48, 136...</td>\n",
       "      <td>[30, 40, 59, 2316, 20, 63225, 4421, 727, 6, 10...</td>\n",
       "      <td>[[14, 5, 18, 4, 6, 24, 3], [5, 7, 6, 16, 5, 12...</td>\n",
       "      <td>[[6, 4], [11, 5, 9], [22, 3, 3, 7], [8, 22, 9,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>919</td>\n",
       "      <td>the battle of trafalgar</td>\n",
       "      <td>the results of which battle allowed the britis...</td>\n",
       "      <td>5726486f708984140094c157</td>\n",
       "      <td>after returning from egypt, napoleon engineere...</td>\n",
       "      <td>Napoleon</td>\n",
       "      <td>8418</td>\n",
       "      <td>158</td>\n",
       "      <td>162</td>\n",
       "      <td>[2, 1324, 4, 27, 326, 495, 2, 132, 8, 6280, 15...</td>\n",
       "      <td>[61, 3986, 23, 598, 3, 545, 9789, 10, 2313, 6,...</td>\n",
       "      <td>[[4, 11, 3], [10, 3, 9, 15, 12, 4, 9], [8, 17]...</td>\n",
       "      <td>[[5, 17, 4, 3, 10], [10, 3, 4, 15, 10, 7, 6, 7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>382</td>\n",
       "      <td>hanged</td>\n",
       "      <td>how was vesey executed in 1822?</td>\n",
       "      <td>5730299db2c2fd14005689a7</td>\n",
       "      <td>by 1820, charleston's population had grown to ...</td>\n",
       "      <td>Charleston,_South_Carolina</td>\n",
       "      <td>15719</td>\n",
       "      <td>74</td>\n",
       "      <td>75</td>\n",
       "      <td>[44, 13, 25121, 2181, 6, 10202, 9]</td>\n",
       "      <td>[18, 9015, 3, 1909, 19, 104, 49, 2555, 8, 2106...</td>\n",
       "      <td>[[11, 8, 20], [20, 5, 9], [24, 3, 9, 3, 21], [...</td>\n",
       "      <td>[[22, 21], [28, 40, 31, 29], [23], [14, 11, 5,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   answer_start  ...                             char_tokenized_context\n",
       "0           154  ...  [[5, 17, 4, 3, 10], [8, 24, 3, 10], [5], [13, ...\n",
       "1           156  ...  [[9, 14, 11, 20, 5, 10, 39, 3, 7, 3, 19, 19, 3...\n",
       "2           224  ...  [[6, 4], [11, 5, 9], [22, 3, 3, 7], [8, 22, 9,...\n",
       "3           919  ...  [[5, 17, 4, 3, 10], [10, 3, 4, 15, 10, 7, 6, 7...\n",
       "4           382  ...  [[22, 21], [28, 40, 31, 29], [23], [14, 11, 5,...\n",
       "\n",
       "[5 rows x 13 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HRlxHm3xwnFn"
   },
   "source": [
    "We display some useful stats in order to define the padding size (at the word and character level, for both question and context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 773904,
     "status": "ok",
     "timestamp": 1613672100062,
     "user": {
      "displayName": "corentin magyar",
      "photoUrl": "",
      "userId": "04135457892709862388"
     },
     "user_tz": -60
    },
    "id": "2kw5Pk4c6I3S",
    "outputId": "a23e40e6-e3ad-40bb-c85f-5a148682731f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    70010.000000\n",
      "mean        11.275532\n",
      "std          3.715821\n",
      "min          1.000000\n",
      "25%          9.000000\n",
      "50%         11.000000\n",
      "75%         13.000000\n",
      "max         60.000000\n",
      "Name: tokenized_question, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    17505.000000\n",
       "mean        11.335504\n",
       "std          3.754207\n",
       "min          1.000000\n",
       "25%          9.000000\n",
       "50%         11.000000\n",
       "75%         13.000000\n",
       "max         38.000000\n",
       "Name: tokenized_question, dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tr_df['tokenized_question'].str.len().describe())\n",
    "vl_df['tokenized_question'].str.len().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 773900,
     "status": "ok",
     "timestamp": 1613672100063,
     "user": {
      "displayName": "corentin magyar",
      "photoUrl": "",
      "userId": "04135457892709862388"
     },
     "user_tz": -60
    },
    "id": "Dk5TG_Yy9crW",
    "outputId": "efcd4cb1-91bf-45a6-b138-393818e5bf69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "23.0"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tr_df['tokenized_question'].str.len().quantile(0.99))\n",
    "vl_df['tokenized_question'].str.len().quantile(0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 773893,
     "status": "ok",
     "timestamp": 1613672100064,
     "user": {
      "displayName": "corentin magyar",
      "photoUrl": "",
      "userId": "04135457892709862388"
     },
     "user_tz": -60
    },
    "id": "bsKYKsgR6-UK",
    "outputId": "dbd47356-269a-441f-9036-6bf86b8b6c39"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    70010.000000\n",
      "mean       137.824439\n",
      "std         56.941382\n",
      "min         22.000000\n",
      "25%        102.000000\n",
      "50%        127.000000\n",
      "75%        164.000000\n",
      "max        766.000000\n",
      "Name: tokenized_context, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    17505.000000\n",
       "mean       137.211083\n",
       "std         55.912622\n",
       "min         22.000000\n",
       "25%        102.000000\n",
       "50%        126.000000\n",
       "75%        163.000000\n",
       "max        766.000000\n",
       "Name: tokenized_context, dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tr_df['tokenized_context'].str.len().describe())\n",
    "vl_df['tokenized_context'].str.len().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 773887,
     "status": "ok",
     "timestamp": 1613672100065,
     "user": {
      "displayName": "corentin magyar",
      "photoUrl": "",
      "userId": "04135457892709862388"
     },
     "user_tz": -60
    },
    "id": "Cyp5MUtj9woq",
    "outputId": "eef2a8e5-2e4d-4e84-bb69-2783a8a3d48f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "324.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "324.0"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tr_df['tokenized_context'].str.len().quantile(0.99))\n",
    "vl_df['tokenized_context'].str.len().quantile(0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "executionInfo": {
     "elapsed": 789764,
     "status": "ok",
     "timestamp": 1613672115943,
     "user": {
      "displayName": "corentin magyar",
      "photoUrl": "",
      "userId": "04135457892709862388"
     },
     "user_tz": -60
    },
    "id": "fE3J6xJZ7Sqe"
   },
   "outputs": [],
   "source": [
    "def len_words(dataset):\n",
    "\n",
    "  \"\"\"\n",
    "  return the word's length\n",
    "  \"\"\"\n",
    "\n",
    "  count_q = []\n",
    "  count_c = []\n",
    "\n",
    "  for idx, row in dataset.iterrows():\n",
    "    for w in row['char_tokenized_question']:\n",
    "      l = len(w)\n",
    "      count_q.append(l)\n",
    "      \n",
    "    for w in row['char_tokenized_context']:\n",
    "      m = len(w)\n",
    "      count_c.append(m)\n",
    "  \n",
    "  return pd.Series(count_q), pd.Series(count_c)\n",
    "\n",
    "t_q,t_c = len_words(tr_df)\n",
    "v_q,v_c = len_words(vl_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 790092,
     "status": "ok",
     "timestamp": 1613672116279,
     "user": {
      "displayName": "corentin magyar",
      "photoUrl": "",
      "userId": "04135457892709862388"
     },
     "user_tz": -60
    },
    "id": "i8R_-bUv9HGQ",
    "outputId": "c3e22a74-5074-4397-c1a0-60c4d308b5e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    789400.000000\n",
      "mean          4.447926\n",
      "std           2.677579\n",
      "min           1.000000\n",
      "25%           2.000000\n",
      "50%           4.000000\n",
      "75%           6.000000\n",
      "max          30.000000\n",
      "dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    9.649089e+06\n",
       "mean     4.626070e+00\n",
       "std      2.969605e+00\n",
       "min      1.000000e+00\n",
       "25%      2.000000e+00\n",
       "50%      4.000000e+00\n",
       "75%      7.000000e+00\n",
       "max      3.700000e+01\n",
       "dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(t_q.describe())\n",
    "t_c.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 790088,
     "status": "ok",
     "timestamp": 1613672116283,
     "user": {
      "displayName": "corentin magyar",
      "photoUrl": "",
      "userId": "04135457892709862388"
     },
     "user_tz": -60
    },
    "id": "R64fJS7DxkuN",
    "outputId": "7befc878-6d60-4f4b-c430-8e18886553bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    198428.000000\n",
      "mean          4.453232\n",
      "std           2.686696\n",
      "min           1.000000\n",
      "25%           2.000000\n",
      "50%           4.000000\n",
      "75%           6.000000\n",
      "max          24.000000\n",
      "dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    2.401880e+06\n",
       "mean     4.629710e+00\n",
       "std      2.972670e+00\n",
       "min      1.000000e+00\n",
       "25%      2.000000e+00\n",
       "50%      4.000000e+00\n",
       "75%      7.000000e+00\n",
       "max      3.700000e+01\n",
       "dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(v_q.describe())\n",
    "v_c.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 790474,
     "status": "ok",
     "timestamp": 1613672116674,
     "user": {
      "displayName": "corentin magyar",
      "photoUrl": "",
      "userId": "04135457892709862388"
     },
     "user_tz": -60
    },
    "id": "5wxf3tS7xylx",
    "outputId": "68377538-335d-4806-c282-809973798bdc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "13.0"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(t_q.quantile(0.99))\n",
    "t_c.quantile(0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 790474,
     "status": "ok",
     "timestamp": 1613672116682,
     "user": {
      "displayName": "corentin magyar",
      "photoUrl": "",
      "userId": "04135457892709862388"
     },
     "user_tz": -60
    },
    "id": "x6F-O0qjx4Gu",
    "outputId": "dc942fdd-1a72-499d-bbcf-8f3aa1fa6812"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "13.0"
      ]
     },
     "execution_count": 29,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(v_q.quantile(0.99))\n",
    "v_c.quantile(0.99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LC1K9nYs9ht2"
   },
   "source": [
    "There are obviously some outliers. We are compeled to get rid of some samples because of memory issues.\n",
    "\n",
    "We will get rid of contexts that have more than 400 words and questions that have more than 25 words.\n",
    "\n",
    "We will set the length of a word to 15 characters\n",
    "\n",
    "**EDIT :** These numbers are huge but we won't get out of memory errors if we build a sequence generator. If you don't want to use the sequence generator, you should reduce these numbers.\n",
    "\n",
    "**EDIT :** Now that we use a sequence generator, we could define `*_MAXLEN` variables according to the stats provided by the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "executionInfo": {
     "elapsed": 790473,
     "status": "ok",
     "timestamp": 1613672116683,
     "user": {
      "displayName": "corentin magyar",
      "photoUrl": "",
      "userId": "04135457892709862388"
     },
     "user_tz": -60
    },
    "id": "3WiAI_oA_3lY"
   },
   "outputs": [],
   "source": [
    "QUESTION_MAXLEN = 25\n",
    "CONTEXT_MAXLEN = 400\n",
    "WORD_MAXLEN = 15\n",
    "BATCH_SIZE = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 790469,
     "status": "ok",
     "timestamp": 1613672116684,
     "user": {
      "displayName": "corentin magyar",
      "photoUrl": "",
      "userId": "04135457892709862388"
     },
     "user_tz": -60
    },
    "id": "5fZMoWqTL2Wz",
    "outputId": "1c8efeed-e93f-4b38-b064-4bb45e141303"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((70010, 13), (17505, 13))"
      ]
     },
     "execution_count": 31,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_df.shape, vl_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "executionInfo": {
     "elapsed": 790468,
     "status": "ok",
     "timestamp": 1613672116685,
     "user": {
      "displayName": "corentin magyar",
      "photoUrl": "",
      "userId": "04135457892709862388"
     },
     "user_tz": -60
    },
    "id": "ibILzhoh-Mf8"
   },
   "outputs": [],
   "source": [
    "tr_df = tr_df[(tr_df['tokenized_question'].str.len() <= QUESTION_MAXLEN) & (tr_df['tokenized_context'].str.len() <= CONTEXT_MAXLEN) & (tr_df['start'] <= CONTEXT_MAXLEN) & (tr_df['end'] <= CONTEXT_MAXLEN) ].reset_index(drop = True)\n",
    "vl_df = vl_df[(vl_df['tokenized_question'].str.len() <= QUESTION_MAXLEN) & (vl_df['tokenized_context'].str.len() <= CONTEXT_MAXLEN) & (vl_df['start'] <= CONTEXT_MAXLEN) & (vl_df['end'] <= CONTEXT_MAXLEN) ].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 790461,
     "status": "ok",
     "timestamp": 1613672116686,
     "user": {
      "displayName": "corentin magyar",
      "photoUrl": "",
      "userId": "04135457892709862388"
     },
     "user_tz": -60
    },
    "id": "qQTuvAo7oGcA",
    "outputId": "14e2a449-352d-4a08-93ea-547035fe8961"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(69606, 17413)"
      ]
     },
     "execution_count": 33,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_df.shape[0], vl_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 790458,
     "status": "ok",
     "timestamp": 1613672116688,
     "user": {
      "displayName": "corentin magyar",
      "photoUrl": "",
      "userId": "04135457892709862388"
     },
     "user_tz": -60
    },
    "id": "93mWQxFLG3z9",
    "outputId": "6d3b8641-0c65-49b8-bf82-1aa28b8b3ac1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " we get rid of : 580 samples\n"
     ]
    }
   ],
   "source": [
    " print(f' we get rid of : {SAMPLES - (tr_df.shape[0] + vl_df.shape[0])} samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 813650,
     "status": "ok",
     "timestamp": 1613672139887,
     "user": {
      "displayName": "corentin magyar",
      "photoUrl": "",
      "userId": "04135457892709862388"
     },
     "user_tz": -60
    },
    "id": "4lFv9wpDfDOj",
    "outputId": "ed0b4139-02d7-4f7f-f2b9-9ac2c9654289"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset saved in /content/drive/MyDrive/NLP/BIDAF/utils/data/train_set.json\n",
      "dataset saved in /content/drive/MyDrive/NLP/BIDAF/utils/data/valid_set.json\n"
     ]
    }
   ],
   "source": [
    "# save datasets in json format\n",
    "path_to_train_set = os.path.join(os.getcwd(), 'drive/MyDrive/NLP/BIDAF/utils/data/train_set.json')\n",
    "df_to_json(tr_df, path_to_train_set)\n",
    "\n",
    "path_to_valid_set = os.path.join(os.getcwd(), 'drive/MyDrive/NLP/BIDAF/utils/data/valid_set.json')\n",
    "df_to_json(vl_df, path_to_valid_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "executionInfo": {
     "elapsed": 814903,
     "status": "ok",
     "timestamp": 1613672141141,
     "user": {
      "displayName": "corentin magyar",
      "photoUrl": "",
      "userId": "04135457892709862388"
     },
     "user_tz": -60
    },
    "id": "Owvocjs5JCPa"
   },
   "outputs": [],
   "source": [
    "# we save both tokenizers\n",
    "tokenizers_folder = os.path.join(os.getcwd(), 'drive/MyDrive/NLP/BIDAF/utils', 'tokenizers')\n",
    "if not os.path.exists(tokenizers_folder):\n",
    "  os.makedirs(tokenizers_folder)\n",
    "\n",
    "path_word_tokenizer = os.path.join(tokenizers_folder, 'word_tokenizer.pkl')\n",
    "with open(path_word_tokenizer, 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol = pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "path_char_tokenizer = os.path.join(tokenizers_folder, 'char_tokenizer.pkl')\n",
    "with open(path_char_tokenizer, 'wb') as handle:\n",
    "    pickle.dump(char_tokenizer, handle, protocol = pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XX5aJeXaqnZh"
   },
   "source": [
    "We create the iterator. The iterator allows us to work with much bigger data, because it is loaded into memory only when we need them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "executionInfo": {
     "elapsed": 814904,
     "status": "ok",
     "timestamp": 1613672141144,
     "user": {
      "displayName": "corentin magyar",
      "photoUrl": "",
      "userId": "04135457892709862388"
     },
     "user_tz": -60
    },
    "id": "TMEr6c_hiv6Q"
   },
   "outputs": [],
   "source": [
    "# utils/datasets/dataset.py\n",
    "class SQUAD_dataset(tf.keras.utils.Sequence):\n",
    "\n",
    "  \"\"\"\n",
    "  utility class to create a working dataset that\n",
    "  can be given to a neural network\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, data, question_maxlen, context_maxlen, word_maxlen, batch_size, with_answer = True):\n",
    "    self.QUESTION_MAXLEN = question_maxlen\n",
    "    self.CONTEXT_MAXLEN = context_maxlen\n",
    "    self.WORD_MAXLEN = word_maxlen\n",
    "    self.batch_size = batch_size\n",
    "    self.with_answer = with_answer\n",
    "    self.__get_batches(data)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.batches)\n",
    "\n",
    "  def __get_batches(self, data):\n",
    "    batches = [data[i:i+self.batch_size] for i in range(0, len(data), self.batch_size)]\n",
    "    self.batches = batches\n",
    "\n",
    "  def __repr__(self):\n",
    "    template = '''SQUAD_dataset : questions : ({0}, {1}), contexts : ({0}, {2}), char_questions : ({0}, {1}, {3}), char_contexts : ({0}, {2}, {3}), id : ({0}, 1)'''.format(self.batch_size, self.QUESTION_MAXLEN, self.CONTEXT_MAXLEN, self.WORD_MAXLEN)\n",
    "    return template\n",
    "\n",
    "  @classmethod\n",
    "  def from_file(cls, path):\n",
    "    path = os.path.join(os.getcwd(), path)\n",
    "    with open(path, 'rb') as handle:\n",
    "      dataset = pickle.load(handle)\n",
    "    return dataset\n",
    "\n",
    "  def to_pickle(self, path):\n",
    "    path = os.path.join(os.getcwd(), path)\n",
    "    folder = os.path.dirname(path)\n",
    "\n",
    "    if not os.path.exists(folder):\n",
    "      os.makedirs(folder)\n",
    "\n",
    "    with open(path, 'wb') as handle:\n",
    "      pickle.dump(self, handle, protocol = pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    batch = self.batches[idx].reset_index(drop = True)\n",
    "\n",
    "    id = np.asarray(batch['id'])\n",
    "\n",
    "    # questions and contexts words padding\n",
    "    q_w = tf.keras.preprocessing.sequence.pad_sequences(batch['tokenized_question'], padding = 'post', maxlen = self.QUESTION_MAXLEN)\n",
    "    c_w = tf.keras.preprocessing.sequence.pad_sequences(batch['tokenized_context'], padding = 'post', maxlen = self.CONTEXT_MAXLEN)\n",
    "\n",
    "    # question_char padding\n",
    "    q_c = np.zeros((q_w.shape[0], self.QUESTION_MAXLEN, self.WORD_MAXLEN), dtype = np.int32)\n",
    "\n",
    "    for i, value in batch['char_tokenized_question'].iteritems():\n",
    "      v = tf.keras.preprocessing.sequence.pad_sequences(value, padding = 'post', maxlen = self.WORD_MAXLEN, truncating = 'post')\n",
    "      to_add = self.QUESTION_MAXLEN - v.shape[0]\n",
    "      add = np.zeros((to_add, self.WORD_MAXLEN))\n",
    "      arr = np.vstack([v,add])\n",
    "      q_c[i] = arr\n",
    "\n",
    "    # context_char padding\n",
    "    c_c = np.zeros((q_w.shape[0], self.CONTEXT_MAXLEN, self.WORD_MAXLEN), dtype = np.int32)\n",
    "\n",
    "    for i, value in batch['char_tokenized_context'].iteritems():\n",
    "      v = tf.keras.preprocessing.sequence.pad_sequences(value, padding = 'post', maxlen = self.WORD_MAXLEN, truncating = 'post')\n",
    "      to_add = self.CONTEXT_MAXLEN - v.shape[0]\n",
    "      add = np.zeros((to_add, self.WORD_MAXLEN))\n",
    "      arr = np.vstack([v,add])\n",
    "      c_c[i] = arr\n",
    "\n",
    "    # one hot encode start and end\n",
    "    if self.with_answer:\n",
    "      y_start = tf.keras.utils.to_categorical(batch['start'].values, self.CONTEXT_MAXLEN)\n",
    "      y_end = tf.keras.utils.to_categorical(batch['end'].values, self.CONTEXT_MAXLEN)\n",
    "\n",
    "      # (inputs), (outputs), (id)\n",
    "      return (q_w, c_w, q_c, c_c), (y_start, y_end), (id,)\n",
    "    return (q_c, c_w, q_c, c_c), (id,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "executionInfo": {
     "elapsed": 815637,
     "status": "ok",
     "timestamp": 1613672141879,
     "user": {
      "displayName": "corentin magyar",
      "photoUrl": "",
      "userId": "04135457892709862388"
     },
     "user_tz": -60
    },
    "id": "rGTS5gSunCRa"
   },
   "outputs": [],
   "source": [
    "tr_data = SQUAD_dataset(tr_df, batch_size = BATCH_SIZE, question_maxlen = QUESTION_MAXLEN, context_maxlen = CONTEXT_MAXLEN, word_maxlen = WORD_MAXLEN)\n",
    "vl_data = SQUAD_dataset(vl_df, batch_size = BATCH_SIZE, question_maxlen = QUESTION_MAXLEN, context_maxlen = CONTEXT_MAXLEN, word_maxlen = WORD_MAXLEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 815634,
     "status": "ok",
     "timestamp": 1613672141882,
     "user": {
      "displayName": "corentin magyar",
      "photoUrl": "",
      "userId": "04135457892709862388"
     },
     "user_tz": -60
    },
    "id": "9VciOKiQtyAx",
    "outputId": "78e41535-bfcf-4f37-c55c-008fcbcb76e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6961\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1742"
      ]
     },
     "execution_count": 39,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of batches\n",
    "print(len(tr_data))\n",
    "len(vl_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 815630,
     "status": "ok",
     "timestamp": 1613672141883,
     "user": {
      "displayName": "corentin magyar",
      "photoUrl": "",
      "userId": "04135457892709862388"
     },
     "user_tz": -60
    },
    "id": "1GtQIVC9zZ5g",
    "outputId": "11a991ff-250f-476c-e144-9de367c12482"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SQUAD_dataset : questions : (10, 25), contexts : (10, 400), char_questions : (10, 25, 15), char_contexts : (10, 400, 15), id : (10, 1)"
      ]
     },
     "execution_count": 40,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "executionInfo": {
     "elapsed": 829990,
     "status": "ok",
     "timestamp": 1613672156245,
     "user": {
      "displayName": "corentin magyar",
      "photoUrl": "",
      "userId": "04135457892709862388"
     },
     "user_tz": -60
    },
    "id": "etbiAh9h1-Fg"
   },
   "outputs": [],
   "source": [
    "tr_data.to_pickle('drive/MyDrive/NLP/BIDAF/utils/datasets/train_dataset.pkl')\n",
    "vl_data.to_pickle('drive/MyDrive/NLP/BIDAF/utils/datasets/valid_dataset.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QSo5-GcJQe18"
   },
   "source": [
    "Now that the preprocessing is over we can preprocess a (mock) unseen dataset. It is basically the same that the one we have seen just before, but it does not contain the start and end span (text and answer_start fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23355,
     "status": "ok",
     "timestamp": 1613672181762,
     "user": {
      "displayName": "corentin magyar",
      "photoUrl": "",
      "userId": "04135457892709862388"
     },
     "user_tz": -60
    },
    "id": "W8eli84fbubp",
    "outputId": "01f8d755-5f6a-4a8f-8641-19caf072ef2c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the json file\n",
      "[INFO] processing...\n",
      "[INFO] there are 87599 questions with single answer\n",
      "[INFO] there are 18891 different contexts\n",
      "[INFO] there are 442 unrelated subjects\n",
      "[INFO] Done\n"
     ]
    }
   ],
   "source": [
    "# with_answer = False to parse a dataset with no answer\n",
    "unseen_dataset = load_dataset(SQUAD_PATH, with_answer = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "executionInfo": {
     "elapsed": 778,
     "status": "ok",
     "timestamp": 1613672189123,
     "user": {
      "displayName": "corentin magyar",
      "photoUrl": "",
      "userId": "04135457892709862388"
     },
     "user_tz": -60
    },
    "id": "au8aZNFfde5I"
   },
   "outputs": [],
   "source": [
    "s = unseen_dataset.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 169795,
     "status": "ok",
     "timestamp": 1613672359856,
     "user": {
      "displayName": "corentin magyar",
      "photoUrl": "",
      "userId": "04135457892709862388"
     },
     "user_tz": -60
    },
    "id": "McWg6RJy-NOl",
    "outputId": "428c45c3-e90e-4d07-e499-8c05d68961c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " we get rid of : 496 samples\n"
     ]
    }
   ],
   "source": [
    "unseen_dataset = clean_dataset(unseen_dataset, with_answer = False)\n",
    "unseen_dataset = tokenize(unseen_dataset, tokenizer, char_tokenizer)\n",
    "unseen_dataset = unseen_dataset[(unseen_dataset['tokenized_question'].str.len() <= QUESTION_MAXLEN) & (unseen_dataset['tokenized_context'].str.len() <= CONTEXT_MAXLEN)].reset_index(drop = True)\n",
    "print(f' we get rid of : {s - (unseen_dataset.shape[0])} samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16858,
     "status": "ok",
     "timestamp": 1613672397056,
     "user": {
      "displayName": "corentin magyar",
      "photoUrl": "",
      "userId": "04135457892709862388"
     },
     "user_tz": -60
    },
    "id": "2oPFVcMAAIb-",
    "outputId": "e8ddb325-d2ff-4c16-8aeb-82b3bc57a435"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset saved in /content/drive/MyDrive/NLP/BIDAF/utils/data/unseen_set.json\n"
     ]
    }
   ],
   "source": [
    "unseen_path = os.path.join(os.getcwd(), 'drive/MyDrive/NLP/BIDAF/utils/data/unseen_set.json')\n",
    "df_to_json(unseen_dataset, unseen_path, with_answer = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "executionInfo": {
     "elapsed": 888,
     "status": "ok",
     "timestamp": 1613672408730,
     "user": {
      "displayName": "corentin magyar",
      "photoUrl": "",
      "userId": "04135457892709862388"
     },
     "user_tz": -60
    },
    "id": "YYsbL8bs_ZFf"
   },
   "outputs": [],
   "source": [
    "unseen_data = SQUAD_dataset(unseen_dataset, batch_size = BATCH_SIZE, question_maxlen = QUESTION_MAXLEN, context_maxlen = CONTEXT_MAXLEN, word_maxlen = WORD_MAXLEN, with_answer = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 610,
     "status": "ok",
     "timestamp": 1613672410982,
     "user": {
      "displayName": "corentin magyar",
      "photoUrl": "",
      "userId": "04135457892709862388"
     },
     "user_tz": -60
    },
    "id": "koM-e00x_wsM",
    "outputId": "840f4327-399a-4f3a-9553-884f20d6dab6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SQUAD_dataset : questions : (10, 25), contexts : (10, 400), char_questions : (10, 25, 15), char_contexts : (10, 400, 15), id : (10, 1)"
      ]
     },
     "execution_count": 47,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unseen_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "executionInfo": {
     "elapsed": 10503,
     "status": "ok",
     "timestamp": 1613672425191,
     "user": {
      "displayName": "corentin magyar",
      "photoUrl": "",
      "userId": "04135457892709862388"
     },
     "user_tz": -60
    },
    "id": "DOWz8V-2_yze"
   },
   "outputs": [],
   "source": [
    "unseen_data.to_pickle('drive/MyDrive/NLP/BIDAF/utils/datasets/unseen_dataset.pkl')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNsHQpQwqVWqbY6932LTBPj",
   "collapsed_sections": [],
   "mount_file_id": "1q6JJXsQvZONqiS4NEslvgYcg-tXfMUcf",
   "name": "bidaf_preprocessing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
