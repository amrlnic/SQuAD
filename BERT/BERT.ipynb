{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "21b02101abb1424680d6418e7967496b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_36ae7fe6557b4daa8b07d97499f0daf1",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_df2c0a5dbba743a18d89237e10c6c988",
              "IPY_MODEL_84732c8455a74d3aaa236130b7fbe4b8"
            ]
          }
        },
        "36ae7fe6557b4daa8b07d97499f0daf1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "df2c0a5dbba743a18d89237e10c6c988": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_2bc1f1f34b3645859dbe2d213103e116",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_17489a2a45564c14b714cef48a4a6d51"
          }
        },
        "84732c8455a74d3aaa236130b7fbe4b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ada9e6097de24041a4613cdcbefc9568",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:00&lt;00:00, 1.30MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_95c5615d068e45d5b916aa778cf12943"
          }
        },
        "2bc1f1f34b3645859dbe2d213103e116": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "17489a2a45564c14b714cef48a4a6d51": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ada9e6097de24041a4613cdcbefc9568": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "95c5615d068e45d5b916aa778cf12943": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55828WPMjIbR"
      },
      "source": [
        "# Set up"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEbFUGQyh6ch"
      },
      "source": [
        "%%capture\r\n",
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5heFvOoGiP8h"
      },
      "source": [
        "import os, re, json, requests, io, string\r\n",
        "import numpy as np\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow import keras\r\n",
        "from tensorflow.keras import layers\r\n",
        "from tokenizers import BertWordPieceTokenizer\r\n",
        "from transformers import BertTokenizer, TFBertModel, BertConfig\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import pickle\r\n",
        "import random\r\n",
        "\r\n",
        "MAX_LEN = 384\r\n",
        "random.seed(42)\r\n",
        "configuration = BertConfig()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ifdy9499Jl-v",
        "outputId": "de5c1382-f3e0-4215-e93b-bf3cc7735d38"
      },
      "source": [
        "# Load the Drive helper and mount -> we could use this to save the weights of the models\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/gdrive', force_remount = True)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ns95EZ9jjXL_"
      },
      "source": [
        "# Set up tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "21b02101abb1424680d6418e7967496b",
            "36ae7fe6557b4daa8b07d97499f0daf1",
            "df2c0a5dbba743a18d89237e10c6c988",
            "84732c8455a74d3aaa236130b7fbe4b8",
            "2bc1f1f34b3645859dbe2d213103e116",
            "17489a2a45564c14b714cef48a4a6d51",
            "ada9e6097de24041a4613cdcbefc9568",
            "95c5615d068e45d5b916aa778cf12943"
          ]
        },
        "id": "aO1kSoCVjg2X",
        "outputId": "7134db2e-f66a-435e-d499-3fdfe8c91939"
      },
      "source": [
        "# Save the slow pretrained tokenizer\r\n",
        "slow_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\r\n",
        "save_path = \"bert_base_uncased/\"\r\n",
        "if not os.path.exists(save_path):\r\n",
        "    os.makedirs(save_path)\r\n",
        "slow_tokenizer.save_pretrained(save_path)\r\n",
        "\r\n",
        "# Load the fast tokenizer from saved file\r\n",
        "tokenizer = BertWordPieceTokenizer(\"bert_base_uncased/vocab.txt\", lowercase=True)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "21b02101abb1424680d6418e7967496b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JoFJwoUhjlZR"
      },
      "source": [
        "# Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0jaIxPTLmxH",
        "outputId": "69e85d47-74aa-4d8f-f529-1d3e079bb460"
      },
      "source": [
        "\r\n",
        "url = \"https://raw.githubusercontent.com/amrlnic/SQuAD/main/data/training_set.json\" # Make sure the url is the raw version of the file on GitHub\r\n",
        "download = requests.get(url).content\r\n",
        "data = json.loads(download)\r\n",
        "\r\n",
        "def load_dataset(file, record_path = ['data', 'paragraphs', 'qas', 'answers'], verbose = True):\r\n",
        "\r\n",
        "  \"\"\"\r\n",
        "  parse the SQUAD dataset into a dataframe\r\n",
        "  \"\"\"\r\n",
        "\r\n",
        "  if verbose:\r\n",
        "      print(\"Reading the json file\")\r\n",
        "\r\n",
        "  if verbose:\r\n",
        "      print(\"[INFO] processing...\")\r\n",
        "\r\n",
        "  # parsing different level's in the json file\r\n",
        "  js = pd.json_normalize(file , record_path )\r\n",
        "  m = pd.json_normalize(file, record_path[:-1] )\r\n",
        "  r = pd.json_normalize(file, record_path[:-2])\r\n",
        "  t = pd.json_normalize(file, record_path[0])\r\n",
        "\r\n",
        "  title = pd.json_normalize(file['data'], record_path = ['paragraphs'], meta = 'title')\r\n",
        "\r\n",
        "  #combining it into single dataframe\r\n",
        "  idx = np.repeat(r['context'].values, r.qas.str.len())\r\n",
        "  ndx  = np.repeat(m['id'].values, m['answers'].str.len())\r\n",
        "  m['context'] = idx\r\n",
        "  m['title'] = np.repeat(title['title'].values, r.qas.str.len())\r\n",
        "  js['q_idx'] = ndx\r\n",
        "  main = pd.concat([ m[['id','question','context', 'title']].set_index('id'), js.set_index('q_idx')], 1, sort = False).reset_index()\r\n",
        "  main['c_id'] = main['context'].factorize()[0]\r\n",
        "  if verbose:\r\n",
        "      print(f\"[INFO] there are {main.shape[0]} questions with single answer\")\r\n",
        "      print(f\"[INFO] there are {main.groupby('c_id').sum().shape[0]} different contexts\")\r\n",
        "      print(f\"[INFO] there are {len(t)} unrelated subjects\")\r\n",
        "      print(\"[INFO] Done\")\r\n",
        "  return main\r\n",
        "\r\n",
        "squad_dataset = load_dataset(data)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading the json file\n",
            "[INFO] processing...\n",
            "[INFO] there are 87599 questions with single answer\n",
            "[INFO] there are 18891 different contexts\n",
            "[INFO] there are 442 unrelated subjects\n",
            "[INFO] Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "lzHPUWA45dzO",
        "outputId": "aca16f64-e80a-4776-e17a-c2a862bc864a"
      },
      "source": [
        "squad_dataset.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>question</th>\n",
              "      <th>context</th>\n",
              "      <th>title</th>\n",
              "      <th>answer_start</th>\n",
              "      <th>text</th>\n",
              "      <th>c_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5733be284776f41900661182</td>\n",
              "      <td>To whom did the Virgin Mary allegedly appear i...</td>\n",
              "      <td>Architecturally, the school has a Catholic cha...</td>\n",
              "      <td>University_of_Notre_Dame</td>\n",
              "      <td>515</td>\n",
              "      <td>Saint Bernadette Soubirous</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5733be284776f4190066117f</td>\n",
              "      <td>What is in front of the Notre Dame Main Building?</td>\n",
              "      <td>Architecturally, the school has a Catholic cha...</td>\n",
              "      <td>University_of_Notre_Dame</td>\n",
              "      <td>188</td>\n",
              "      <td>a copper statue of Christ</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5733be284776f41900661180</td>\n",
              "      <td>The Basilica of the Sacred heart at Notre Dame...</td>\n",
              "      <td>Architecturally, the school has a Catholic cha...</td>\n",
              "      <td>University_of_Notre_Dame</td>\n",
              "      <td>279</td>\n",
              "      <td>the Main Building</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5733be284776f41900661181</td>\n",
              "      <td>What is the Grotto at Notre Dame?</td>\n",
              "      <td>Architecturally, the school has a Catholic cha...</td>\n",
              "      <td>University_of_Notre_Dame</td>\n",
              "      <td>381</td>\n",
              "      <td>a Marian place of prayer and reflection</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5733be284776f4190066117e</td>\n",
              "      <td>What sits on top of the Main Building at Notre...</td>\n",
              "      <td>Architecturally, the school has a Catholic cha...</td>\n",
              "      <td>University_of_Notre_Dame</td>\n",
              "      <td>92</td>\n",
              "      <td>a golden statue of the Virgin Mary</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                      index  ... c_id\n",
              "0  5733be284776f41900661182  ...    0\n",
              "1  5733be284776f4190066117f  ...    0\n",
              "2  5733be284776f41900661180  ...    0\n",
              "3  5733be284776f41900661181  ...    0\n",
              "4  5733be284776f4190066117e  ...    0\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Akd8JN0Z7Isv"
      },
      "source": [
        "# Pre - processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCWKKV2RAhML"
      },
      "source": [
        "SAMPLES = squad_dataset.shape[0]\r\n",
        "\r\n",
        "def preprocess_sentence(text):\r\n",
        "\r\n",
        "  \"\"\"\r\n",
        "  lowercase and strip the given text\r\n",
        "  \"\"\"\r\n",
        "\r\n",
        "  text = text.lower()\r\n",
        "  text = text.strip()\r\n",
        "  return text\r\n",
        "\r\n",
        "def clean_dataset(dataset):\r\n",
        "\r\n",
        "  \"\"\"\r\n",
        "  preprocess the dataset\r\n",
        "  \"\"\"\r\n",
        "\r\n",
        "  _dataset = dataset.copy()\r\n",
        "\r\n",
        "  cleaned_questions = _dataset['question'].apply(preprocess_sentence)\r\n",
        "  cleaned_texts = _dataset['text'].apply(preprocess_sentence)\r\n",
        "\r\n",
        "  # we process only different contexts and then we duplicate them\r\n",
        "  unique_context = pd.Series(_dataset['context'].unique())\r\n",
        "  count_c = _dataset.groupby('c_id').count()['text']\r\n",
        "  cleaned_contexts = unique_context.apply(preprocess_sentence)\r\n",
        "\r\n",
        "  _dataset['question'] = cleaned_questions\r\n",
        "  _dataset['text'] = cleaned_texts\r\n",
        "  _dataset['context'] = pd.Series(np.repeat(cleaned_contexts, count_c).tolist())\r\n",
        "\r\n",
        "  return _dataset"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSVZ45xxqqqz"
      },
      "source": [
        "squad_dataset = clean_dataset(squad_dataset)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgvUMpP7RjKl"
      },
      "source": [
        "# Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7q_hIu8RUDI"
      },
      "source": [
        "def split(dataset, train_size = 0.8):\r\n",
        "\r\n",
        "  \"\"\"\r\n",
        "  split the dataset in two part: the training and the validation\r\n",
        "  \"\"\"\r\n",
        "\r\n",
        "  # find unique titles\r\n",
        "  titles = squad_dataset['title']\r\n",
        "  unique_titles = titles.unique()\r\n",
        "\r\n",
        "\r\n",
        "  n_titles = len(unique_titles)\r\n",
        "  titles_seq = list(range(n_titles))\r\n",
        "\r\n",
        "  train_len = int(n_titles*train_size)\r\n",
        "\r\n",
        "  # sample train indexes\r\n",
        "  train_ind = random.sample(titles_seq, train_len)\r\n",
        "  test_ind = list(set(titles_seq) - set(train_ind))\r\n",
        "\r\n",
        "  train_titles = unique_titles[train_ind]\r\n",
        "  test_titles = unique_titles[test_ind]\r\n",
        "\r\n",
        "  squad_columns = list(squad_dataset.columns)\r\n",
        "\r\n",
        "  # initialize empty train and test df\r\n",
        "  train_data = pd.DataFrame(columns = squad_columns)\r\n",
        "  test_data = pd.DataFrame(columns = squad_columns)\r\n",
        "\r\n",
        "  for train_title in train_titles:\r\n",
        "\r\n",
        "    train_section = squad_dataset[squad_dataset['title'] == train_title]\r\n",
        "    train_data = train_data.append(train_section)\r\n",
        "\r\n",
        "  for test_title in test_titles:\r\n",
        "\r\n",
        "    test_section = squad_dataset[squad_dataset['title'] == test_title]\r\n",
        "    test_data = test_data.append(test_section)\r\n",
        "\r\n",
        "\r\n",
        "  return train_data, test_data"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qmlwoWxZRdt6",
        "outputId": "58a44f68-0f07-4bcd-a871-fefbfb3d2abe"
      },
      "source": [
        "tr_df, vl_df = split(squad_dataset)\r\n",
        "tr_df.shape[0],vl_df.shape[0]"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(69129, 18470)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8kC65SqDvTZ"
      },
      "source": [
        "# Filter rows"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rq8B58osDLvX"
      },
      "source": [
        "def skip(row):\r\n",
        "\r\n",
        "  \"\"\"\r\n",
        "  Create the input sequences and find the rows that we have to skip\r\n",
        "  \"\"\"\r\n",
        "\r\n",
        "  answer = row['text']\r\n",
        "  context = row['context']\r\n",
        "  start_char_idx = row['answer_start']\r\n",
        "  question = row['question']\r\n",
        "\r\n",
        "  # initialize skip column\r\n",
        "  row['skip'] = False\r\n",
        "\r\n",
        "\r\n",
        "  # Find end character index of answer in context\r\n",
        "  end_char_idx = start_char_idx + len(answer)\r\n",
        "  if end_char_idx >= len(context):\r\n",
        "    row['skip'] = True\r\n",
        "    return row\r\n",
        "\r\n",
        "  # Mark the character indexes in context that are in answer\r\n",
        "  is_char_in_ans = [0] * len(context)\r\n",
        "  for idx in range(start_char_idx, end_char_idx):\r\n",
        "    is_char_in_ans[idx] = 1\r\n",
        "\r\n",
        "  # Tokenize context\r\n",
        "  tokenized_context = tokenizer.encode(context)\r\n",
        "  row['tokenized context'] = tokenized_context\r\n",
        "\r\n",
        "  # Find tokens that were created from answer characters\r\n",
        "  ans_token_idx = []\r\n",
        "  for idx, (start, end) in enumerate(tokenized_context.offsets):\r\n",
        "    if sum(is_char_in_ans[start:end]) > 0:\r\n",
        "      ans_token_idx.append(idx)\r\n",
        "\r\n",
        "  if len(ans_token_idx) == 0:\r\n",
        "    row['skip'] = True\r\n",
        "    return row\r\n",
        "\r\n",
        "  # Find start and end token index for tokens from answer\r\n",
        "  start_token_idx = ans_token_idx[0]\r\n",
        "  end_token_idx = ans_token_idx[-1]\r\n",
        "\r\n",
        "  row['start token idx'] = start_token_idx\r\n",
        "  row['end token idx'] = end_token_idx\r\n",
        "\r\n",
        "  # Tokenize question\r\n",
        "  tokenized_question = tokenizer.encode(question)\r\n",
        "  row['tokenized question'] = tokenized_question\r\n",
        "\r\n",
        "  # Inputs of the model: here are used to determine whether to skip the row or not\r\n",
        "  input_ids = tokenized_context.ids + tokenized_question.ids[1:]\r\n",
        "  token_type_ids = [0] * len(tokenized_context.ids) + [1] * len(\r\n",
        "            tokenized_question.ids[1:]\r\n",
        "        )\r\n",
        "  attention_mask = [1] * len(input_ids)\r\n",
        "\r\n",
        "  padding_length = MAX_LEN - len(input_ids)\r\n",
        "\r\n",
        "  if padding_length > 0:  # pad\r\n",
        "    input_ids = input_ids + ([0] * padding_length)\r\n",
        "    attention_mask = attention_mask + ([0] * padding_length)\r\n",
        "    token_type_ids = token_type_ids + ([0] * padding_length)\r\n",
        "  elif padding_length < 0:\r\n",
        "    row['skip'] = True\r\n",
        "  \r\n",
        "  row['input ids'] = np.array(input_ids)\r\n",
        "  row['token type ids'] = np.array(token_type_ids)\r\n",
        "  row['attention mask'] = np.array(attention_mask)\r\n",
        "\r\n",
        "  return row\r\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8eTsENy6jU-t",
        "outputId": "f0e4879d-1cab-4e4c-c740-58fe91e557c8"
      },
      "source": [
        "# takes a while\r\n",
        "tr_df = tr_df.apply(skip, axis = 1)\r\n",
        "vl_df = vl_df.apply(skip, axis = 1)\r\n",
        "\r\n",
        "len(tr_df[tr_df['skip']]), len(vl_df[vl_df['skip']])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1031, 421)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZ3CXWdL8CNk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57d0169f-6fa2-4c06-a947-da0a63397199"
      },
      "source": [
        "# we get rid of samples where the answer doesn't match the context\r\n",
        "\r\n",
        "tr_df = tr_df[tr_df['skip'] == False]\r\n",
        "vl_df = vl_df[vl_df['skip'] == False]\r\n",
        "\r\n",
        "len(tr_df), len(vl_df)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(68098, 18049)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVWNLTKlN2xq"
      },
      "source": [
        "# Save datasets as json"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axtiEuSsN1Cn"
      },
      "source": [
        "def df_to_json(df, path):\r\n",
        "\r\n",
        "  \"\"\"\r\n",
        "  parse the given dataframe into the SQUAD json format\r\n",
        "  \"\"\"\r\n",
        "  \r\n",
        "  data = []\r\n",
        "\r\n",
        "  for title, articles in df.groupby('title'):\r\n",
        "    chapter = {'title': title}\r\n",
        "    paragraphs = []\r\n",
        "    for context, contents in articles.groupby('context'):\r\n",
        "      paragraph = {'context': context}\r\n",
        "      qas = []\r\n",
        "      for i, content in contents.iterrows():\r\n",
        "        qa = {'answers': [{'answer_start': content['answer_start'], 'text': content['text']}], 'question': content['question'], 'id': content['index']}\r\n",
        "        qas.append(qa)\r\n",
        "      paragraph.update({'qas': qas})\r\n",
        "      paragraphs.append(paragraph)\r\n",
        "    chapter.update({'paragraphs': paragraphs})\r\n",
        "    data.append(chapter)\r\n",
        "  raw_data = {'data': data}\r\n",
        "\r\n",
        "  with open(path, 'w') as handle:\r\n",
        "    json.dump(raw_data, handle)\r\n",
        "\r\n",
        "  print(f'dataset saved in {path}')"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4lFv9wpDfDOj",
        "outputId": "0ca316f5-a294-463c-e295-a2d206a2abd0"
      },
      "source": [
        "# save datasets in json format\r\n",
        "path_to_train_set = os.path.join(os.getcwd(), 'BERT_train_set.json')\r\n",
        "df_to_json(tr_df, path_to_train_set)\r\n",
        "\r\n",
        "path_to_valid_set = os.path.join(os.getcwd(), 'BERT_valid_set.json')\r\n",
        "df_to_json(vl_df, path_to_valid_set)"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dataset saved in /content/BERT_train_set.json\n",
            "dataset saved in /content/BERT_valid_set.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHS6UOM707bE"
      },
      "source": [
        "# Define input and output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0XTuuH6TfPX"
      },
      "source": [
        "train_path = \"/content/gdrive/My Drive/Colab Notebooks/SQUAD_project/train_df\"\r\n",
        "val_path = \"/content/gdrive/My Drive/Colab Notebooks/SQUAD_project/val_df\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2rLUe9YJ4dN"
      },
      "source": [
        "# Save dataframes on drive \r\n",
        "\r\n",
        "pickle.dump( tr_df, open(train_path, \"wb\" ) )\r\n",
        "pickle.dump( vl_df, open(val_path, \"wb\" ) )"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fkm4rVAHKiUm"
      },
      "source": [
        "# Load dataframes\r\n",
        "\r\n",
        "tr_df = pickle.load( open(train_path, \"rb\" ) )  \r\n",
        "tr_df = pickle.load( open(val_path, \"rb\" ) )  "
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGCwc1sb_fZL"
      },
      "source": [
        "def create_inputs_targets(squad_examples):\r\n",
        "\r\n",
        "  '''\r\n",
        "  Function to create inputs for the model\r\n",
        "\r\n",
        "  squad_examples (df)\r\n",
        "  '''\r\n",
        "\r\n",
        "    dataset_dict = {\r\n",
        "        \"input ids\": [],\r\n",
        "        \"token type ids\": [],\r\n",
        "        \"attention mask\": [],\r\n",
        "        \"start token idx\": [],\r\n",
        "        \"end token idx\": [],\r\n",
        "    }\r\n",
        "\r\n",
        "    n_items = len(squad_examples)\r\n",
        "    for i in range(n_items):\r\n",
        "        item = squad_examples.iloc[i]\r\n",
        "        for key in dataset_dict:\r\n",
        "          dataset_dict[key].append(getattr(item, key))\r\n",
        "    for key in dataset_dict:\r\n",
        "        dataset_dict[key] = np.array(dataset_dict[key])\r\n",
        "\r\n",
        "    x = [\r\n",
        "        dataset_dict[\"input ids\"],\r\n",
        "        dataset_dict[\"token type ids\"],\r\n",
        "        dataset_dict[\"attention mask\"],\r\n",
        "        ]\r\n",
        "    y = [dataset_dict[\"start token idx\"], dataset_dict[\"end token idx\"]]\r\n",
        "    return x, y"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gm2SvlR2Cegy"
      },
      "source": [
        "x_train, y_train = create_inputs_targets(tr_df)\r\n",
        "x_eval, y_eval = create_inputs_targets(vl_df)"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-8L86bUpuRD"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VH6_m_uD3D0m"
      },
      "source": [
        "def create_model(enc_dim, dec_dim, rec_mod = 'biLSTM', bert_ft = True, dropout = False, drop_prob = 0.5):\r\n",
        "\r\n",
        "    \"\"\" \r\n",
        "    Returns a keras model for predicting the start and the end of the answer\r\n",
        "\r\n",
        "    enc_dim (int): encoding dimension\r\n",
        "    dec_dim (int): decoding dimension\r\n",
        "    rec_mod (string): type of recurrent modules // 'biLSTM' or 'GRU'\r\n",
        "    bert_ft (boolean): whether or not the bert will be fine - tuned\r\n",
        "    dropout (boolean): whether or not using the dropout\r\n",
        "    drop_prob (double): dropout probability\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    # use pre - trained BERT for creating the embeddings\r\n",
        "    bert_model = TFBertModel.from_pretrained(\"bert-base-uncased\")\r\n",
        "    if not bert_ft:\r\n",
        "      for layer in bert_model.layers:\r\n",
        "        layer.trainable = False\r\n",
        "\r\n",
        "    # input\r\n",
        "    input_ids = layers.Input(shape=(MAX_LEN,), dtype=tf.int32)\r\n",
        "    token_type_ids = layers.Input(shape=(MAX_LEN,), dtype=tf.int32)\r\n",
        "    attention_mask = layers.Input(shape=(MAX_LEN,), dtype=tf.int32)\r\n",
        "    embeddings = bert_model(\r\n",
        "        input_ids, token_type_ids = token_type_ids, attention_mask = attention_mask\r\n",
        "    )[0]\r\n",
        "\r\n",
        "    # encoder - decoder\r\n",
        "\r\n",
        "    if rec_mod == 'biLSTM':\r\n",
        "\r\n",
        "      encoder = layers.Bidirectional(layers.LSTM(enc_dim, return_sequences = True), \r\n",
        "                                          merge_mode = 'concat')(embeddings)\r\n",
        "\r\n",
        "      decoder = layers.Bidirectional(layers.LSTM(dec_dim, return_sequences = True), \r\n",
        "                                                      merge_mode = 'concat')(encoder)\r\n",
        "\r\n",
        "      high_dim = dec_dim*2 # number of units of the dense layers of the highway network\r\n",
        "\r\n",
        "    \r\n",
        "    else:\r\n",
        "\r\n",
        "      encoder = layers.GRU(enc_dim, return_sequences = True)(embeddings)\r\n",
        "\r\n",
        "      decoder = layers.GRU(dec_dim, return_sequences = True)(encoder)\r\n",
        "\r\n",
        "      high_dim = dec_dim\r\n",
        "\r\n",
        "\r\n",
        "    # highway network\r\n",
        "    x_proj = layers.Dense(units = high_dim, activation = 'relu')(decoder)\r\n",
        "    x_gate = layers.Dense(units = high_dim, activation = 'sigmoid')(decoder)\r\n",
        "\r\n",
        "    x = (x_proj * x_gate) + (1 - x_gate) * decoder\r\n",
        "\r\n",
        "    # dropout\r\n",
        "    if dropout:\r\n",
        "      x = layers.Dropout(drop_prob)(x)\r\n",
        "\r\n",
        "    # output\r\n",
        "\r\n",
        "    start_logits = layers.Dense(1, use_bias = False)(x)\r\n",
        "    start_logits = layers.Flatten()(start_logits)\r\n",
        "\r\n",
        "    end_logits = layers.Dense(1, use_bias = False)(x)\r\n",
        "    end_logits = layers.Flatten()(end_logits)\r\n",
        "\r\n",
        "    start_probs = layers.Activation(keras.activations.softmax)(start_logits)\r\n",
        "    end_probs = layers.Activation(keras.activations.softmax)(end_logits)\r\n",
        "\r\n",
        "\r\n",
        "    model = keras.Model(\r\n",
        "        inputs = [input_ids, token_type_ids, attention_mask],\r\n",
        "        outputs = [start_probs, end_probs]\r\n",
        "    )\r\n",
        "\r\n",
        "    loss = keras.losses.SparseCategoricalCrossentropy(from_logits = False)\r\n",
        "    optimizer = keras.optimizers.Adam(lr = 5e-5)\r\n",
        "    model.compile(optimizer = optimizer, loss = [loss, loss])\r\n",
        "\r\n",
        "    return model"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOkcMBHzGWA_"
      },
      "source": [
        "encoding_dimension = 128\r\n",
        "decoding_dimension = 64\r\n",
        "rec_mod = 'GRU'\r\n",
        "ft = True\r\n",
        "dropout = True\r\n",
        "drop_prob = 0.5\r\n",
        "\r\n",
        "use_tpu = True\r\n",
        "if use_tpu:\r\n",
        "    # Create distribution strategy\r\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\r\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\r\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\r\n",
        "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\r\n",
        "\r\n",
        "    # Create model\r\n",
        "    with strategy.scope():\r\n",
        "        model = create_model(encoding_dimension, decoding_dimension, rec_mod, ft, dropout, drop_prob)\r\n",
        "else:\r\n",
        "    model = create_model(encoding_dimension, decoding_dimension, rec_mod, ft, dropout, drop_prob)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9qyPZRDbWqa",
        "outputId": "2981451e-5f69-4fb8-ac70-07b2892f7de4"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_4 (InputLayer)            [(None, 384)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_6 (InputLayer)            [(None, 384)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_5 (InputLayer)            [(None, 384)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf_bert_model_2 (TFBertModel)   TFBaseModelOutputWit 109482240   input_4[0][0]                    \n",
            "                                                                 input_6[0][0]                    \n",
            "                                                                 input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "gru_2 (GRU)                     (None, 384, 128)     344832      tf_bert_model_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "gru_3 (GRU)                     (None, 384, 64)      37248       gru_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 384, 64)      4160        gru_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 384, 64)      4160        gru_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.subtract_1 (TFOpLambda) (None, 384, 64)      0           dense_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.multiply_2 (TFOpLambda) (None, 384, 64)      0           dense_4[0][0]                    \n",
            "                                                                 dense_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.multiply_3 (TFOpLambda) (None, 384, 64)      0           tf.math.subtract_1[0][0]         \n",
            "                                                                 gru_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_1 (TFOpLam (None, 384, 64)      0           tf.math.multiply_2[0][0]         \n",
            "                                                                 tf.math.multiply_3[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "dropout_112 (Dropout)           (None, 384, 64)      0           tf.__operators__.add_1[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 384, 1)       64          dropout_112[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_7 (Dense)                 (None, 384, 1)       64          dropout_112[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "flatten_2 (Flatten)             (None, 384)          0           dense_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "flatten_3 (Flatten)             (None, 384)          0           dense_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 384)          0           flatten_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 384)          0           flatten_3[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 109,872,768\n",
            "Trainable params: 109,872,768\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44YHPxAzL4uH"
      },
      "source": [
        "## Create evaluation Callback\r\n",
        "\r\n",
        "This callback will compute the exact match score using the validation data\r\n",
        "after every epoch.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75z_-Z3iZlwv"
      },
      "source": [
        "def normalize_text(text):\r\n",
        "    text = text.lower()\r\n",
        "\r\n",
        "    # Remove punctuations\r\n",
        "    exclude = set(string.punctuation)\r\n",
        "    text = \"\".join(ch for ch in text if ch not in exclude)\r\n",
        "\r\n",
        "    # Remove articles\r\n",
        "    regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\r\n",
        "    text = re.sub(regex, \" \", text)\r\n",
        "\r\n",
        "    # Remove extra white space\r\n",
        "    text = \" \".join(text.split())\r\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-LvzUW2bQY6"
      },
      "source": [
        "\n",
        "class ExactMatch(keras.callbacks.Callback):\n",
        "    \"\"\"\n",
        "    Each `SquadExample` object contains the character level offsets for each token\n",
        "    in its input paragraph. We use them to get back the span of text corresponding\n",
        "    to the tokens between our predicted start and end tokens.\n",
        "    All the ground-truth answers are also present in each `SquadExample` object.\n",
        "    We calculate the percentage of data points where the span of text obtained\n",
        "    from model predictions matches one of the ground-truth answers.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, x_eval, y_eval):\n",
        "        self.x_eval = x_eval\n",
        "        self.y_eval = y_eval\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        pred_start, pred_end = self.model.predict(self.x_eval)\n",
        "        count = 0\n",
        "\n",
        "        for idx, (start, end) in enumerate(zip(pred_start, pred_end)):\n",
        "          squad_eg = vl_df.iloc[idx]\n",
        "          offsets = squad_eg['tokenized context'].offsets\n",
        "          start = np.argmax(start)\n",
        "          end = np.argmax(end)\n",
        "          if start >= len(offsets):\n",
        "              continue\n",
        "          pred_char_start = offsets[start][0]\n",
        "          if end < len(offsets):\n",
        "            pred_char_end = offsets[end][1]\n",
        "            pred_ans = squad_eg.context[pred_char_start:pred_char_end]\n",
        "          else:\n",
        "            pred_ans = squad_eg.context[pred_char_start:]\n",
        "\n",
        "          normalized_pred_ans = normalize_text(pred_ans)\n",
        "          normalized_true_ans = normalize_text(squad_eg['text'])\n",
        "          #normalized_true_ans = [normalize_text(_) for _ in squad_eg.all_answers]\n",
        "          #if normalized_pred_ans in normalized_true_ans:\n",
        "          if normalized_pred_ans == normalized_true_ans:\n",
        "                count += 1\n",
        "        acc = count / len(self.y_eval[0])\n",
        "        print(f\"\\nepoch = {epoch+1}, exact match score = {acc:.2f}\")\n",
        "\n"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24aY5QQbboAN"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUTb0el4bQY7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc964740-2adc-4989-d7e4-d45978a79842"
      },
      "source": [
        "# weights path\n",
        "filepath = '/content/gdrive/My Drive/Colab Notebooks/bert_encDec_weights.h5'\n",
        "\n",
        "# checkpoint callback \n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
        "        filepath = filepath,\n",
        "        save_weights_only = True,\n",
        "        )\n",
        "\n",
        "exact_match_callback = ExactMatch(x_eval, y_eval)\n",
        "model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    epochs = 1,\n",
        "    verbose = 1,\n",
        "    batch_size = 256,\n",
        "    callbacks = [exact_match_callback, checkpoint],\n",
        ")\n"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  6/267 [..............................] - ETA: 2:46 - loss: 2.1927 - activation_2_loss: 1.1616 - activation_3_loss: 1.0311WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0052s vs `on_train_batch_end` time: 0.6321s). Check your callbacks.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0052s vs `on_train_batch_end` time: 0.6321s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "267/267 [==============================] - 169s 634ms/step - loss: 2.0713 - activation_2_loss: 1.1056 - activation_3_loss: 0.9658\n",
            "\n",
            "epoch = 1, exact match score = 0.64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f6415e775d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0yFLYWbb2Pf"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msdw2-Amgysq"
      },
      "source": [
        "! git clone https://github.com/amrlnic/SQuAD.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOrunjMib4xH"
      },
      "source": [
        "predictions = model.predict(x_eval) \r\n",
        "\r\n",
        "predictions2 = {}\r\n",
        "for i in range(len(predictions[0])):\r\n",
        "  start=np.argmax(predictions[0][i])\r\n",
        "  end=np.argmax(predictions[1][i])\r\n",
        "  tokenized_answer = x_eval[0][i:i+1][0][start:end+1]\r\n",
        "\r\n",
        "  decoded = tokenizer.decode(tokenized_answer)\r\n",
        "\r\n",
        "  predictions2[vl_df.iloc[i]['index']] = decoded\r\n",
        "\r\n",
        "##### Save model predictions on val set as a .JSON file  #####\r\n",
        "\r\n",
        "import json\r\n",
        "\r\n",
        "with open('pred.json', 'w') as fp:\r\n",
        "    json.dump(predictions2, fp)"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ySWMq7wpcyaF",
        "outputId": "3c8ad90f-8294-4eb4-ee6f-e64306f7270d"
      },
      "source": [
        "!python3 SQuAD/evaluation/evaluate.py SQuAD/BERT/BERT_valid_set.json pred.json"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"exact\": 57.39930190038229,\n",
            "  \"f1\": 72.44498738548172,\n",
            "  \"total\": 18049,\n",
            "  \"HasAns_exact\": 57.39930190038229,\n",
            "  \"HasAns_f1\": 72.44498738548172,\n",
            "  \"HasAns_total\": 18049\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}