{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55828WPMjIbR"
      },
      "source": [
        "# Set up"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5uIC_dIFytkR",
        "outputId": "64938351-bad2-45dd-df52-1d8146dd3437"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEbFUGQyh6ch"
      },
      "source": [
        "%%capture\r\n",
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5heFvOoGiP8h"
      },
      "source": [
        "import os, re, json, requests, io, string\r\n",
        "import numpy as np\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow import keras\r\n",
        "from tensorflow.keras import layers\r\n",
        "from tokenizers import BertWordPieceTokenizer\r\n",
        "from transformers import BertTokenizer, TFBertModel, BertConfig\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import pickle\r\n",
        "\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "\r\n",
        "#import nltk\r\n",
        "#from nltk import word_tokenize\r\n",
        "#nltk.download('punkt')\r\n",
        "#import gensim.downloader as gloader\r\n",
        "\r\n",
        "MAX_LEN = 384\r\n",
        "configuration = BertConfig()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ns95EZ9jjXL_"
      },
      "source": [
        "# Set up tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aO1kSoCVjg2X"
      },
      "source": [
        "# Save the slow pretrained tokenizer\r\n",
        "slow_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\r\n",
        "save_path = \"bert_base_uncased/\"\r\n",
        "if not os.path.exists(save_path):\r\n",
        "    os.makedirs(save_path)\r\n",
        "slow_tokenizer.save_pretrained(save_path)\r\n",
        "\r\n",
        "# Load the fast tokenizer from saved file\r\n",
        "tokenizer = BertWordPieceTokenizer(\"bert_base_uncased/vocab.txt\", lowercase=True)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JoFJwoUhjlZR"
      },
      "source": [
        "# Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0jaIxPTLmxH",
        "outputId": "644ab71c-b0cc-4fb8-fa90-62dd02e0451e"
      },
      "source": [
        "\r\n",
        "url = \"https://raw.githubusercontent.com/amrlnic/SQuAD/main/data/training_set.json\" # Make sure the url is the raw version of the file on GitHub\r\n",
        "download = requests.get(url).content\r\n",
        "data = json.loads(download)\r\n",
        "\r\n",
        "def load_dataset(file, record_path = ['data', 'paragraphs', 'qas', 'answers'], verbose = True):\r\n",
        "\r\n",
        "  \"\"\"\r\n",
        "  parse the SQUAD dataset into a dataframe\r\n",
        "  \"\"\"\r\n",
        "\r\n",
        "  if verbose:\r\n",
        "      print(\"Reading the json file\")\r\n",
        "\r\n",
        "  if verbose:\r\n",
        "      print(\"[INFO] processing...\")\r\n",
        "\r\n",
        "  # parsing different level's in the json file\r\n",
        "  js = pd.json_normalize(file , record_path )\r\n",
        "  m = pd.json_normalize(file, record_path[:-1] )\r\n",
        "  r = pd.json_normalize(file, record_path[:-2])\r\n",
        "  t = pd.json_normalize(file, record_path[0])\r\n",
        "\r\n",
        "  title = pd.json_normalize(file['data'], record_path = ['paragraphs'], meta = 'title')\r\n",
        "\r\n",
        "  #combining it into single dataframe\r\n",
        "  idx = np.repeat(r['context'].values, r.qas.str.len())\r\n",
        "  ndx  = np.repeat(m['id'].values, m['answers'].str.len())\r\n",
        "  m['context'] = idx\r\n",
        "  m['title'] = np.repeat(title['title'].values, r.qas.str.len())\r\n",
        "  js['q_idx'] = ndx\r\n",
        "  main = pd.concat([ m[['id','question','context', 'title']].set_index('id'), js.set_index('q_idx')], 1, sort = False).reset_index()\r\n",
        "  main['c_id'] = main['context'].factorize()[0]\r\n",
        "  if verbose:\r\n",
        "      print(f\"[INFO] there are {main.shape[0]} questions with single answer\")\r\n",
        "      print(f\"[INFO] there are {main.groupby('c_id').sum().shape[0]} different contexts\")\r\n",
        "      print(f\"[INFO] there are {len(t)} unrelated subjects\")\r\n",
        "      print(\"[INFO] Done\")\r\n",
        "  return main\r\n",
        "\r\n",
        "squad_dataset = load_dataset(data)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading the json file\n",
            "[INFO] processing...\n",
            "[INFO] there are 87599 questions with single answer\n",
            "[INFO] there are 18891 different contexts\n",
            "[INFO] there are 442 unrelated subjects\n",
            "[INFO] Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "lzHPUWA45dzO",
        "outputId": "aca16f64-e80a-4776-e17a-c2a862bc864a"
      },
      "source": [
        "squad_dataset.head()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>question</th>\n",
              "      <th>context</th>\n",
              "      <th>title</th>\n",
              "      <th>answer_start</th>\n",
              "      <th>text</th>\n",
              "      <th>c_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5733be284776f41900661182</td>\n",
              "      <td>To whom did the Virgin Mary allegedly appear i...</td>\n",
              "      <td>Architecturally, the school has a Catholic cha...</td>\n",
              "      <td>University_of_Notre_Dame</td>\n",
              "      <td>515</td>\n",
              "      <td>Saint Bernadette Soubirous</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5733be284776f4190066117f</td>\n",
              "      <td>What is in front of the Notre Dame Main Building?</td>\n",
              "      <td>Architecturally, the school has a Catholic cha...</td>\n",
              "      <td>University_of_Notre_Dame</td>\n",
              "      <td>188</td>\n",
              "      <td>a copper statue of Christ</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5733be284776f41900661180</td>\n",
              "      <td>The Basilica of the Sacred heart at Notre Dame...</td>\n",
              "      <td>Architecturally, the school has a Catholic cha...</td>\n",
              "      <td>University_of_Notre_Dame</td>\n",
              "      <td>279</td>\n",
              "      <td>the Main Building</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5733be284776f41900661181</td>\n",
              "      <td>What is the Grotto at Notre Dame?</td>\n",
              "      <td>Architecturally, the school has a Catholic cha...</td>\n",
              "      <td>University_of_Notre_Dame</td>\n",
              "      <td>381</td>\n",
              "      <td>a Marian place of prayer and reflection</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5733be284776f4190066117e</td>\n",
              "      <td>What sits on top of the Main Building at Notre...</td>\n",
              "      <td>Architecturally, the school has a Catholic cha...</td>\n",
              "      <td>University_of_Notre_Dame</td>\n",
              "      <td>92</td>\n",
              "      <td>a golden statue of the Virgin Mary</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                      index  ... c_id\n",
              "0  5733be284776f41900661182  ...    0\n",
              "1  5733be284776f4190066117f  ...    0\n",
              "2  5733be284776f41900661180  ...    0\n",
              "3  5733be284776f41900661181  ...    0\n",
              "4  5733be284776f4190066117e  ...    0\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Akd8JN0Z7Isv"
      },
      "source": [
        "# Pre - processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCWKKV2RAhML"
      },
      "source": [
        "SAMPLES = squad_dataset.shape[0]\r\n",
        "\r\n",
        "def preprocess_sentence(text):\r\n",
        "\r\n",
        "  \"\"\"\r\n",
        "  lowercase and strip the given text\r\n",
        "  \"\"\"\r\n",
        "\r\n",
        "  text = text.lower()\r\n",
        "  text = text.strip()\r\n",
        "  return text\r\n",
        "\r\n",
        "def clean_dataset(dataset):\r\n",
        "\r\n",
        "  \"\"\"\r\n",
        "  preprocess the dataset\r\n",
        "  \"\"\"\r\n",
        "\r\n",
        "  _dataset = dataset.copy()\r\n",
        "\r\n",
        "  cleaned_questions = _dataset['question'].apply(preprocess_sentence)\r\n",
        "  cleaned_texts = _dataset['text'].apply(preprocess_sentence)\r\n",
        "\r\n",
        "  # we process only different contexts and then we duplicate them\r\n",
        "  unique_context = pd.Series(_dataset['context'].unique())\r\n",
        "  count_c = _dataset.groupby('c_id').count()['text']\r\n",
        "  cleaned_contexts = unique_context.apply(preprocess_sentence)\r\n",
        "\r\n",
        "  _dataset['question'] = cleaned_questions\r\n",
        "  _dataset['text'] = cleaned_texts\r\n",
        "  _dataset['context'] = pd.Series(np.repeat(cleaned_contexts, count_c).tolist())\r\n",
        "\r\n",
        "  return _dataset"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSVZ45xxqqqz"
      },
      "source": [
        "squad_dataset = clean_dataset(squad_dataset)"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8kC65SqDvTZ"
      },
      "source": [
        "## Filter rows"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rq8B58osDLvX"
      },
      "source": [
        "def skip(row):\r\n",
        "\r\n",
        "  \"\"\"\r\n",
        "  find the rows for which an answer can't be found\r\n",
        "  \"\"\"\r\n",
        "\r\n",
        "\r\n",
        "  answer = row['text']\r\n",
        "  context = row['context']\r\n",
        "  start_char_idx = row['answer_start']\r\n",
        "  question = row['question']\r\n",
        "\r\n",
        "  # initialize skip column\r\n",
        "  row['skip'] = False\r\n",
        "\r\n",
        "\r\n",
        "  # Find end character index of answer in context\r\n",
        "  end_char_idx = start_char_idx + len(answer)\r\n",
        "  if end_char_idx >= len(context):\r\n",
        "    row['skip'] = True\r\n",
        "    return row\r\n",
        "\r\n",
        "  # Mark the character indexes in context that are in answer\r\n",
        "  is_char_in_ans = [0] * len(context)\r\n",
        "  for idx in range(start_char_idx, end_char_idx):\r\n",
        "    is_char_in_ans[idx] = 1\r\n",
        "\r\n",
        "  # Tokenize context\r\n",
        "  tokenized_context = tokenizer.encode(context)\r\n",
        "  #row['tokenized context'] = tokenized_context\r\n",
        "\r\n",
        "  # Find tokens that were created from answer characters\r\n",
        "  ans_token_idx = []\r\n",
        "  for idx, (start, end) in enumerate(tokenized_context.offsets):\r\n",
        "    if sum(is_char_in_ans[start:end]) > 0:\r\n",
        "      ans_token_idx.append(idx)\r\n",
        "\r\n",
        "  if len(ans_token_idx) == 0:\r\n",
        "    row['skip'] = True\r\n",
        "    return row\r\n",
        "\r\n",
        "  # Tokenize question\r\n",
        "  tokenized_question = tokenizer.encode(question)\r\n",
        "  #row['tokenized question'] = tokenized_question\r\n",
        "\r\n",
        "  # Inputs of the model: here are used to determine whether to skip the row or not\r\n",
        "  input_ids = tokenized_context.ids + tokenized_question.ids[1:]\r\n",
        "  token_type_ids = [0] * len(tokenized_context.ids) + [1] * len(\r\n",
        "            tokenized_question.ids[1:]\r\n",
        "        )\r\n",
        "\r\n",
        "\r\n",
        "  padding_length = MAX_LEN - len(input_ids)\r\n",
        "\r\n",
        "  if padding_length < 0:\r\n",
        "    row['skip'] = True\r\n",
        "\r\n",
        "  return row\r\n",
        "\r\n",
        "\r\n",
        "def split(dataset, test_size = 0.2, random_state = 42):\r\n",
        "\r\n",
        "  \"\"\"\r\n",
        "  split the dataset in two part: the training and the validation\r\n",
        "  \"\"\"\r\n",
        "\r\n",
        "  # random_state for deterministic state\r\n",
        "  tr, vl = train_test_split(dataset, test_size = test_size, random_state = random_state)\r\n",
        "  tr.reset_index(drop = True, inplace = True)\r\n",
        "  vl.reset_index(drop = True, inplace = True)\r\n",
        "\r\n",
        "  return tr,vl\r\n",
        "\r\n",
        "def df_to_json(df, path):\r\n",
        "\r\n",
        "  \"\"\"\r\n",
        "  parse the given dataframe into the SQUAD json format\r\n",
        "  \"\"\"\r\n",
        "  \r\n",
        "  data = []\r\n",
        "\r\n",
        "  for title, articles in df.groupby('title'):\r\n",
        "    chapter = {'title': title}\r\n",
        "    paragraphs = []\r\n",
        "    for context, contents in articles.groupby('context'):\r\n",
        "      paragraph = {'context': context}\r\n",
        "      qas = []\r\n",
        "      for i, content in contents.iterrows():\r\n",
        "        qa = {'answers': [{'answer_start': content['answer_start'], 'text': content['text']}], 'question': content['question'], 'id': content['index']}\r\n",
        "        qas.append(qa)\r\n",
        "      paragraph.update({'qas': qas})\r\n",
        "      paragraphs.append(paragraph)\r\n",
        "    chapter.update({'paragraphs': paragraphs})\r\n",
        "    data.append(chapter)\r\n",
        "  raw_data = {'data': data}\r\n",
        "\r\n",
        "  with open(path, 'w') as handle:\r\n",
        "    json.dump(raw_data, handle)\r\n",
        "\r\n",
        "  print(f'dataset saved in {path}')"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8pXuOXlpLsx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87602a45-5718-4b08-811a-919e8fd05269"
      },
      "source": [
        "tr_df, vl_df = split(squad_dataset)\r\n",
        "tr_df.shape[0],vl_df.shape[0]"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(70079, 17520)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8eTsENy6jU-t",
        "outputId": "f96f1acc-8905-418a-8ed1-3912e65632f6"
      },
      "source": [
        "# takes a while\r\n",
        "tr_df = tr_df.apply(skip, axis = 1)\r\n",
        "vl_df = vl_df.apply(skip, axis = 1)\r\n",
        "\r\n",
        "len(tr_df[tr_df['skip']]), len(vl_df[vl_df['skip']])"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1185, 267)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZ3CXWdL8CNk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76f33134-41d2-4d7a-f0a5-95d97207158c"
      },
      "source": [
        "# we get rid of samples where the answer doesn't match the context\r\n",
        "\r\n",
        "tr_df = tr_df[tr_df['skip'] == False]\r\n",
        "vl_df = vl_df[vl_df['skip'] == False]\r\n",
        "\r\n",
        "len(tr_df), len(vl_df)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(68894, 17253)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4lFv9wpDfDOj",
        "outputId": "911eac83-2e70-422f-feb5-55f0c72e46c0"
      },
      "source": [
        "# save datasets in json format\r\n",
        "path_to_train_set = os.path.join(os.getcwd(), 'BERT_train_set.json')\r\n",
        "df_to_json(tr_df, path_to_train_set)\r\n",
        "\r\n",
        "path_to_valid_set = os.path.join(os.getcwd(), 'BERT_valid_set.json')\r\n",
        "df_to_json(vl_df, path_to_valid_set)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dataset saved in /content/BERT_train_set.json\n",
            "dataset saved in /content/BERT_valid_set.json\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}